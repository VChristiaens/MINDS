{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "651c89d8",
   "metadata": {},
   "source": [
    "# <img style=\"float: center;\" src='https://github.com/STScI-MIRI/MRS-ExampleNB/raw/main/assets/banner1.png' alt=\"stsci_logo\" width=\"900px\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa04a6ac",
   "metadata": {},
   "source": [
    "<a id=\"title_ID\"></a>\n",
    "# MIRI MRS Data Reduction Notebook #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2fb85c",
   "metadata": {},
   "source": [
    "**Description**: This notebook is originally based on David Law Flight's notebook + 2 Patrick Kavanagh's notebooks (to take into account residual fringe correction), from which we added complementary steps, external to the official `jwst` pipeline, developed by members of the MINDS collaboration.\n",
    "<br>\n",
    "**MINDS Authors**: **Valentin Christiaens** (background subtraction, bad pixel correction, automatic source centroid identification, spectrum extraction, spike filtering), **Matthias Samland** (background subraction, multiprocessing, bad pixel correction fixes and improvements, restructure), **Danny Gasman** (aperture corrections, point source specific corrections (psff) + spectral leak correction), **Milou Temmink** (continuum subtraction), and **Giulia Perotti** (base version - merged David Law and Patrick Kavanagh's notebooks).\n",
    "<br>\n",
    "**Attribution**: This notebook is the result of significant efforts from MINDS authors. We released the code on ASCL. If it is useful to your research proper attribution is kindly appreciated: [Christiaens, Samland, Gasman, Temmink & Perotti (2024)](https://ui.adsabs.harvard.edu/abs/2024ascl.soft03007C/abstract).\n",
    "\n",
    "Please also cite the following references, when relevant:<br>\n",
    "- [Gomez Gonzalez et al. (2017)](https://ui.adsabs.harvard.edu/abs/2017AJ....154....7G/abstract) and [Christiaens et al. (2023)](https://ui.adsabs.harvard.edu/abs/2023JOSS....8.4774C/abstract) for the VIP package, which is used to complement `jwst` pipeline steps for bad pixel correction, background subtraction and centroid finding;\n",
    "- [Gasman et al. (2023)](https://ui.adsabs.harvard.edu/abs/2023A%26A...673A.102G/abstract) for point-source specific calibrations (if you use the notebook with `psff=True`);\n",
    "- [Temmink et al. (2024)](https://ui.adsabs.harvard.edu/abs/2024A%26A...686A.117T/abstract) for continuum subtraction methods (if you use the notebook with `contsub=True`).\n",
    "\n",
    "**Contact**: valentin.christiaens@uliege.be or samland@mpia.de\n",
    "<br>\n",
    "**Last Update**: 06 June 2024\n",
    "<br>\n",
    "**Compatible JWST Pipeline Versions**: >= 1.13.0 (tested by Valentin up to v1.14.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d988f765",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to provide a framework for batch processing of MIRI MRS data acquired as part of the MINDS project through all three pipeline stages. Input data for this notebook can be obtained by downloading the 'uncal' files from MAST and placing them in directories defined below. Data are assumed to be located in one or two observation folders (science, and background if available) according to paths set up below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cd26b0",
   "metadata": {},
   "source": [
    "<font color='white'>-</font> **General comments on the notebook:** <a class=\"anchor\" id=\"det1\"></a>                                       <div class=\"alert alert-block alert-warning\"> To not clutter the notebook with additional custom-made routines for background subtraction, bad pixel correction, continuum subtraction, multiprocessing or spectrum extraction, these are collated in **5** separate modules: **utils_bkg.py**, **utils_bpc.py**, **utils_cs.py**, **utils_mp.py** and **utils_phot.py**, respectively. The routines either depend on the [VIP](https://vip.readthedocs.io/) package, or inspired of routines therein.\n",
    "    \n",
    "Final flux errors in output x1d files include **Poisson noise** (propagated from the official pipeline). In the case ``bkg_method`` is set to 'sdither', also includes **background noise** (through estimated background maps propagated to stage 3), combined in quadrature. These should still be considered as lower limits on the final uncertainties (which should consider other sources of uncertainty not yet propagated by the jwst pipeline). Background noise is estimated as the standard deviation of measured photometry in apertures of the same size as used for source spectrum extraction, but on the stage 3 spectral cubes corresponding to propagated background maps.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac4c79e",
   "metadata": {},
   "source": [
    "<font color='white'>-</font> **Disclaimer on PSFF reduction (added by Danny):** <a class=\"anchor\" id=\"det1\"></a>                                       <div class=\"alert alert-block alert-warning\"> Set *psff=True* when you wish to run the data reduction specifically for point sources, if *target acquisition* was on at the beginning of the observation. Using this option includes a dedicated fringe flat, which does not require the additional residual fringe correction (which might otherwise alter molecular features), and a clean spectrophotometric calibration. Scaling of individual spectra should not be required to get a continuous result. It will remove the spectral leak at 12.2 micron. **The accompanying reference files should be present in the same location as the notebook, in the folder \"psff_ref\".** <br>\n",
    "A few caveats regarding the current version:\n",
    "* It is specifically for point sources in the point source-specific dither patterns (positive or negative), and will not perform well for more extended targets or a different dither pattern.\n",
    "* It relies on the pointing being repeatable, something which we have already seen to not be the case in channel 1. It may be that you will still have residual fringes in this part of the spectrum, though this will be different per target.\n",
    "* Since the current version of the fringe flat is directly taken from a reference star (an A star), and I have not yet cleaned it up, there may be some features that are introduced because of this (e.g. H lines). These will likely be spikes, and very clearly not real. You can safely blank these out of your spectrum.\n",
    "\n",
    "To assess the reliability of given spectral features, **it is highly recommended to also run the notebook with  *psff=False***, and compare the resulting spectra.\n",
    "    \n",
    "When using this version of the data reduction in your paper (`psff=True`), **please cite [Gasman et al. (2023)](https://ui.adsabs.harvard.edu/abs/2023A%26A...673A.102G/abstract)**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb9b0c7",
   "metadata": {},
   "source": [
    "<font color='white'>-</font> **Comments about v8 (added by Valentin):** <a class=\"anchor\" id=\"det1\"></a>                                       <div class=\"alert alert-block alert-warning\">\n",
    "    <b>What has changed compared to v007b?</b>\n",
    "    <br>- The `pixel_replace` step of the `jwst` pipeline is now systematically included as it leads to cleaner extracted spectra. The default behaviour is to use the 'mingrad' method, but this can be changed through the `pixel_replace_algo` parameter (which can also take 'fit_profile'). By default, VIP-based bad pixel identification/correction after stage 2 is still on, and `outlier_detection` step at stage 3 still turned off, based on tests for the extraction of the spectrum of PDS 70. Some clumps of bad pixels leading to spikes in a few channels of the final spectra are indeed not yet captured with neither `pixel_replace` nor `outlier_detection` - but well captured and corrected by the VIP-based algorithm. For a faster reduction, the VIP-based correction can however be turned off by setting `do_bpc2=False`.\n",
    "    <br>- In terms of discarded spectral channels, the default behaviour now is to keep the red end of each band, and remove the overlapping channels from the blue end of each subsequent band (i.e. starting from the second band). The option of setting manually the number of channels to be removed at the blue or red end of each band is still available.\n",
    "    <br>- Possibility to provide additional optional parameters to the extract1D function used to extract the spectrum at MINDS-notebook inferred centroid location, such as the aperture size (`apsize`, i.e. not limited to psff case anymore), \n",
    "    <br>- The spectral leak fix is now turned on by default, even when psff=False. It can be skipped by setting `do_leak_correction=False`.\n",
    "    <br>- The determination of the centroid position for faint sources (in a weighted band-average frame) is now more robust towards NaN values or negative background values. By default, centroid is not based on the pipeline `autocen` feature, but the latter option can be useed by setting `autocen=True`.\n",
    "    <br>- Inclusion of Milou's continuum subtraction code. By default, a continuum-subtracted version of the extracted spectrum is also saved (set by `contsub=True`).\n",
    "    <br>- Optional dependency to `ray` added, for better multiprocessing if the package is installed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa3aa8e",
   "metadata": {},
   "source": [
    "<font color='white'>-</font> **Comments about v8b (added by Valentin):** <a class=\"anchor\" id=\"det1\"></a>                                       <div class=\"alert alert-block alert-warning\">\n",
    "    <b>What has changed compared to v008?</b>\n",
    "    <br>- The residual fringe correction done at the spectrum level is now only based on the jwst pipeline implementation, and is part of the extract1D step (i.e. it may change and be futher improved over time), while until v8 the correction was based on a separate module provided by P. Kavanagh before its inclusion in the jwst pipeline.\n",
    "    <br>- All dependencies are collected within a single requirements.txt file, including version constraints, which makes their installation more convenient.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7cc2ef",
   "metadata": {},
   "source": [
    "<font color='white'>-</font> **Comments about v8c (added by Valentin):** <a class=\"anchor\" id=\"det1\"></a>                                       <div class=\"alert alert-block alert-warning\">\n",
    "    <b>What has changed compared to v008b?</b>\n",
    "    <br>- An option can be set to select whether 1d residual fringe correction should be done before or after spectral leak correction. By default, it is done before, as in the official pipeline. In some cases (e.g. PDS 70), it can better remove the spurious bump of the spectral leak if the leak correction is done before rfc1d.\n",
    "    <br>- Bug correction: the stitched 1d residual fringe correction spectrum was not properly saved in the csv file (wrong variable was being saved).\n",
    "    <br>- Minor bug corrections and additional option propagated in the continuum subtraction utility: (i) more robust identification of spikes in Baseline() function; (ii) option for baseline estimation with smaller spacing in CH4 now properly implemented; (iii) new option to interpolate the estimated baseline between CH3 and CH4 when a different baseline spacing is used for CH4. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb34011",
   "metadata": {},
   "source": [
    "<font color='white'>-</font> **Comments on fringing:** <a class=\"anchor\" id=\"det1\"></a>                                       <div class=\"alert alert-block alert-warning\"> As is common in IR detectors, effects in the MIRI MRS detectors results in the input signal being modulated as a function of wavelength, so-called 'fringing'. Two fringe components have been identified in MRS data, the primary fringe originating in the detector substrate (see Argyriou et. al, 2020 A&A, 641, 150) and a second high frequency, low amplitude fringe in channels 3 and 4, thought to originate in the MRS dichroics. \n",
    "\n",
    "The JWST calibration pipeline contains two steps to remove fringes from MRS data. These are the *fringe* step in Spec2Pipeline and the *residual_fringe* step which should be run before cube building at the level 3 stage. The *fringe* step divides the detector level data by a static fringe flat derived from an extended source for the primary fringe component only. However, this can leave residuals for either spatially unresolved sources or a source with spatial structure. These residuals are corrected for by the *residual_fringe* step on detector level data, which also attemps to correct for the dichroic fringe in channels 3 and 4.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d9868",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7befc42c",
   "metadata": {},
   "source": [
    "**Table of contents**\n",
    "\n",
    "* [0. Installation of dependencies](#0.-Installation-of-dependencies) \n",
    "* [1. Configuration of reduction parameters](#1.-Configuration-of-reduction-parameters)\n",
    "    - [1.1. **Create a source list**](#1.1.-Create-a-source-list)\n",
    "    - [1.2. Data reduction parameters setup](#1.2.-Data-reduction-parameters-setup)\n",
    "* [2. Imports and setup](#2.-Imports-and-setup)    \n",
    "* [3. Stage 1 processing](#3.-Stage-1-processing)\n",
    "    - [3.1. Det1 Pipeline](#3.1.-Det1-Pipeline)\n",
    "    - [3.2. Straylight correction](#3.2.-Straylight-correction)\n",
    "    - [3.3. Background subtraction](#3.3.-Background-subtraction)\n",
    "    - [3.4. Optional bad pixel identification](#3.4.-Bad-pixel-identification)\n",
    "* [4. Stage 2](#4.-Stage-2)\n",
    "    - [4.1. **Spec2 Pipeline**](#4.1.-Spec2-Pipeline)\n",
    "    - [4.2. Bad pixel correction](#4.2.-Bad-pixel-correction)\n",
    "* [5. Stage 3](#5.-Stage-3)\n",
    "    - [5.1. **Spec3 Pipeline**](#5.1.-Spec3-Pipeline)\n",
    "    - [5.2. Optional bad pixel correction](#5.2.-Optional-bad-pixel-correction)\n",
    "    - [5.3. Spectrum extraction](#5.3.-Spectrum-extraction)\n",
    "        - [5.3.1 Centering](#5.3.1.-Centering)\n",
    "        - [5.3.2 **Residual background subtraction**](#5.3.2.-Residual-background-subtraction)\n",
    "        - [5.3.3 Aperture photometry + spike filtering + aperture correction](#5.3.3.-Aperture-photometry-+-spike-filtering)\n",
    "        - [5.3.4 **Spectral leak fix in band 3A**](#5.3.4.-Spectral-leak-fix-in-band-3A)\n",
    "    - [5.4. Stitch and write results in final directory](#5.4.-Stitch-and-write-results-in-final-directory)\n",
    "    - [5.5. **Continuum subtraction**](#5.5.-Continuum-subtraction) \n",
    "* [6. Plots](#6.-Plots)\n",
    "    - [6.1. Plots with DataFrames](#6.1.-Plots-with-DataFrames)\n",
    "    - [6.2. Comparison plots](#6.2.-Comparison-plots) \n",
    "    - [6.3. **Final plot**](#6.3.-Final-plot)\n",
    "    - [6.4. Aperture image](#6.4.-Aperture-image)\n",
    "\n",
    "*Note: Bold face corresponds to changes or new additions compared to notebook v007b.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f815dc1f",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ec45d0",
   "metadata": {},
   "source": [
    "# 0. Installation of dependencies\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163384df",
   "metadata": {},
   "source": [
    "Since this new pipeline makes use of custom-made tools besides the jwst pipeline, a couple of additional dependencies should be installed.\n",
    "Running the following commands in your terminal *should* get you all set up:\n",
    "\n",
    "\n",
    "<font color='white'>-</font> **Disclaimer (added by Valentin):** <a class=\"anchor\" id=\"det1\"></a>                                       <div class=\"alert alert-block alert-warning\"> For safety, it is recommended to make a new conda environment for the MINDS pipeline, to not mess up an environment in which your jwst pipeline works flawlessly. The series of commands below in the provided order works on my machine, and *should* hopefully work on yours. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1325b324",
   "metadata": {},
   "source": [
    "`conda create -n minds_env python=3.11 ipython jupyter` (replace ``minds_env`` with the name you want for your conda environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f5dd8d",
   "metadata": {},
   "source": [
    "`pip install git+https://github.com/VChristiaens/MINDS.git`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2504e509",
   "metadata": {},
   "source": [
    "`pip install -U \"ray[default]\"`  (optional, better multiprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4894b92",
   "metadata": {},
   "source": [
    "# 1. Configuration of reduction parameters\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d47f511",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"> \n",
    "    <b>Important: </b>\n",
    "<a class=\"anchor\" id=\"note1\"></a>                                       \n",
    "If you want to run this notebook in <b>multiprocessing</b>, it is recommended to run the following 3 lines in your terminal <b>before</b> launching this jupyter notebook from the same terminal:\n",
    "\n",
    "`export MKL_NUM_THREADS=1`\n",
    "\n",
    "`export NUMEXPR_NUM_THREADS=1`\n",
    "\n",
    "`export OMP_NUM_THREADS=1`\n",
    "\n",
    "`export OPENBLAS_NUM_THREADS=1`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbc937",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# UNCOMMENT 4 lines below ONLY IF you encounter a CRDS_PATH related bug (which can happen the first time you run this notebook)\n",
    "# import pathlib\n",
    "# ROOT_DIR = pathlib.Path('~/')\n",
    "# os.environ[\"CRDS_PATH\"] = str(ROOT_DIR / \"crds_cache/\")\n",
    "# os.environ[\"CRDS_SERVER_URL\"] = \"https://jwst-crds.stsci.edu\"\n",
    "\n",
    "# set the jwst pipeline crds context (leave blank to use latest)\n",
    "os.environ[\"CRDS_CONTEXT\"] = \"\" # either leave blank or refer to a pmap file version >= 1094 (e.g. os.environ[\"CRDS_CONTEXT\"] = \"jwst_1235.pmap\")\n",
    "# note from Yannis: if the above does not work, try uncommenting the line below:\n",
    "# %env CRDS_CONTEXT jwst_1190.pmap\n",
    "\n",
    "# set environmental variable to prevent numpy from using multiprocessing outside of the intended parallelization.\n",
    "# EDIT: unclear whether it works from within the notebook.\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "# os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# Do not touch\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import sys, pdb\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6be5cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sources:\n",
    "    def __init__(self,number_of_sources=0, source_names = [], input_dirs = [], output_dirs = [], input_bkg_dirs = [], output_bkg_dirs = []):\n",
    "        self.number_of_sources = number_of_sources\n",
    "        self.source_names = source_names\n",
    "        self.input_dirs = input_dirs\n",
    "        self.input_bkg_dirs = input_bkg_dirs\n",
    "        self.output_dirs = output_dirs\n",
    "        self.output_bkg_dirs = output_bkg_dirs\n",
    "        if self.number_of_sources>0:\n",
    "            if isinstance(self.source_names,list):\n",
    "                if len(self.source_names)!=self.number_of_sources:\n",
    "                    print('Warning: Number of sources and number of source names do not match, adding empty names')\n",
    "                    for i in range(self.number_of_sources-len(self.source_names)):\n",
    "                        self.source_names.append('')\n",
    "            if isinstance(self.output_dirs,list):\n",
    "                if len(self.output_dirs)!=self.number_of_sources:\n",
    "                    print('Warning: Number of sources and number of output directories do not match, adding empty names')\n",
    "                    for i in range(self.number_of_sources-len(self.output_dirs)):\n",
    "                        self.output_dirs.append('')\n",
    "            if isinstance(self.input_dirs,list):\n",
    "                if len(self.input_dirs)!=self.number_of_sources:\n",
    "                    print('Warning: Number of sources and number of input directories do not match, adding empty names')\n",
    "                    for i in range(self.number_of_sources-len(self.input_dirs)):\n",
    "                        self.input_dirs.append('')\n",
    "            if isinstance(self.output_bkg_dirs,list):\n",
    "                if len(self.output_bkg_dirs)!=self.number_of_sources:\n",
    "                    print('Warning: Number of sources and number of output bkg directories do not match, adding empty names')\n",
    "                    for i in range(self.number_of_sources-len(self.output_bkg_dirs)):\n",
    "                        self.output_bkg_dirs.append('')\n",
    "            if isinstance(self.input_bkg_dirs,list):\n",
    "                if len(self.input_bkg_dirs)!=self.number_of_sources:\n",
    "                    print('Warning: Number of sources and number of input bkg directories do not match, adding empty names')\n",
    "                    for i in range(self.number_of_sources-len(self.input_bkg_dirs)):\n",
    "                        self.input_bkg_dirs.append('')\n",
    "                        \n",
    "                        \n",
    "        self.final_output_dirs       = output_dirs*1\n",
    "        self.det1_dirs               = output_dirs*1\n",
    "        self.det1_dirs_ori           = [None for i in range(self.number_of_sources)]\n",
    "        self.det1_bkg_dirs           = output_dirs*1\n",
    "        self.spec2_dirs              = output_dirs*1\n",
    "        self.spec2_bkg_dirs          = output_dirs*1\n",
    "        self.spec3_dirs              = output_dirs*1\n",
    "        self.fin3_dirs               = output_dirs*1\n",
    "        self.figs_dirs               = output_dirs*1\n",
    "        self.asnfiles                = [None]*self.number_of_sources \n",
    "        self.asnfiles_ifua           = [None]*self.number_of_sources \n",
    "        self.asnfiles_ifua_rfc1d     = [None]*self.number_of_sources \n",
    "        self.outnames                = [None]*self.number_of_sources \n",
    "        self.outnames_ifua           = [None]*self.number_of_sources \n",
    "        self.outnames_ifua_rfc1d     = [None]*self.number_of_sources \n",
    "        self.cxy_med_ifua            = [None]*self.number_of_sources\n",
    "        self.cxy_med                 = [None]*self.number_of_sources\n",
    "        self.l3asn_dirs              = [None]*self.number_of_sources\n",
    "        self.calfiles_x1d_rfc1d      = [None]*self.number_of_sources\n",
    "        self.calfiles_x1d            = [None]*self.number_of_sources\n",
    "        self.spectrum_df             = [None]*self.number_of_sources\n",
    "        self.spectrum_df_vanilla_pip = [None]*self.number_of_sources\n",
    "        self.all_wavelength          = [None for i in range(self.number_of_sources)]\n",
    "        self.all_corrected_flux      = [None for i in range(self.number_of_sources)]\n",
    "\n",
    "    def __str__(self):\n",
    "        output = 'Source directories: \\n'\n",
    "        output += f'Number of sources: {self.number_of_sources}\\n'\n",
    "        for i in range(self.number_of_sources):\n",
    "            output += f'\\nSource {i+1}:\\n'\n",
    "            output += f'Name: {self.source_names[i]}\\n'\n",
    "            output += f'Input dir: {self.input_dirs[i]}\\n'\n",
    "            output += f'Input bkg dir: {self.input_bkg_dirs[i]}\\n'\n",
    "            output += f'Output dir: {self.output_dirs[i]}\\n'\n",
    "            output += f'Output bkg dir: {self.output_bkg_dirs[i]}\\n'\n",
    "        return(output)\n",
    "    \n",
    "    def set_number_of_sources(self,num=None):\n",
    "        if not isinstance(num,int):\n",
    "            return(self.number_of_sources)\n",
    "        else:\n",
    "            self.number_of_sources = num\n",
    "            self.source_names = ['']*num\n",
    "            self.input_dirs   = ['']*num\n",
    "            self.output_dirs  = ['']*num\n",
    "            self.input_bkg_dirs   = ['']*num\n",
    "            self.output_bkg_dirs  = ['']*num\n",
    "    \n",
    "    def add_source(self,source_name='',input_dir='',output_dir='',input_bkg_dir='',output_bkg_dir=''):\n",
    "        self.number_of_sources += 1\n",
    "        self.source_names.append(source_name)\n",
    "        self.input_dirs.append(input_dir)\n",
    "        self.output_dirs.append(output_dir)\n",
    "        self.input_bkg_dirs.append(input_bkg_dir)\n",
    "        self.output_bkg_dirs.append(output_bkg_dir)\n",
    "        self.final_output_dirs.append(output_dir)\n",
    "        self.det1_dirs.append(output_dir)\n",
    "        self.det1_dirs_ori.append(None)\n",
    "        self.det1_bkg_dirs.append(output_dir)\n",
    "        self.spec2_dirs.append(output_dir)\n",
    "        self.spec2_bkg_dirs.append(output_dir)\n",
    "        self.spec3_dirs.append(output_dir)\n",
    "        self.fin3_dirs.append(output_dir)\n",
    "        self.figs_dirs.append(output_dir)\n",
    "        self.asnfiles.append([None])\n",
    "        self.asnfiles_ifua.append([None])\n",
    "        self.asnfiles_ifua_rfc1d.append([None])\n",
    "        self.outnames.append([None])\n",
    "        self.outnames_ifua.append([None])\n",
    "        self.outnames_ifua_rfc1d.append([None])\n",
    "        self.cxy_med_ifua.append([None])\n",
    "        self.cxy_med.append([None])\n",
    "        self.l3asn_dirs.append([None])\n",
    "        self.calfiles_x1d_rfc1d.append([None])\n",
    "        self.calfiles_x1d.append([None])\n",
    "        self.spectrum_df.append([None])\n",
    "        self.spectrum_df_vanilla_pip.append([None])\n",
    "        self.all_wavelength.append(None)\n",
    "        self.all_corrected_flux.append(None)\n",
    "        \n",
    "    def set_final_out_dir(self,ver,bg_observation,final_output_subdir,det1_subdir,spec2_subdir,spec3_subdir,fin3_subdir,figs_subdir):\n",
    "        for i in range(self.number_of_sources):\n",
    "            self.final_output_dirs[i] += final_output_subdir\n",
    "            self.det1_dirs[i]          = join(self.output_dirs[i],       det1_subdir)      # Detector1 pipeline outputs will go here\n",
    "            self.spec2_dirs[i]         = join(self.output_dirs[i],       spec2_subdir)     # Spec2 pipeline outputs will go here\n",
    "            self.spec3_dirs[i]         = join(self.output_dirs[i],       spec3_subdir)     # Spec3 pipeline outputs will go here\n",
    "            self.fin3_dirs[i]          = join(self.final_output_dirs[i], fin3_subdir)      # Final Spec3 pipeline outputs will go here (final cubes and spectra)\n",
    "            self.figs_dirs[i]          = join(self.final_output_dirs[i], figs_subdir)      # whether the figures will be save\n",
    "            if bg_observation:\n",
    "                self.det1_bkg_dirs[i]      = join(self.output_bkg_dirs[i],   det1_subdir)  # Detector1 pipeline outputs will go here\n",
    "                self.spec2_bkg_dirs[i]     = join(self.output_bkg_dirs[i],   spec2_subdir) # Spec2 pipeline outputs will go here\n",
    "            else:\n",
    "                self.det1_bkg_dirs[i]      = None\n",
    "                self.spec2_bkg_dirs[i]     = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afabaec2",
   "metadata": {},
   "source": [
    "## 1.1. Create a source list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ac222",
   "metadata": {},
   "source": [
    "Two ways of doing it, use whichever is easier for you <br>\n",
    "Option 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a6049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_list = sources(number_of_sources=3,\n",
    "                      source_names=['J0439','TWA27','J1558'],\n",
    "                      input_dirs=['/Users/users/arabhavi/PhD/MINDS/data_reduction/data_download/2MASS-J04390163+2336029/mastDownload/JWST/',\n",
    "                                  '/Users/users/arabhavi/PhD/MINDS/data_reduction/data_download/2MASS-J12073346-3932539/mastDownload/JWST/',\n",
    "                                  '/Users/users/arabhavi/PhD/MINDS/data_reduction/data_download/2MASS-J15582981-2310077/mastDownload/JWST/'],\n",
    "                      output_dirs=['/Users/users/arabhavi/PhD/MINDS/data_reduction/multireduce/test/J0439/',\n",
    "                                   '/Users/users/arabhavi/PhD/MINDS/data_reduction/multireduce/test/TWA27/',\n",
    "                                   '/Users/users/arabhavi/PhD/MINDS/data_reduction/multireduce/test/J1558/'])\n",
    "print(source_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a111d83a",
   "metadata": {},
   "source": [
    "Option 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_list = sources()\n",
    "\n",
    "source_list.add_source(source_name='J0439',\n",
    "                       input_dir='/Users/users/arabhavi/PhD/MINDS/data_reduction/data_download/2MASS-J04390163+2336029/mastDownload/JWST/',\n",
    "                       output_dir='/Users/users/arabhavi/PhD/MINDS/data_reduction/multireduce/test/J0439/')\n",
    "\n",
    "source_list.add_source(source_name='TWA27',\n",
    "                       input_dir='/Users/users/arabhavi/PhD/MINDS/data_reduction/data_download/2MASS-J12073346-3932539/mastDownload/JWST/',\n",
    "                       output_dir='/Users/users/arabhavi/PhD/MINDS/data_reduction/multireduce/test/TWA27/')\n",
    "\n",
    "source_list.add_source(source_name='J1558',\n",
    "                       input_dir='/Users/users/arabhavi/PhD/MINDS/data_reduction/data_download/2MASS-J15582981-2310077/mastDownload/JWST/',\n",
    "                       output_dir='/Users/users/arabhavi/PhD/MINDS/data_reduction/multireduce/test/J1558/')\n",
    "\n",
    "print(source_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557fab6a",
   "metadata": {},
   "source": [
    "## 1.2. Data reduction parameters setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245d80cb",
   "metadata": {},
   "source": [
    "Set dataset-dependent parameters in the next box. It *should* not be necessary to edit cells beyond this one in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afa22f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "# Sources reduced with v1.0.2: 2MASS-J0438, 2MASS-J1605, AA_Tau, DM_Tau, PDS70, SY_Cha\n",
    "# Sources reduced with v1.0.3: PDS70\n",
    "\n",
    "# det1_dir_ori = None #join(output_dir, 'stage1{}/'.format('_MINDSv1.0.3a'))      #e.g.: join(output_dir, 'stage1{}/'.format('_MINDSv1.0.2')) #If you ALREADY have rate files, and want to skip running Detector1 stage, which is a bit tedious, just provide the path to your rate files here, and set overwrite_31=False below - this would then skip Detector1 step and consider the rate files in the folder provided here.\n",
    "\n",
    "# Whether or not to run a given pipeline stage, and overwrite its products?\n",
    "do_sec3 = False # Det1 Pipeline Stage & associated steps\n",
    "overwrite_31=False # Overwrite output of section 3.1? False will skip section 3.1 if outputs already exist, otherwise will run it - as long as do_sec3 is True\n",
    "overwrite_32=False # Overwrite output of section 3.2? False will skip section 3.2 if outputs already exist, otherwise will run it - as long as do_sec3 is True\n",
    "overwrite_33=False # Overwrite output of section 3.3? False will skip section 3.3 if outputs already exist, otherwise will run it - as long as do_sec3 is Truenoddin\n",
    "overwrite_34=False # Overwrite output of section 3.4? False will skip section 3.4 if outputs already exist, otherwise will run it - as long as do_sec3 is True\n",
    "do_sec4 = False # Spec2 Pipeline Stage & associated steps\n",
    "overwrite_41=False # Overwrite output of section 4.1? False will skip section 4.1 if outputs already exist, otherwise will run it - as long as do_sec4 is True\n",
    "overwrite_42=False # Overwrite output of section 4.2? False will skip section 4.2 if outputs already exist, otherwise will run it - as long as do_sec4 is True\n",
    "do_sec5 = True # Spec3 Pipeline Stage & associated steps\n",
    "overwrite_51=False # Overwrite output of section 5.1? False will skip section 5.1 if outputs already exist, otherwise will run it - as long as do_sec5 is True\n",
    "overwrite_52=False # Overwrite output of section 5.2? False will skip section 5.2 if outputs already exist, otherwise will run it - as long as do_sec5 is True\n",
    "overwrite_53=True # Overwrite output of section 5.3? False will skip section 5.3 if outputs already exist, otherwise will run it - as long as do_sec5 is True\n",
    "# note: there is no overwrite_54, as Sec. 5.4 on corresponds to the last part of the notebook with low amount of data processing involved, it is thus always True by default.\n",
    "\n",
    "# Multiprocessing: \n",
    "usage = None      # {None, 'quarter', 'half', 'all'}. Takes precedence over maxp.\n",
    "if usage is None:\n",
    "    maxp = 1      # [if 'usage' is None] Set the maximum number of CPUs to use.\n",
    "\n",
    "# Background subtraction - params below are first used in Sec 3.3\n",
    "bkg_method = 'annulus'  # {'sdither', 'ddither', 'annulus'} Method used for background subtraction. 'sdither' (median-filtered smoothed background of minimum of 4 dithers; default), 'ddither' (direct corresponding dither subtraction; needs a change of default apsize value from default), or 'annulus' (annular subtraction during aperture photometry)\n",
    "subtract_residual_bkg = True  # whether to subtract residual background level estimated in an annulus before extracting the final spectra and writing the final s3d cubes (can make a difference for long wavelengths)\n",
    "bg_observation = False  # whether dedicated background observations or other reference observations are available to estimate the BKG. If True, these should be provided through the 'input_bgdir' parameter. If False, the pipeline will assume a dithering strategy was followed and will estimate the BKG from the dithered images themselves.\n",
    "dedicated_bg = False    # [only relevant if bg_observation = True & bkg_method='ddither'] If provided, does the background proxy correspond to a dedicated (empty field) background [True], or to another reference (faint star) observation [False]?\n",
    "rm_persist = False      # [if bkg_method = 'sdither'] Whether to correct for detector persistence effects during background subtraction. Note: this is a very simplistic approach, but can help reduce the effect of persistence if looking for extended signals - as there is currently no method in the official pipeline to deal with it (as of 1.11)\n",
    "\n",
    "# Suffix\n",
    "ver = '_MINDSv1.0.3a'  # suffix you'd like to add to output directories to better identify your different reductions\n",
    "if bkg_method != 'annulus': # add the selected background subtraction method to suffix when not the default one\n",
    "    ver += '_{}'.format(bkg_method)\n",
    "\n",
    "# Point source optimized reference files - param below are first used in Sec 4.1\n",
    "psff = False    # [KEEP AS FALSE - NOT USABLE FOR NOW as appropriate time correction of photometry is missing] Run point source optimized reduction using Danny's reference files?  # Only applicable for point sources and observations where target acquisition was ON. (Needs re-running the notebooks at least from Sec. 4.1 to test it) \n",
    "\n",
    "# Spectrum extraction + spike filtering - params below are first used in Sec 5.3\n",
    "do_leak_correction = True # whether to correct, regardless of psff=True.\n",
    "apsize = 2.0              # Float setting the size of the aperture in FWHM. [if psff=False] Can take any value. [If psff=True] Can only take value in {1.5, 2.0, 2.5, 3.0, 3.5} (recommended: 2.0). \n",
    "if bkg_method == 'ddither' and not (bg_observation and dedicated_bg):\n",
    "    apsize = 1.5          # force aperture to be smaller in case of direct-dither subtraction (to minimize self-subtraction)\n",
    "## Extract1d parameters\n",
    "autocen = False           # [only used if psff=False]: whether [True] to use the 'autocen' feature of the pipeline, or [False] the centroid inferred in a weighted average frame for each band, where weights are proportional to the integrated flux (i.e., robust to faint sources)\n",
    "overwrite_target_classification = True  # [only used if psff=False] Whether to overwrite target classification in header. This will influence the behaviour of default extract1d routine (which is activated if psff=False) at the moment of extracting the spectrum: for an extended source, all spaxels are integrated, while for a point source, only spaxels in a small aperture are used.\n",
    "new_sourcetype = 'POINT'  # {'POINT','EXTENDED'} [if psff=False & overwrite_target_classification=True] The new source type, if target classification is requested to change.\n",
    "## spike filtering parameters [only if psff = True]\n",
    "spike_filtering = False   # Whether to correct for residual spikes after stage 3, when extracting spectrum. Can be a single boolean or a list of 12 booleans (1 for each band). The spectrum of spaxels included in the aperture photometry are checked successively and outliers occurring on single spaxels are corrected. This spike filtering can help reduce spikes, but beware that \"spike-like\" real spectral features may be erroneously identified as outliers.\n",
    "sp_sig = 5                # [if spike_filtering = True] Sigma used to identify spikes by sigma filtering of individual spaxel spectra in the aperture photometry aperture. Can be a single float/int or a list of 12 floats/ints.\n",
    "max_nspax = 2             # [if spike_filtering = True] Max. number of spaxels identified as spectral outliers (with sp_sig criterion) for a given spaxel at a given wavelength to be considered a spike. The idea is that spikes should occur locally, and not on several spaxels - in the latter case it is more likely an authentic spectral feature. \n",
    "neg_only = True           # [if spike_filtering = True] Whether to correct only negative spikes with the spike filtering algorithm.\n",
    "\n",
    "# Operations on spectra - params below are first used in Sec 5.4\n",
    "do_rfc1d = True         # whether to consider residual fringe correction at the spectrum level. RFC is NOT recommeded if psff is True, as it shouldn't be necessary and may suppress molecular features. BUT in some cases (e.g. at least for PDS 70) this is useful to remove residual fringe effects.\n",
    "do_rfc1d_bef_leak = True# [do_leak_correction=True & do_rfc1d=True] If both leak correction and do_rfc1d are requested, whether to do_rfc1d first (as in the official pipeline), or the opposite (False). In some cases, the opposite order can lead to a lower residual bump.\n",
    "nch_cut_per_band = \"no overlap - cut blue ends\" # {None, \"no overlap - cut blue ends\", \"no overlap - cut red ends\", int, list of int, or list of tuples of int}. Determine how to cut the channels at the edge of each band.\n",
    "    # None or 0 -> no removal of spectral channels at the blue and red ends of each band.\n",
    "    # \"no overlap - cut blue ends\": Removes exact number of spectral channels at the blue end of each band (starting with the 2nd one) to avoid any overlap with the red end of the previous band\n",
    "    # \"no overlap - cut red ends\": Removes exact number of spectral channels at the red end of each band (except the last one) to avoid any overlap with the blue end of the subsequent band\n",
    "    # int: same number of channels to be removed at both the blue and red ends of all bands.\n",
    "    # list of 12 int: Respective number of channels to remove at both the blue and red ends of each band \n",
    "    # list of 12 tuples of 2 int: Number of channels to remove at the blue and red end (tuples), respectively, of each band (list).\n",
    "correct_jumps = None # Whether and how to correct for jumps between bands.\n",
    "# if the above is not satisfactory, set: do_rfc1d = True and correct_jumps = 'occam' (or 'scale_only')\n",
    "#correct_jumps arguments: {'scale_only', 'occam', None}\n",
    "    # 'scale_only': allows scaling of bands such that their shortward section have a similar level as the overlapping longward section of previous band above;\n",
    "    # 'occam': either (i) 'scale_only' above, or (ii) correct for slope of the band to match neighbouring band levels. Default behaviour is (i) EXCEPT if this leads to all subsequent bands to require a similar scaling (e.g. third band requires 1.10 factor to match second band, BUT all subsequent bands also require ~1.10 factor => Occam Razor: it's more likely that the slope of third band is wrong, while all subsequent bands could be well calibrated).\n",
    "    # None: no correction for jumps\n",
    "    \n",
    "# Continuum subtraction - params below are first used in Sec 5.5\n",
    "contsub = True              # Whether to also save a continuum-subtracted version of the spectrum.\n",
    "mask_12mu_feature = True    # Whether to mask a feature between 12.118µm and 12.132µm when estimating the continuum. If True, this region of the spectrum is interpolated for continuum estimation.\n",
    "## Dictionary of parameters used in the Baseline function used to estimate the continuum:\n",
    "bl_params = {'WL' : 100, # window length\n",
    "             'NKf' : 75, # frequency of knots for all bands but Ch.4, such that input number of knots: NK = len(band)/NKf\n",
    "             'NKf_ch4' : 25, # frequency of knots for Ch.4, such that input number of knots: NK = len(band)/NKf\n",
    "             'Quant' : 0.05, # quantile used to estimate where the continuum is located\n",
    "             'Quant_ch4' : 0.1, # quantile used to estimate where the continuum is located, in Ch. 4\n",
    "             'IP_ch4' : False, # if NKf_ch4 != NKf: whether to smooth the estimated continuum between CH3 and CH4. If False can lead to a jump, but if True can lead to poorer continuum estimate near the transition between CH3 and CH4.\n",
    "             'SD' : 3, # spline_degree\n",
    "             'DO' : 3, # diff_order\n",
    "             'MI' : 9999, # max iter\n",
    "             'Lam' : 1e2, # lam\n",
    "             'Tol' : 1e-9, # tolerance\n",
    "             'sigma_clip' : (3,4), # upper and lower sigma clip thresholds. By default, outliers are clipped at +2sigma and -3sigma. \n",
    "             'W_max_out' : 27.8}  # max wavelength beyond which the spectrum is discarded for the calculation of STD enabling the identification of outliers.\n",
    "# PLOTS - params below are always used (i.e. used in Sec. 6 which is always run)\n",
    "show_plots = True # whether to show all plots\n",
    "interactive_plots = True # whether to make the plots interactive\n",
    "wl_min=4.5        # min wavelength for plots (sec 6.2. and later)\n",
    "wl_max=27.       # max wavelength for plots (sec 6.2. and later)\n",
    "DN_count_file = None # Path+filename of an optional csv file which compiles the max. DN count measured in each band for different sources. Will add a row to that table. Such table can be useful to know which observations to use for reference star differential imaging PSF subtraction, since the BFE depends on the max DN count.\n",
    "spitzer_file = None  # join(work_dir,\"PDS_70_-IRS_combined_spectrum_v2.tbl\") # Path+filename of Spitzer spectrum. Assumes it is parsable by pandas.read_csv, and contains columns labelled \"lambda\" and \"Flux\"\n",
    "photom_file = None   # join(work_dir,\"photom_pds70.txt\")  # Path+filename of additional photometry points for comparison. Default: should be parsable with pandas.read_csv, with 4 columns (wavelength, flux (Jy), flux error, instrument name)\n",
    "compare_spec = True          # whether to also plot other spectra obtained with different reduction parameters (i.e. present in the same output folder).\n",
    "make_aperture_image = True  # whether to create a figure showing the aperture size with respect to a spectral image at a given wavelength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04ed9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================================================================\n",
    "# Import packages for multiprocessing.  These won't be used on the online demo, but can be\n",
    "# very useful for local data processing unless/until they get integrated natively into\n",
    "# the cube building code.\n",
    "    \n",
    "num_cores = multiprocessing.cpu_count()\n",
    "print('number of cores', num_cores)\n",
    "if usage == 'quarter':\n",
    "    maxp = num_cores // 4 or 1\n",
    "elif usage == 'half':\n",
    "    maxp = num_cores // 2 or 1\n",
    "elif usage == 'all':\n",
    "    maxp = num_cores\n",
    "    \n",
    "print('We will use '+str(maxp)+' CPUs in this run')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d0cfdf",
   "metadata": {},
   "source": [
    "TLDR recommended settings:\n",
    "\n",
    "    - Dedicated background observation available?\n",
    "        => set: input_bgdir, output_bgdir, bg_observation=True, dedicated_bg=True, the rest as default.\n",
    "    \n",
    "    - No dedicated background observation BUT observation of a faint object to be used as background proxy?\n",
    "        => set: input_bgdir, output_bgdir, bg_observation=True, dedicated_bg=False, the rest as default.\n",
    "        \n",
    "    - Dither-based background estimation and subtraction?\n",
    "        => set: bg_observation=False, the rest as default."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1992e2",
   "metadata": {},
   "source": [
    "Additional parameters that can be adapted - although tests so far suggest it is unlikely you'll need to change their value: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885a36c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Det1Pipeline parameters to exclude more frames in the beginning, which may lead to less striping, but depends on number of groups available.\n",
    "mask_ramps = False # whether to mask some ramps\n",
    "N_first = 3 # [if mask_ramps=True] This controls if you want to exclude more than the first frame from the ramp fit.\n",
    "N_last = 1  # [if mask_ramps=True]  This controls if you want to exclude more than the last frame from the ramp fit.\n",
    "use_agg_det1_params = False # Whether to use more aggressive Det1 parameters to flag cosmic rays - as suggested during data reduction meeting of 27/10/2022. Disclaimer: not well tested. Note: all else equal, does not lead to significant differences in final spectra of PDS 70. So left as False (i.e. default).\n",
    "\n",
    "# Bad pixel correction/flagging\n",
    "do_bpc1 = False           # Whether to also flag bad pixels between stages 1 and 2. If False, only performed after stage 2. Note: it is easier to capture more bad pixels before stage 2, but some warm pixels may be corrected after flat division during stage 2.\n",
    "do_bpc2 = True            # Whether to identify and correct bad pixels using pixel_replace step during stage 2.\n",
    "do_bpc3 = False           # Whether to identify and correct bad pixels after stage 3.\n",
    "sig_bpc = 5               # Sigma considered for clipping and correcting bad pixels.\n",
    "nit_bp = 3                # Number of bad pixel identification iterations.\n",
    "flag_only_bef3 = False    # Whether to flag only (modifying DQ map) additional bad pixels identified before stage 3\n",
    "sig_star_res_in_bkg = 3.  # [only if bkg_method = 'sdither']: number of standard deviations away from the median to identify residual star signals in proxy BKG map - these residual star signals are then interpolated with a Gaussian kernel. Safer to consider a low threshold to avoid any residual star signals in BKG map proxy.\n",
    "pixel_replace_algo = 'mingrad' # {'fit_profile', 'mingrad'} Algorithm used in PipelineStep pixel_replace. \n",
    "skip_pixel_replace_spec3 = False  # Whether to skip pixel replace step in spec3. Preliminary tests suggest it does improve the quality of the spectrum.\n",
    "\n",
    "# background subtraction\n",
    "persist_frac = 1./100       # [only if bkg_method = 'sdither' and rm_persist = True]: Residual persistence factor used for correction.\n",
    "nit_bg = 3                  # Maximum number of iterations for PSF and BKG estimations during clean background subtraction step.\n",
    "r_min_res_bg_sub = 4        # [only if subtract_residual_bkg=True] Radius in FWHM beyond which the residual background level is estimated\n",
    "bkg_hpf_kersz = (5, 41)     # Size of moving nan-median filter window to filter background estimate. ~41 pixels is roughly the horizontal period of vertical slices. small vertical extent to reproduce effects of residual dark current \n",
    "perc_bkg_level = 50         # percentile used in each (moving) cell of filter to identify background level in first BKG estimate image. 50 for median of good pixels (i.e. outliers such as residual PSF intensities are flagged as NaN)\n",
    "\n",
    "# Spec3 options (params below first used in Sec 5.1)\n",
    "skip_outlier_det_s3 = True  # Whether to skip outlier detection step in stage 3\n",
    "dith_combi_method = 'drizzle'   # {'drizzle', 'emsm', 'msm'}. Method to combine different dithers in cube building.\n",
    "\n",
    "# Centering\n",
    "cen_method=None         # {None, 'cc', 'gauss'} Method for alignment of spectral frames before aperture photometry. None for no realignment (just takes median position), 'cc' for cross-correlation based realignement, 'gauss' for 2D Gaussian fit.\n",
    "discard_band_cen=None   # List of bands to be discarded for centering (i.e. the ones where the object is too faint to be seen in the mean spectral frame). E.g. ['2A','3B','4C']. None does not discard any.\n",
    "imlib='ndimage-interp'  # Image library for sub-px shift operations. 'opencv' to use a Lanczos4 interpolation kernel (fastest), 'ndimage-interp' for a biquintic interpolation kernel (trade-off speed vs accuracy), 'vip-fft' to use a FT-based method (best preservation of flux but prone to Gibbs artefacts due to poor spatial sampling).\n",
    "crop_sz = 7             # Sub-image crop size used i) to look for a local maximum at the center of weighted mean band image, ii) then to fit a 2D Gaussian model to find the star centroid in a sub-frame centered on the local maximum. In other words, the crop_sz should be set to at least double the expected difference between star center and actual image center, but would benefit from being as small as possible (to avoid high noise level at frame edges) ##crop_szs = [13,11,11,9] # Sub-image crop sizes (one per band) used when fitting a 2D Gaussian model to find the star centroid.\n",
    "\n",
    "# Spectrum extraction + spike filtering\n",
    "if psff:\n",
    "    apcorr = 'FWHM'     # Aperture correction mode. 'FWHM': Aperture correction associated to apertures proportional to FWHM (default: 2.5FWHM).\n",
    "else:\n",
    "    apcorr = 'ex1d'     # Aperture correction mode. 'ex1d': using extract1d + default aperture corrections, respectively.\n",
    "sp_corr_win = 15        # Odd integer corresponding to spectral window used to identify spikes. E.g. 5 means it will look at the flux in the 2 channels on each side of each channel to identify outliers.\n",
    "spec_mode = 'per_band'  # {'per_band', 'per_channel'} whether to consider step 3 spectrum to be extracted per band or per channel [per band is better for final spectrum-level residual fringe correction]\n",
    "thr_occam = 0.05        # estimated relative uncertainty on the overall photometric calibration of each band - this information is used to judge whether flux rescaling or slope correction is better to correct for a potential jump between consecutive bands (e.g. if set to 0.05, rescaling factors between 0.95 and 1.05 can be expected, and won't trigger a slope correction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8d18c9",
   "metadata": {},
   "source": [
    "Output subdirectories to keep science data products organized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523730c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_list.set_final_out_dir(ver=ver,\n",
    "                              bg_observation=bg_observation,\n",
    "                              final_output_subdir='/final_outputs/',     # Where final outputs will be located (cubes & spectra)\n",
    "                              det1_subdir = 'stage1{}/'.format(ver),     # Detector1 pipeline outputs will go here\n",
    "                              spec2_subdir = 'stage2{}/'.format(ver),    # Spec2 pipeline outputs will go here\n",
    "                              spec3_subdir = 'stage3{}/'.format(ver),    # Spec3 pipeline outputs will go here\n",
    "                              fin3_subdir = 'stage3{}/'.format(ver),     # Final Spec3 pipeline outputs will go here (final cubes and spectra)\n",
    "                              figs_subdir = 'figures/')                  # whether the figures will be save\n",
    "if not bg_observation:\n",
    "    dodet1bg = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad6cd09",
   "metadata": {},
   "source": [
    "**Do not touch anything below in this notebook - except if you know what you're doing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c566a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import minds\n",
    "\n",
    "# We need to check that the desired output directories exist, and if not create them\n",
    "for i in range(source_list.number_of_sources):\n",
    "    if not os.path.exists(source_list.det1_dirs[i]):\n",
    "        os.makedirs(source_list.det1_dirs[i])\n",
    "    if source_list.det1_dirs_ori[i] is None:\n",
    "        source_list.det1_dirs_ori[i] = source_list.det1_dirs[i]\n",
    "    if not os.path.exists(source_list.spec2_dirs[i]):\n",
    "        os.makedirs(source_list.spec2_dirs[i])\n",
    "    if not os.path.exists(source_list.spec3_dirs[i]):\n",
    "        os.makedirs(source_list.spec3_dirs[i])\n",
    "    if not os.path.exists(source_list.fin3_dirs[i]):\n",
    "        os.makedirs(source_list.fin3_dirs[i])\n",
    "    if not os.path.exists(source_list.figs_dirs[i]):\n",
    "        os.makedirs(source_list.figs_dirs[i])\n",
    "    if bg_observation:\n",
    "        if not os.path.exists(source_list.det1_bkg_dir[i]):\n",
    "            os.makedirs(source_list.det1_bkg_dir[i])\n",
    "        if not os.path.exists(source_list.spec2_bkg_dir[i]):\n",
    "            os.makedirs(source_list.spec2_bkg_dir[i])\n",
    "\n",
    "# Don't change these\n",
    "firstframe_skip = False\n",
    "lastframe_skip = False\n",
    "rscd_skip = False\n",
    "    \n",
    "# psff-related files\n",
    "psff_dir='./psff_ref/'\n",
    "phot_ver='9B.04.19'\n",
    "fringe_ver='2'\n",
    "\n",
    "# format nch_bad_per_band\n",
    "if nch_cut_per_band is None:\n",
    "    nch_bad_per_band = [(0,0)]*12\n",
    "elif type(nch_cut_per_band) == str:\n",
    "    nch_bad_per_band = None\n",
    "elif np.isscalar(nch_cut_per_band):\n",
    "    nch_bad_per_band = [(nch_cut_per_band,nch_cut_per_band)]*12\n",
    "elif len(nch_cut_per_band) != 12:\n",
    "    raise ValueError(\"nch_cut_per_band should have length 12 if not scalar\")\n",
    "else:\n",
    "    nch_bad_per_band = nch_cut_per_band\n",
    "    for i in range(12):\n",
    "        if np.isscalar(nch_cut_per_band[i]):\n",
    "            nch_bad_per_band[i] = (nch_bad_per_band[i], nch_bad_per_band[i])\n",
    "\n",
    "#distortion_dir='./distortion_solution/'\n",
    "# \n",
    "\n",
    "# Format of aperture correction factor files, and allowed aperture list\n",
    "apcorr_fn = os.path.join(minds.__path__[0], \"apcorr_factors_ap1.5-3.5FWHM{}_smooth.fits\")  # filename for aperture correction factors\n",
    "list_aps = [1.5, 2.0, 2.5, 3.0, 3.5]  # list of authorized aperture sizes (in FWHM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37be7fd",
   "metadata": {},
   "source": [
    "TO DO for next version:\n",
    "- Update time-correction for photometric calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14036a3f",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd0c995",
   "metadata": {},
   "source": [
    "# 2. Imports and setup\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e3464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's use the entire available screen width for the notebook\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2ef4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import isdir, isfile, splitext, basename, split, dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7191bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic system utilities for interacting with files\n",
    "import glob\n",
    "import time\n",
    "\n",
    "# Astropy utilities for opening FITS and ASCII files\n",
    "from astropy.io import fits\n",
    "# Astropy utilities for making plots\n",
    "from astropy.visualization import (LinearStretch, LogStretch, ImageNormalize, ZScaleInterval)\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# Matplotlib for making plots\n",
    "import matplotlib.pyplot as plt\n",
    "if show_plots:\n",
    "    if interactive_plots:\n",
    "        %matplotlib widget\n",
    "    else:\n",
    "        %matplotlib inline\n",
    "\n",
    "# Photutils for aperture photometry\n",
    "import pickle\n",
    "\n",
    "from scipy.interpolate import interp1d as I1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322125b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the base JWST package\n",
    "import jwst\n",
    "from packaging import version\n",
    "vjwst = jwst.__version__\n",
    "print(\"JWST pipeline version: \", vjwst)\n",
    "\n",
    "if version.parse(vjwst) < version.parse(\"1.13.0\"):\n",
    "    raise ValueError(\"Please update JWST package to a version larger or equal than 1.13.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e6997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the pmap version is > 1094, when not default\n",
    "if os.environ[\"CRDS_CONTEXT\"]:\n",
    "    if int(os.environ[\"CRDS_CONTEXT\"][-9:-5])<1095:\n",
    "        raise ValueError(\"Please use a pmap version larger or equal than 1095\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fdfe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JWST pipelines (encompassing many steps)\n",
    "from jwst.pipeline import Detector1Pipeline\n",
    "from jwst.pipeline import Spec2Pipeline\n",
    "from jwst.pipeline import Spec3Pipeline\n",
    "\n",
    "# JWST pipeline utilities\n",
    "from stdatamodels.jwst import datamodels\n",
    "from stdatamodels.jwst.datamodels import dqflags\n",
    "from jwst.associations import asn_from_list as afl # Tools for creating association files\n",
    "from jwst.associations.lib.rules_level3_base import DMS_Level3_Base # Definition of a Lvl3 association file\n",
    "from jwst.datamodels import ModelContainer\n",
    "from jwst.residual_fringe import ResidualFringeStep\n",
    "from jwst.extract_1d import Extract1dStep\n",
    "from jwst.spectral_leak import SpectralLeakStep, spectral_leak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c489cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional utilities besides jwst pipeline\n",
    "from minds.bkg import read_file_info, detector_background_subtraction, clean_background_subtraction, res_bkg_sub\n",
    "from minds.bpc import bpix_corr2d, bpix_corr3d, bpc2\n",
    "if contsub:\n",
    "    from minds.cs import Baseline, PureBaseline, PlotSpectrum\n",
    "from minds.mp import runmany, add_dq, rundet1, run_straylight, runspec2\n",
    "from minds.phot import (recenter_cubes, extract_ap, find_nearest, write_x1d, spike_filter, \n",
    "                            calc_scal_fac, identify_scalefac_vs_bending, bend_spectrum)\n",
    "import vip_hci\n",
    "from vip_hci.fits import open_fits, write_fits\n",
    "from vip_hci.preproc import frame_fix_badpix_isolated, frame_crop\n",
    "from vip_hci.var import frame_center\n",
    "vvip = vip_hci.__version__\n",
    "print(\"VIP version: \", vvip)\n",
    "if version.parse(vvip) < version.parse(\"1.4.0\"):\n",
    "    raise ValueError(\"Please update vip_hci package to a version larger or equal than 1.4.0\")\n",
    "\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "from scipy.interpolate import UnivariateSpline, interp1d\n",
    "\n",
    "columns = ['FILENAME', 'CHANNEL', 'BAND', 'VISIT', 'EXPCOUNT', 'EXPOSURE', 'PATT_NUM',\n",
    "           'EFFINTTM', 'EFFEXPTM', 'DATE-BEG', 'DATE-END']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1d77c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a timer to keep track of runtime\n",
    "time0 = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e976c13",
   "metadata": {},
   "source": [
    "### 2.1. Define notebook-wise utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22291fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to check existing outputs (useful for overwrite parameter)\n",
    "def check_output_exist(input_paths, output_dir, input_ending='uncal.fits', output_ending='rate.fits'):\n",
    "    exists = []\n",
    "    output_paths = [os.path.join(output_dir, os.path.basename(x).replace(input_ending, output_ending)) for x in input_paths]\n",
    "    for out_path in output_paths:\n",
    "        exists.append(isfile(out_path))\n",
    "    exists = np.array(exists)\n",
    "    return exists, output_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85910ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_band_to_idx(band):\n",
    "    # note the index order is alphabetical, to match the order of calfiles list (i.e. not in increasing wavelength order of the bands)\n",
    "    idx = int((int(band[0])-1)*3)\n",
    "    if band[1] == 'B':\n",
    "        idx += 1\n",
    "    elif band[1] == 'C':\n",
    "        idx += 2\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00764cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_files_wl(files):\n",
    "    \"\"\"Utility to sort files by wavelength (i.e. not alphabetically for bands spectra)\"\"\"\n",
    "    files.sort()\n",
    "    sorted_files = []\n",
    "    for ff, fi in enumerate(files):\n",
    "        if ff%3 == 1 or ff%3 == 2:\n",
    "            sorted_files.insert(len(sorted_files)-ff%3, fi)\n",
    "        else:\n",
    "            sorted_files.append(fi)\n",
    "    return sorted_files    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549a7f86",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c9165",
   "metadata": {},
   "source": [
    "# 3. Stage 1 processing\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6037b2ae",
   "metadata": {},
   "source": [
    "## 3.1. Det1 Pipeline\n",
    "------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29f018e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "In this section we process our simulated data through the Detector1 pipeline to create Lvl2a data products (i.e., uncalibrated slope images).\n",
    "See https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_detector1.html\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214682e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_ramp(fnames, N_first=1, N_last=1):\n",
    "    for fname in fnames:\n",
    "        with datamodels.open(fname) as input_model:\n",
    "            sci_ngroups = input_model.data.shape[1]\n",
    "            print(input_model.data.shape)\n",
    "            # Create output as a copy of the input science data model\n",
    "            output = input_model.copy()\n",
    "\n",
    "            # Update the step status, and if ngroups > 3, set all of the GROUPDQ in\n",
    "            # the first group to 'DO_NOT_USE'\n",
    "            if sci_ngroups > (N_first + N_last + 1):\n",
    "                output.groupdq[:, :N_first, :, :] = \\\n",
    "                    np.bitwise_or(output.groupdq[:, :N_first, :, :], dqflags.group['DO_NOT_USE'])\n",
    "                output.groupdq[:, (sci_ngroups - N_last):, :, :] = \\\n",
    "                    np.bitwise_or(\n",
    "                        output.groupdq[:, (sci_ngroups - N_last):, :, :], dqflags.group['DO_NOT_USE'])\n",
    "            else:   # too few groups\n",
    "                print(\"Too few groups to apply correction\")\n",
    "            output.save(fname.replace(\"dq.fits\", \"masked.fits\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fcfc06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if do_sec3:\n",
    "    for isource in range(source_list.number_of_sources):\n",
    "        source = source_list.source_names[isource]\n",
    "        print(f'Source {isource+1}: {source}:')\n",
    "        input_dir = source_list.input_dirs[isource]\n",
    "        input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "        det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "        det1_dir = source_list.det1_dirs[isource]\n",
    "        det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "        \n",
    "# Now let's look for input files of the form *uncal.fits for the science observation\n",
    "        if mask_ramps:\n",
    "            sstring = os.path.join(input_dir, 'jw*mirifu*uncal.fits')\n",
    "            lvl1b_files = sorted(glob.glob(sstring))\n",
    "            print('Found ' + str(len(lvl1b_files)) + ' files to process')\n",
    "\n",
    "            # check existing files\n",
    "            if not overwrite_31:\n",
    "                exists, output_paths = check_output_exist(input_paths=lvl1b_files, output_dir=det1_dir_ori, \n",
    "                                                          input_ending='uncal.fits', output_ending='dq.fits')\n",
    "                # limit file list\n",
    "                lvl1b_files = [lvl1b_files[i] for i in range(len(lvl1b_files)) if not exists[i]]\n",
    "\n",
    "            if len(lvl1b_files)>0:\n",
    "                if maxp > 1:\n",
    "                    # Run the pipeline on these input files by a simple loop over our pipeline function\n",
    "        #             rundq_partial = partial(add_dq, outdir=det1_dir)\n",
    "        #             runmany(maxp, rundq_partial, lvl1b_files) \n",
    "                    runmany(maxp, add_dq, lvl1b_files, outdir=det1_dir)\n",
    "                else:\n",
    "                    for file in lvl1b_files:\n",
    "                        add_dq(file, outdir=det1_dir)\n",
    "        else:\n",
    "            print('Skipping DQ adding for science observation')        \n",
    "\n",
    "    # Now let's look for input files of the form *uncal.fits from the background observation\n",
    "        if bg_observation and mask_ramps:\n",
    "            sstring = os.path.join(input_bgdir, 'jw*mirifu*uncal.fits')\n",
    "            lvl1b_files = sorted(glob.glob(sstring))\n",
    "            print('Found ' + str(len(lvl1b_files)) + ' files to process')\n",
    "\n",
    "            if not overwrite_31:\n",
    "                exists, output_paths = check_output_exist(input_paths=lvl1b_files, output_dir=det1_bgdir, \n",
    "                                                          input_ending='uncal.fits', output_ending='dq.fits')\n",
    "                # limit file list\n",
    "                lvl1b_files = [lvl1b_files[i] for i in range(len(lvl1b_files)) if not exists[i]]\n",
    "\n",
    "            if len(lvl1b_files)>0:\n",
    "                if maxp > 1:\n",
    "                    # Run the pipeline on these input files by a simple loop over our pipeline function\n",
    "        #             rundq_partial = partial(add_dq, outdir=det1_bgdir)\n",
    "        #             runmany(maxp, rundq_partial, lvl1b_files)           \n",
    "                    runmany(maxp, add_dq, lvl1b_files, outdir=det1_bgdir)\n",
    "                else:\n",
    "                    for file in lvl1b_files:\n",
    "                        add_dq(file, outdir=det1_bgdir)\n",
    "        else:\n",
    "            print('Skipping DQ adding for science observation')        \n",
    "\n",
    "        if mask_ramps:\n",
    "            sstring = os.path.join(det1_dir, 'jw*mirifu*dq.fits')\n",
    "            files = sorted(glob.glob(sstring))\n",
    "            mask_ramp(files, N_first=N_first, N_last=N_last)\n",
    "        if bg_observation and mask_ramps:\n",
    "            sstring = os.path.join(det1_bgdir, 'jw*mirifu*dq.fits')\n",
    "            files = sorted(glob.glob(sstring))\n",
    "            mask_ramp(files, N_first=N_first, N_last=N_last)\n",
    "\n",
    "    # Let's look for input files of the form *uncal.fits from the science observation\n",
    "        if mask_ramps:\n",
    "            input_ending = 'masked.fits'\n",
    "            sstring = os.path.join(det1_dir, 'jw*mirifu*'+input_ending)\n",
    "        else:\n",
    "            input_ending = 'uncal.fits'\n",
    "            sstring = os.path.join(input_dir, 'jw*mirifu*uncal.fits')\n",
    "        lvl1b_files = sorted(glob.glob(sstring))\n",
    "        print('Found ' + str(len(lvl1b_files)) + ' science input files to process')\n",
    "\n",
    "        if not overwrite_31:\n",
    "            exists, output_paths = check_output_exist(input_paths=lvl1b_files, output_dir=det1_dir_ori, \n",
    "                                                      input_ending=input_ending, output_ending='rate.fits')\n",
    "            # limit file list\n",
    "            lvl1b_files = [lvl1b_files[i] for i in range(len(lvl1b_files)) if not exists[i]]\n",
    "\n",
    "        if len(lvl1b_files)>0:\n",
    "            if maxp>1:\n",
    "                # Run the pipeline on these input files by a simple loop over our pipeline function\n",
    "                #rundet1_partial = partial(rundet1, outdir=det1_dir, use_agg_det1_params=use_agg_det1_params)\n",
    "                #runmany(maxp, rundet1_partial, lvl1b_files)\n",
    "                runmany(maxp, rundet1, lvl1b_files, outdir=det1_dir, use_agg_det1_params=use_agg_det1_params)\n",
    "            else:\n",
    "                for file in lvl1b_files:\n",
    "                    rundet1(file, outdir=det1_dir, use_agg_det1_params=use_agg_det1_params)\n",
    "\n",
    "    # Now let's look for input files of the form *uncal.fits from the background observation\n",
    "        if bg_observation:\n",
    "            if mask_ramps:\n",
    "                input_ending = 'masked.fits'\n",
    "                sstring = os.path.join(det1_bgdir, 'jw*mirifu*'+input_ending)\n",
    "            else:\n",
    "                input_ending = 'uncal.fits'\n",
    "                sstring = os.path.join(input_bgdir, 'jw*mirifu*'+input_ending)\n",
    "            lvl1b_files = sorted(glob.glob(sstring))\n",
    "            print('Found ' + str(len(lvl1b_files)) + ' background input files to process')\n",
    "\n",
    "            if not overwrite_31:\n",
    "                exists, output_paths = check_output_exist(input_paths=lvl1b_files, output_dir=det1_bgdir, \n",
    "                                                          input_ending=input_ending, output_ending='rate.fits')\n",
    "                # limit file list\n",
    "                lvl1b_files = [lvl1b_files[i] for i in range(len(lvl1b_files)) if not exists[i]]\n",
    "\n",
    "            if len(lvl1b_files)>0:\n",
    "                if maxp > 1:\n",
    "                    # Run the pipeline on these input files by a simple loop over our pipeline function\n",
    "                    #rundet1_partial = partial(rundet1, outdir=det1_bgdir)\n",
    "                    #runmany(maxp, rundet1_partial, lvl1b_files)\n",
    "                    runmany(maxp, rundet1, lvl1b_files, outdir=det1_bgdir, use_agg_det1_params=use_agg_det1_params)\n",
    "                else:\n",
    "                    for file in lvl1b_files:\n",
    "                        rundet1(file, outdir=det1_bgdir)\n",
    "        else:\n",
    "            print('Skipping Detector1 processing for dedicated BKG observation')\n",
    "                \n",
    "                \n",
    "else:\n",
    "    print('Skipping Detector1 processing')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ec2e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # First we'll define a function that will call the detector1 pipeline with our desired set of parameters\n",
    "# def rundet1(filename, outdir, use_agg_det1_params=False):\n",
    "#     print(filename)\n",
    "    \n",
    "#     ## skip if file exists and no overwrite is desired:\n",
    "#     print(join(outdir,basename(splitext(filename)[0][:-5]), 'rate.fits'))\n",
    "    \n",
    "#     # This initial setup is just to make sure that we get the latest parameter reference files\n",
    "#     # pulled in for our files.  This is a temporary workaround to get around an issue with\n",
    "#     # how this pipeline calling method works.\n",
    "#     crds_config = Detector1Pipeline.get_config_from_reference(filename)\n",
    "#     det1 = Detector1Pipeline.from_config_section(crds_config)\n",
    "#     #det1 = Detector1Pipeline() # Instantiate the pipeline\n",
    "    \n",
    "#     det1.output_dir = outdir # Specify where the output should go\n",
    "    \n",
    "#     # New set of recommended parameters (cfr. data reduction meeting Oct. 27)\n",
    "#     if use_agg_det1_params:\n",
    "#         det1.jump.rejection_threshold = 4\n",
    "#         det1.jump.expand_large_events = True\n",
    "#         det1.jump.min_jump_area = 15\n",
    "#         det1.jump.use_ellipses = True\n",
    "#         det1.jump.expand_factor = 3\n",
    "#         det1.jump.after_jump_flag_dn1 = 10\n",
    "#         det1.jump.after_jump_flag_time1 = 20\n",
    "#         det1.jump.after_jump_flag_dn2 = 1000\n",
    "#         det1.jump.after_jump_flag_time2 = 3000\n",
    "    \n",
    "#     # Overrides for whether or not certain steps should be skipped\n",
    "#     #det1.dq_init.skip = False\n",
    "#     #det1.saturation.skip = False\n",
    "#     #det1.firstframe.skip = False\n",
    "#     #det1.lastframe.skip = False\n",
    "#     #det1.reset.skip = False\n",
    "#     #det1.linearity.skip = False\n",
    "#     #det1.rscd.skip = False\n",
    "#     #det1.dark_current.skip = False\n",
    "#     #det1.refpix.skip = False\n",
    "#     #det1.jump.skip = False\n",
    "#     #det1.ramp_fit.skip = False\n",
    "#     #det1.gain_scale.skip = False\n",
    "    \n",
    "#     # Bad pixel mask overrides\n",
    "#     #det1.dq_init.override_mask = 'myfile.fits'\n",
    "\n",
    "#     # Saturation overrides\n",
    "#     #det1.saturation.override_saturation = 'myfile.fits'\n",
    "    \n",
    "#     # Reset overrides\n",
    "#     #det1.reset.override_reset = 'myfile.fits'\n",
    "        \n",
    "#     # Linearity overrides\n",
    "#     #det1.linearity.override_linearity = 'myfile.fits'\n",
    "\n",
    "#     # RSCD overrides\n",
    "#     #det1.rscd.override_rscd = 'myfile.fits'\n",
    "        \n",
    "#     # DARK overrides\n",
    "#     #det1.dark_current.override_dark = 'myfile.fits'\n",
    "        \n",
    "#     # GAIN overrides\n",
    "#     #det1.jump.override_gain = 'myfile.fits'\n",
    "#     #det1.ramp_fit.override_gain = 'myfile.fits'\n",
    "                \n",
    "#     # READNOISE overrides\n",
    "#     #det1.jump.override_readnoise = 'myfile.fits'\n",
    "#     #det1.ramp_fit.override_readnoise = 'myfile.fits'\n",
    "        \n",
    "#     det1.save_results = True # Save the final resulting _rate.fits files\n",
    "#     det1(filename) # Run the pipeline on an input list of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d984a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f5d1be-6ac4-48fc-b1e0-caf3224b8d55",
   "metadata": {},
   "source": [
    "Let's estimate the **maximum DN count** in each band, in the rate files - this is to have an idea of the amplitude of the BFE effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6318e64b-0de6-4ea6-a9c1-8556f2b4dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.table import Table, vstack\n",
    "\n",
    "for isource in range(source_list.number_of_sources):\n",
    "    source = source_list.source_names[isource]\n",
    "    print(f'Source {isource+1}: {source}:')\n",
    "    input_dir = source_list.input_dirs[isource]\n",
    "    input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "    det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "    det1_dir = source_list.det1_dirs[isource]\n",
    "    det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "    source = source_list.source_names[isource]\n",
    "    \n",
    "    sstring = det1_dir_ori + '*mirifu*_rate.fits'\n",
    "    ratefiles = sorted(glob.glob(sstring))\n",
    "    nfiles = len(ratefiles)\n",
    "    print('Found ' + str(nfiles) + ' input files to process')\n",
    "\n",
    "    DN_data = []\n",
    "    DN_columns = []\n",
    "\n",
    "    for i, rate_file in enumerate(ratefiles):\n",
    "\n",
    "        det_img, head = fits.getdata(rate_file, ext=1, header=True)\n",
    "        head0 = fits.getheader(rate_file)\n",
    "\n",
    "        int_time = float(head0['EFFINTTM'])\n",
    "\n",
    "        if head0['CHANNEL'] == '34':\n",
    "            lab_l = 'CH4{} - peak DN'\n",
    "            lab_r = 'CH3{} - peak DN'\n",
    "        else:\n",
    "            lab_l = 'CH1{} - peak DN'\n",
    "            lab_r = 'CH2{} - peak DN'        \n",
    "\n",
    "        if 'LONG' in head0['BAND']:\n",
    "            lab_l = lab_l.format('C')\n",
    "            lab_r = lab_r.format('C')\n",
    "        elif 'MEDIUM' in head0['BAND']:\n",
    "            lab_l = lab_l.format('B')\n",
    "            lab_r = lab_r.format('B')\n",
    "        else:\n",
    "            lab_l = lab_l.format('A')\n",
    "            lab_r = lab_r.format('A')\n",
    "\n",
    "        # estimate in each detector half\n",
    "        half_x = det_img.shape[1]//2\n",
    "        left_half = det_img[2:-2,:half_x]\n",
    "        right_half = det_img[2:-2,half_x:]\n",
    "        # max_left = np.array([np.nanpercentile(left_half, 99.5))])\n",
    "        # max_right = np.array([float(np.nanpercentile(right_half, 99.5))])\n",
    "        max_left = float(np.nanpercentile(left_half, 99.99))*int_time     # not nanmax to avoid bad pixels\n",
    "        max_right = float(np.nanpercentile(right_half, 99.99))*int_time   # not nanmax to avoid bad pixels\n",
    "\n",
    "        # check WHETHER EXISTS. If so only update if larger!\n",
    "        if lab_r in DN_columns:\n",
    "            ind = DN_columns.index(lab_r)\n",
    "            if max_right > DN_data[ind]:\n",
    "                DN_data[ind] = round(max_right, 1)\n",
    "            ind = DN_columns.index(lab_l)\n",
    "            if max_left > DN_data[ind]:\n",
    "                DN_data[ind] = round(max_left, 1)\n",
    "        else:\n",
    "            # write in the right order\n",
    "            if head0['CHANNEL'] == '34':\n",
    "                DN_columns.append(lab_r)\n",
    "                DN_columns.append(lab_l)\n",
    "                DN_data.append(round(max_right,1))\n",
    "                DN_data.append(round(max_left,1))\n",
    "                # dict_df[lab_r] = max_right\n",
    "                # dict_df[lab_l] = max_left\n",
    "            else:\n",
    "                DN_columns.append(lab_l)\n",
    "                DN_columns.append(lab_r)\n",
    "                DN_data.append(round(max_left,1))\n",
    "                DN_data.append(round(max_right,1))\n",
    "                # dict_df[lab_l] = max_left        \n",
    "                # dict_df[lab_r] = max_right\n",
    "\n",
    "    # SORT\n",
    "    DN_columns = np.array(DN_columns)\n",
    "    DN_data = np.array(DN_data)\n",
    "    band_order = np.argsort(DN_columns)\n",
    "    DN_data_sort = DN_data[band_order]\n",
    "    DN_columns_sort = DN_columns[band_order]\n",
    "    DN_columns = DN_columns_sort.tolist()\n",
    "    DN_data = DN_data_sort.tolist()\n",
    "\n",
    "    # prepend integration time\n",
    "    DN_data = [source, round(int_time,1)]+DN_data\n",
    "    DN_columns = ['Target', 'Integration time (s)']+DN_columns\n",
    "\n",
    "    DN_data = [np.array([DN_data[i]]) for i in range(len(DN_data))]\n",
    "\n",
    "    # add all other keywords even if no value associated with them\n",
    "    # DN_columns.extend(['Reported extended emission?', 'Persistence?', 'Spectrum quality (/10)', 'Comment'])\n",
    "    # DN_data.extend([np.ma.masked, np.ma.masked, np.ma.masked, np.ma.masked])\n",
    "    # dict_df['Reported extended emission?'] = ''\n",
    "    # dict_df['Persistence?'] = ''\n",
    "    # dict_df['Spectrum quality (/10)'] = ''\n",
    "    # dict_df['Comment'] = ''\n",
    "\n",
    "    Table_counts = Table(DN_data, names=tuple(DN_columns))\n",
    "    Table_counts.write(join(det1_dir, \"DN_peak_count.csv\"), format='csv', overwrite=True)  \n",
    "\n",
    "    # DN_count_df = pd.DataFrame(dict_df, index=1)\n",
    "    # DN_count_df.to_csv(join(det1_dir, \"Peak_DN_count.csv\"))\n",
    "\n",
    "    # also update the file for all targets\n",
    "    if DN_count_file is not None:\n",
    "        # DN_count_df_all = pd.read_csv(DN_count_file)\n",
    "        # DN_count_df_all = pd.concat([DN_count_df_all, DN_count_df], ignore_index=True)\n",
    "        # DN_count_df.to_csv(DN_count_file, mode='w')\n",
    "        tab_all = Table.read(DN_count_file)\n",
    "        tab_all = vstack([tab_all, Table_counts])\n",
    "        tab_all.write(DN_count_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49002529",
   "metadata": {},
   "source": [
    "## 3.2. Straylight correction\n",
    "------------------\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "In this section we use the Spec2 pipeline to further calibrate the (bad-pixel corrected) detector rate images.  \n",
    "See https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec2.html\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4c8927",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only straylight step on \"rate\" files to clean potential cross-artifacts before doing the\n",
    "# detector based background subtraction and correct for background offsets (pedestals)\n",
    "# def run_straylight(filename, outdir):\n",
    "#     crds_config = Spec2Pipeline().get_config_from_reference(filename)\n",
    "#     spec2 = Spec2Pipeline().from_config_section(crds_config)\n",
    "# #     spec2 = Spec2Pipeline()\n",
    "#     spec2.output_dir = outdir\n",
    "\n",
    "#     # Straylight overrides\n",
    "#     # spec2.straylight.override_mrsxartcorr = 'myfile.fits'\n",
    "    \n",
    "#     # Overrides for whether or not certain steps should be skipped\n",
    "#     spec2.assign_wcs.skip = False\n",
    "#     spec2.bkg_subtract.skip = True\n",
    "#     spec2.flat_field.skip = True\n",
    "#     spec2.srctype.skip = True\n",
    "#     spec2.straylight.skip = False\n",
    "#     spec2.straylight.suffix = \"straylight\"\n",
    "#     spec2.straylight.save_results = True\n",
    "#     spec2.fringe.skip = True\n",
    "#     spec2.photom.skip = True\n",
    "#     spec2.cube_build.skip = True\n",
    "#     spec2.extract_1d.skip = True\n",
    "    \n",
    "#     # Do we need to set this false to prevent _cal file to be created?\n",
    "#     # spec2.save_results = True\n",
    "#     spec2(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41df117",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_sec3:\n",
    "    for isource in range(source_list.number_of_sources):\n",
    "        source = source_list.source_names[isource]\n",
    "        print(f'Source {isource+1}: {source}:')\n",
    "        input_dir = source_list.input_dirs[isource]\n",
    "        input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "        det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "        det1_dir = source_list.det1_dirs[isource]\n",
    "        det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "        \n",
    "        sstring = det1_dir_ori + '*mirifu*_rate.fits'\n",
    "        ratefiles = sorted(glob.glob(sstring))\n",
    "        ratefiles = np.array(ratefiles)\n",
    "        print('Found ' + str(len(ratefiles)) + ' input files to process')\n",
    "\n",
    "        if not overwrite_32:\n",
    "            exists, output_paths = check_output_exist(input_paths=ratefiles, output_dir=det1_dir, \n",
    "                                                      input_ending='rate.fits', output_ending='straylight.fits')\n",
    "            # limit file list\n",
    "            ratefiles = [ratefiles[i] for i in range(len(ratefiles)) if not exists[i]]\n",
    "\n",
    "        if len(ratefiles)>0:\n",
    "            if maxp > 1:\n",
    "                # Do straylight correction first\n",
    "                #run_straylight_partial = partial(run_straylight, outdir=det1_dir)\n",
    "                #runmany(maxp, run_straylight_partial, ratefiles)\n",
    "                runmany(maxp, run_straylight, ratefiles, outdir=det1_dir)\n",
    "            else:\n",
    "                for file in ratefiles:\n",
    "                    run_straylight(file, outdir=det1_dir)\n",
    "\n",
    "        if bg_observation:\n",
    "            sstring = det1_bgdir + '*mirifu*_rate.fits'\n",
    "            ratefiles = sorted(glob.glob(sstring))\n",
    "            ratefiles = np.array(ratefiles)\n",
    "            print('Found ' + str(len(ratefiles)) + ' input files to process')\n",
    "\n",
    "            if not overwrite_32:\n",
    "                exists, output_paths = check_output_exist(input_paths=ratefiles, output_dir=det1_bgdir, \n",
    "                                                          input_ending='rate.fits', output_ending='straylight.fits')\n",
    "                # limit file list\n",
    "                ratefiles = [ratefiles[i] for i in range(len(ratefiles)) if not exists[i]]\n",
    "\n",
    "            if len(ratefiles)>0:\n",
    "                if maxp > 1:\n",
    "        #             run_straylight_partial = partial(run_straylight, outdir=det1_bgdir)\n",
    "        #             runmany(maxp, run_straylight_partial, ratefiles)\n",
    "                    runmany(maxp, run_straylight, ratefiles, outdir=det1_bgdir)\n",
    "                else:\n",
    "                    for file in ratefiles:\n",
    "                        run_straylight(file, outdir=det1_bgdir)\n",
    "        else:\n",
    "            print('Skipping Straylight correction for background files')\n",
    "                    \n",
    "else:\n",
    "    print('Skipping Straylight correction')        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be7002c",
   "metadata": {},
   "source": [
    "## 3.3. Background subtraction\n",
    "------------------\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "This step performs a clean background subtraction removing the effect of persistence and estimating separate BKG and PSF components directly in the detector images. The latter are estimated iteratively, with the first BKG estimate obtained as the minimum image of the 4 dither images. The subsequent iterations replace vertical line-shaped outliers in the BKG estimate by Gaussian kernel interpolation.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1311c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_sec3:\n",
    "    for isource in range(source_list.number_of_sources):\n",
    "        source = source_list.source_names[isource]\n",
    "        print(f'Source {isource+1}: {source}:')\n",
    "        input_dir = source_list.input_dirs[isource]\n",
    "        input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "        det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "        det1_dir = source_list.det1_dirs[isource]\n",
    "        det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "\n",
    "        pattern = '*mirifu*_straylight.fits'\n",
    "        calfiles = glob.glob(join(det1_dir, pattern))\n",
    "        calfiles = [calfiles[i] for i in range(len(calfiles)) if '_bpc.' not in calfiles[i]]\n",
    "\n",
    "        print('Found ' + str(len(calfiles)) + ' input files to process')\n",
    "        info_table2 = read_file_info(calfiles, columns)\n",
    "        info_table2 = info_table2.sort_values(by='EXPOSURE')\n",
    "        \n",
    "        if bkg_method == 'sdither':\n",
    "            suffix = ['_psf', '_bkg']\n",
    "        else:\n",
    "            suffix = ['_psf']\n",
    "\n",
    "        if bkg_method == 'sdither':\n",
    "            clean_background_subtraction(info_table2, suffix=suffix, sig=sig_star_res_in_bkg, kernel_sz=bkg_hpf_kersz, \n",
    "                                         rm_persist=rm_persist, dedicated_bg=bg_observation, verbose=True, \n",
    "                                         perc=perc_bkg_level, persist_frac=persist_frac, \n",
    "                                         debug=True, overwrite=overwrite_33)\n",
    "        elif bkg_method == 'ddither':\n",
    "            # Reference BG\n",
    "            if bg_observation:\n",
    "                ref_files = sorted(glob.glob(join(det1_bgdir, pattern)))\n",
    "                reference_info_table = read_file_info(ref_files, columns)\n",
    "                reference_info_table = reference_info_table.sort_values(by='EXPOSURE')\n",
    "                print(reference_info_table['FILE_PATH'])\n",
    "            else:\n",
    "                reference_info_table = None\n",
    "            nodding = not dedicated_bg or not bg_observation\n",
    "            detector_background_subtraction(info_table2, reference_info_table=reference_info_table, suffix=suffix[0], \n",
    "                                            nodding=nodding, overwrite=overwrite_33)\n",
    "        else:\n",
    "            n_sci = len(info_table2)\n",
    "            for i in range(n_sci):\n",
    "                sci_filename = info_table2['FILE_PATH'].iloc[i]\n",
    "                dname = dirname(sci_filename)\n",
    "                fname, ext = splitext(basename(sci_filename))\n",
    "                outname_psf = join(dname, fname + suffix[0] + ext)\n",
    "                if not isfile(outname_psf) or overwrite_33:\n",
    "                    os.system(\"cp {} {}\".format(sci_filename, outname_psf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ac0386",
   "metadata": {},
   "source": [
    "## 3.4. Optional bad pixel identification\n",
    "------------------\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "This step performs bad pixel identification on the detector before stage 2, if requested.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_sec3:\n",
    "    for isource in range(source_list.number_of_sources):\n",
    "        source = source_list.source_names[isource]\n",
    "        print(f'Source {isource+1}: {source}:')\n",
    "        input_dir = source_list.input_dirs[isource]\n",
    "        input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "        det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "        det1_dir = source_list.det1_dirs[isource]\n",
    "        det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "        \n",
    "    if do_bpc1:\n",
    "        # Look for science rate files from the Detector1 pipeline\n",
    "        sstring = det1_dir + '*mirifu*_psf.fits'\n",
    "        det1files = sorted(glob.glob(sstring))\n",
    "        det1files=np.array(det1files)\n",
    "        print('Found ' + str(len(det1files)) + ' input files to process')\n",
    "\n",
    "        info_table1 = read_file_info(det1files, columns)\n",
    "\n",
    "        bpix_corr2d(info_table1, sig=sig_bpc, max_nit=2, suffix='_bpc', kernel_sz=3, flag_only=True,\n",
    "                    verbose=True, outpath=det1_dir, debug=False, overwrite=overwrite_34)\n",
    "\n",
    "    if bg_observation and do_bpc1:\n",
    "        # Look for background rate files from the Detector1 pipeline\n",
    "        sstring = det1_bgdir + '*mirifu*_straylight.fits'\n",
    "        det1files = sorted(glob.glob(sstring))\n",
    "        det1files=np.array(det1files)\n",
    "        print('Found ' + str(len(det1files)) + ' input files to process')\n",
    "\n",
    "        info_table1_bg = read_file_info(det1files, columns)\n",
    "        bpix_corr2d(info_table1_bg, sig=sig_bpc, max_nit=2, suffix='_bpc', kernel_sz=3, flag_only=True,\n",
    "                    verbose=True, outpath=det1_bgdir, debug=False, overwrite=overwrite_34)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81419572",
   "metadata": {},
   "source": [
    "# 4. Stage 2\n",
    "------------------\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "In this section we process our data through both the Spec2 pipeline and custom-made routines in order to produce Lvl2b data products (i.e., quick-look background-subtracted data cubes for each individual dither).  \n",
    "See https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec2.html\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cf9226",
   "metadata": {},
   "source": [
    "## 4.1. Spec2 Pipeline\n",
    "------------------\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "In this section we use the Spec2 pipeline to further calibrate the (bad-pixel corrected) detector rate images.  \n",
    "See https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec2.html\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d2c3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will call the spec2 pipeline with our desired set of parameters\n",
    "# Do not do straylight in this step, because already done\n",
    "\n",
    "# def runspec2(filename, outdir, psff=False):\n",
    "#     # This initial setup is just to make sure that we get the latest parameter reference files\n",
    "#     # pulled in for our files.  This is a temporary workaround to get around an issue with\n",
    "#     # how this pipeline calling method works.\n",
    "#     crds_config = Spec2Pipeline.get_config_from_reference(filename)\n",
    "#     spec2 = Spec2Pipeline.from_config_section(crds_config)\n",
    "#     #spec2 = Spec2Pipeline() # Instantiate the pipeline\n",
    "    \n",
    "#     spec2.output_dir = outdir\n",
    "\n",
    "#     # PSFF overrides\n",
    "#     if psff:\n",
    "#         hdu = fits.open(filename)\n",
    "#         channel = hdu[0].header['CHANNEL']\n",
    "#         band = hdu[0].header['BAND']\n",
    "#         ifu = hdu[0].header['DETECTOR']\n",
    "#         dith_num = hdu[0].header['PATT_NUM']\n",
    "#         dith_dir = hdu[0].header['DITHDIRC']\n",
    "#         hdu.close()\n",
    "        \n",
    "#         if band=='SHORT':\n",
    "#             subband='A'\n",
    "#         elif band=='MEDIUM':\n",
    "#             subband='B'\n",
    "#         elif band=='LONG':\n",
    "#             subband='C'\n",
    "            \n",
    "#         if dith_dir=='NEGATIVE':\n",
    "#             dith = 'neg'\n",
    "#         elif dith_dir=='POSITIVE':\n",
    "#             dith = 'pos'\n",
    "        \n",
    "#         spec2.photom.override_photom = join(psff_dir,'PHOTOM/MIRI_FM_{}_{}_PHOTOM_{}_{}dither.fits'.format(ifu,channel+band,phot_ver,dith))\n",
    "#         spec2.fringe.override_fringe = join(psff_dir,'FRINGE/point_source_fringe_flat_{}dither{}_{}_v{}.fits'.format(dith,dith_num,channel+subband,fringe_ver))\n",
    "#         spec2.flat_field.skip = True\n",
    "\n",
    "#     # Assign_wcs overrides\n",
    "#     #spec2.assign_wcs.override_distortion = 'myfile.asdf'\n",
    "#     #spec2.assign_wcs.override_regions = 'myfile.asdf'\n",
    "#     #spec2.assign_wcs.override_specwcs = 'myfile.asdf'\n",
    "#     #spec2.assign_wcs.override_wavelengthrange = 'myfile.asdf'\n",
    "\n",
    "#     # Flatfield overrides\n",
    "#     #spec2.flat.override_flat = 'myfile.fits'\n",
    "#     else:\n",
    "#         spec2.flat_field.skip = False\n",
    "        \n",
    "#     # Background\n",
    "#     spec2.bkg_subtract.skip = True\n",
    "        \n",
    "#     # Straylight overrides\n",
    "#     #spec2.straylight.override_mrsxartcorr = 'myfile.fits'\n",
    "#     spec2.straylight.skip = True # starting from v6e!\n",
    "    \n",
    "#     # Fringe overrides\n",
    "#     spec2.fringe.skip = False\n",
    "    \n",
    "#     # Photom overrides\n",
    "#     #spec2.photom.override_photom = 'myfile.fits'\n",
    "#     spec2.photom.skip = False\n",
    "    \n",
    "#     # Residual fringe correction\n",
    "#     if psff:\n",
    "#         spec2.residual_fringe.skip = True\n",
    "#     else:\n",
    "#         spec2.residual_fringe.skip = False\n",
    "        \n",
    "#     # Bad pixel correction\n",
    "#     spec2.pixel_replace.skip = False\n",
    "#     spec2.pixel_replace.algorithm = pixel_replace_algo\n",
    "        \n",
    "#     # Cubepar overrides\n",
    "#     spec2.cube_build.skip = True\n",
    "#     #spec2.cube_build.override_cubepar = 'myfile.fits'\n",
    "        \n",
    "#     # Extract1D overrides\n",
    "#     #spec2.extract1d.skip = True\n",
    "#     #spec2.extract1d.override_extract1d = 'myfile.asdf'\n",
    "#     #spec2.extract1d.override_apcorr = 'myfile.asdf'\n",
    "    \n",
    "#     # Some cube building options\n",
    "#     spec2.cube_build.weighting = dith_combi_method\n",
    "#     spec2.cube_build.coord_system = 'skyalign'\n",
    "#     spec2.cube_build.output_type = 'channel'\n",
    "    \n",
    "#     spec2.save_results = True\n",
    "#     spec2(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a2f470",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_sec4:\n",
    "    for isource in range(source_list.number_of_sources):\n",
    "        source = source_list.source_names[isource]\n",
    "        print(f'Source {isource+1}: {source}:')\n",
    "        input_dir = source_list.input_dirs[isource]\n",
    "        input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "        det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "        det1_dir = source_list.det1_dirs[isource]\n",
    "        det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "        spec2_dir = source_list.spec2_dirs[isource]\n",
    "        spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "        \n",
    "        psf_files = glob.glob(os.path.join(det1_dir, \"*_psf.fits\"))\n",
    "        bg_files = glob.glob(os.path.join(det1_dir, \"*_bkg.fits\"))\n",
    "        rate_files = sorted(psf_files + bg_files)\n",
    "        rate_files = np.array(rate_files)        \n",
    "        \n",
    "# Look for uncalibrated science slope files from the Detector1 pipeline\n",
    "        # DO PSF files\n",
    "        psf_files = glob.glob(os.path.join(det1_dir, \"jw*_psf*.fits\"))\n",
    "        rate_file_list = [psf_files]\n",
    "        if bkg_method == 'sdither':\n",
    "            bg_files = glob.glob(os.path.join(det1_dir, \"jw*_bkg*.fits\"))\n",
    "            rate_file_list.append(bg_files)\n",
    "\n",
    "        for rr, rate_files in enumerate(rate_file_list):\n",
    "            if do_bpc1:\n",
    "                rate_files = [rate_files[i] for i in range(len(rate_files)) if '_bpc.' in rate_files[i]]\n",
    "                input_ending = 'straylight{}_bpc.fits'.format(suffix[rr])\n",
    "                output_ending = 'straylight{}_bpc_cal.fits'.format(suffix[rr])\n",
    "            else:\n",
    "                rate_files = [rate_files[i] for i in range(len(rate_files)) if '_bpc.' not in rate_files[i]]\n",
    "                input_ending = 'straylight{}.fits'.format(suffix[rr])\n",
    "                output_ending = 'straylight{}_cal.fits'.format(suffix[rr])\n",
    "\n",
    "            rate_files = np.array(rate_files)\n",
    "\n",
    "            print('Found ' + str(len(rate_files)) + ' input files to process')\n",
    "\n",
    "            if not overwrite_41:\n",
    "                exists, output_paths = check_output_exist(input_paths=rate_files, output_dir=spec2_dir, \n",
    "                                                          input_ending=input_ending, output_ending=output_ending)\n",
    "                # limit file list\n",
    "                rate_files = [rate_files[i] for i in range(len(rate_files)) if not exists[i]]\n",
    "\n",
    "            if len(rate_files)>0:\n",
    "                if maxp > 1:\n",
    "                    #runspec2_partial = partial(runspec2, outdir=spec2_dir, psff=psff)\n",
    "                    #runmany(maxp, runspec2_partial, rate_files)\n",
    "                    runmany(maxp, runspec2, rate_files, outdir=spec2_dir, psff=psff, psff_dir=psff_dir,\n",
    "                            phot_ver=phot_ver, fringe_ver=fringe_ver, pixel_replace_algo=pixel_replace_algo,\n",
    "                            dith_combi_method=dith_combi_method)\n",
    "                else:\n",
    "                    for file in rate_files:\n",
    "                        runspec2(file, outdir=spec2_dir, psff=psff, psff_dir=psff_dir,\n",
    "                                 phot_ver=phot_ver, fringe_ver=fringe_ver, pixel_replace_algo=pixel_replace_algo,\n",
    "                                 dith_combi_method=dith_combi_method)\n",
    "                        \n",
    "        if bg_observation and not dedicated_bg:\n",
    "            # Look for uncalibrated background slope files from the Detector1 pipeline\n",
    "            sstring = det1_bgdir + 'jw*mirifu*_straylight*.fits'\n",
    "            bpcfiles = sorted(glob.glob(sstring))\n",
    "\n",
    "            if do_bpc1:\n",
    "                bpcfiles = [bpcfiles[i] for i in range(len(bpcfiles)) if '_bpc.' in bpcfiles[i]]\n",
    "                input_ending = 'straylight_bpc.fits'\n",
    "                output_ending = 'straylight_bpc_cal.fits'\n",
    "            else:\n",
    "                bpcfiles = [bpcfiles[i] for i in range(len(bpcfiles)) if '_bpc.' not in bpcfiles[i]]    \n",
    "                input_ending = 'straylight.fits'\n",
    "                output_ending = 'straylight_cal.fits'\n",
    "\n",
    "            bpcfiles = np.array(bpcfiles)\n",
    "            print('Found ' + str(len(bpcfiles)) + ' input background files to process')\n",
    "\n",
    "            if not overwrite_41:\n",
    "                exists, output_paths = check_output_exist(input_paths=bpcfiles, output_dir=spec2_bgdir, \n",
    "                                                          input_ending=input_ending, output_ending=output_ending)\n",
    "                # limit file list\n",
    "                bpcfiles = [bpcfiles[i] for i in range(len(bpcfiles)) if not exists[i]]\n",
    "\n",
    "            if len(bpcfiles)>0:   \n",
    "                if maxp > 1:\n",
    "        #             runspec2_partial = partial(runspec2, outdir=spec2_bgdir, psff=False)\n",
    "        #             runmany(maxp, runspec2_partial, bpcfiles)\n",
    "                    runmany(maxp, runspec2, bpcfiles, outdir=spec2_bgdir, psff=False, psff_dir=psff_dir,\n",
    "                            phot_ver=phot_ver, fringe_ver=fringe_ver, pixel_replace_algo=pixel_replace_algo,\n",
    "                            dith_combi_method=dith_combi_method)\n",
    "                else:\n",
    "                    for file in bpcfiles:\n",
    "                        runspec2(file, outdir=spec2_bgdir, psff=False, psff_dir=psff_dir,\n",
    "                                 phot_ver=phot_ver, fringe_ver=fringe_ver, pixel_replace_algo=pixel_replace_algo,\n",
    "                                 dith_combi_method=dith_combi_method)\n",
    "        else:\n",
    "            print('Skipping Spec2 processing for background files')                        \n",
    "                        \n",
    "else:\n",
    "    print('Skipping Spec2 processing')        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa48f0b2",
   "metadata": {},
   "source": [
    "## 4.2. Bad pixel correction\n",
    "------------------\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Stage 2 of the pipeline appears to re-introduce individual bad pixels, through the application of imperfect flat-field and photometric calibration. These bad pixels need to be corrected to avoid propagation into spikes in the final spectrum.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ec50f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if do_sec4:\n",
    "    for isource in range(source_list.number_of_sources):\n",
    "        source = source_list.source_names[isource]\n",
    "        print(f'Source {isource+1}: {source}:')\n",
    "        input_dir = source_list.input_dirs[isource]\n",
    "        input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "        det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "        det1_dir = source_list.det1_dirs[isource]\n",
    "        det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "        spec2_dir = source_list.spec2_dirs[isource]\n",
    "        spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "        \n",
    "# Look for uncalibrated science slope files from the Detector1 pipeline\n",
    "        psf_files = glob.glob(os.path.join(spec2_dir, \"*_straylight_psf_*cal.fits\"))\n",
    "        bg_files = glob.glob(os.path.join(spec2_dir, \"*_straylight_bkg_*cal.fits\"))\n",
    "\n",
    "        calfiles = sorted(psf_files + bg_files)\n",
    "\n",
    "        if do_bpc2:\n",
    "            if do_bpc1:\n",
    "                calfiles = [calfiles[i] for i in range(len(calfiles)) if '_bpc_' in calfiles[i]]\n",
    "            else:\n",
    "                calfiles = [calfiles[i] for i in range(len(calfiles)) if '_bpc_' not in calfiles[i]]\n",
    "\n",
    "            calfiles = np.array(calfiles)\n",
    "            endswith = \"cal.fits\" \n",
    "\n",
    "            if maxp > 1:\n",
    "    #             run_bpc2_partial = partial(bpc2, outdir=spec2_dir, endswith=endswith, nit_bp=nit_bp, \n",
    "    #                                        overwrite=overwrite_42, flag_only=flag_only_bef3)\n",
    "    #             runmany(maxp, run_bpc2_partial, calfiles)\n",
    "                runmany(maxp, bpc2, calfiles, outdir=spec2_dir, endswith=endswith, nit_bp=nit_bp, overwrite=overwrite_42, \n",
    "                        flag_only=flag_only_bef3)\n",
    "            else:\n",
    "                for file in calfiles:\n",
    "                    bpc2(file, outdir=spec2_dir, endswith=endswith, nit_bp=nit_bp, overwrite=overwrite_42, \n",
    "                         flag_only=flag_only_bef3)\n",
    "        else:\n",
    "            for file in calfiles:\n",
    "                nfile = file.replace('_cal.fits', '_bpc2.fits')\n",
    "                os.system(\"cp {} {}\".format(file, nfile))\n",
    "\n",
    "    # Note: warnings below are due to the presence of inf values that can appear in the array if 0 values are present in the fringe flat divided in step 2. They can be ignored.        \n",
    "# Look for uncalibrated science slope files from the Detector1 pipeline\n",
    "        if bg_observation:\n",
    "            calfiles = sorted(glob.glob(os.path.join(spec2_bgdir, \"*_cal.fits\")))\n",
    "\n",
    "            if do_bpc2:\n",
    "                if do_bpc1:\n",
    "                    calfiles = [calfiles[i] for i in range(len(calfiles)) if '_bpc_' in calfiles[i]]\n",
    "                else:\n",
    "                    calfiles = [calfiles[i] for i in range(len(calfiles)) if '_bpc_' not in calfiles[i]]\n",
    "\n",
    "                calfiles = np.array(calfiles)\n",
    "                endswith = \"cal.fits\"\n",
    "\n",
    "                print('Found ' + str(len(calfiles)) + ' input files to process')\n",
    "\n",
    "                if maxp > 1:\n",
    "                    #bpc2_partial = partial(bpc2, outdir=spec2_bgdir, endswith=endswith, overwrite=overwrite_42, \n",
    "                    #                       flag_only=flag_only_bef3)\n",
    "                    #runmany(maxp, bpc2_partial, calfiles)\n",
    "                    runmany(maxp, bpc2, calfiles, outdir=spec2_bgdir, endswith=endswith, overwrite=overwrite_42, \n",
    "                            flag_only=flag_only_bef3)\n",
    "\n",
    "                else:\n",
    "                    for file in calfiles:\n",
    "                        bpc2(file, outdir=spec2_bgdir, endswith=endswith, overwrite=overwrite_42, \n",
    "                             flag_only=flag_only_bef3)\n",
    "            else:\n",
    "                for file in calfiles:\n",
    "                    nfile = file.replace('_cal.fits', '_bpc2.fits')\n",
    "                    os.system(\"cp {} {}\".format(file, nfile))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b351ac6",
   "metadata": {},
   "source": [
    "# 5. Stage 3\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed989c3b",
   "metadata": {},
   "source": [
    "Let's first define the expected suffixes from previous stage based on whether dedicated background exist or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac756cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if bkg_method == 'sdither':\n",
    "    suffix = ['_psf', '_bkg']\n",
    "else:\n",
    "    suffix = ['_psf']\n",
    "\n",
    "if not bg_observation and bkg_method=='sdither':\n",
    "    suff=['{}_bpc2'.format(suffix[0]), '{}_bpc2'.format(suffix[1])]\n",
    "else:\n",
    "    suff=['{}_bpc2'.format(suffix[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6840e09a",
   "metadata": {},
   "source": [
    "Let's define a useful function to write out a Lvl3 association file from an input list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a5298c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writel3asn(scifiles, asnfile, prodname, bg_files=None):\n",
    "    # Define the basic association of science files\n",
    "    asn = afl.asn_from_list(scifiles, rule=DMS_Level3_Base, product_name=prodname)\n",
    "        \n",
    "    if bg_files is not None:   # Add background files to the association\n",
    "        nbg=len(bg_files)\n",
    "        for ii in range(0, nbg):\n",
    "            asn['products'][0]['members'].append({'expname': bg_files[ii], 'exptype': 'background'})\n",
    "        \n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d615e86",
   "metadata": {},
   "source": [
    "Now let's create the association files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e9bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_sec5:\n",
    "    for isource in range(source_list.number_of_sources):\n",
    "        source = source_list.source_names[isource]\n",
    "        print(f'Source {isource+1}: {source}:')\n",
    "        input_dir = source_list.input_dirs[isource]\n",
    "        input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "        det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "        det1_dir = source_list.det1_dirs[isource]\n",
    "        det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "        spec2_dir = source_list.spec2_dirs[isource]\n",
    "        spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "        spec3_dir = source_list.spec3_dirs[isource]\n",
    "        \n",
    "# if needed, consider 2 lists for PSF and BKG, respectively\n",
    "        l3asn_dir = os.path.join(spec3_dir, \"l3asn_files\")\n",
    "        source_list.l3asn_dirs[isource] = l3asn_dir\n",
    "        \n",
    "        if not isdir(l3asn_dir):\n",
    "            os.makedirs(l3asn_dir)\n",
    "        source_list.asnfiles[isource] = []\n",
    "        source_list.asnfiles_ifua[isource] = []\n",
    "        source_list.asnfiles_ifua_rfc1d[isource] = []\n",
    "        source_list.outnames[isource] = []\n",
    "        source_list.outnames_ifua[isource] = []\n",
    "        source_list.outnames_ifua_rfc1d[isource] = []\n",
    "        \n",
    "        for s, su in enumerate(suff):\n",
    "            sstring = os.path.join(spec2_dir, 'jw*mirifu*{}.fits'.format(su))\n",
    "            calfiles = np.array(sorted(glob.glob(sstring)))\n",
    "\n",
    "            if do_bpc1:\n",
    "                calfiles = [calfiles[i] for i in range(len(calfiles)) if '_bpc_' in calfiles[i]]\n",
    "            else:\n",
    "                calfiles = [calfiles[i] for i in range(len(calfiles)) if '_bpc_' not in calfiles[i]]\n",
    "\n",
    "            print('Found ' + str(len(calfiles)) + ' science files to process')\n",
    "\n",
    "            outname = 'Level3{}'.format(suffix[s])\n",
    "            outname_ifua = 'Level3_ifua{}'.format(suffix[s])\n",
    "            asnfile=os.path.join(l3asn_dir, 'l3asn{}.json'.format(suffix[s]))\n",
    "            asnfile_ifua=os.path.join(l3asn_dir, 'l3asn_ifua{}.json'.format(suffix[s]))\n",
    "            source_list.asnfiles[isource].append(asnfile)\n",
    "            source_list.asnfiles_ifua[isource].append(asnfile_ifua)\n",
    "            source_list.outnames[isource].append(outname)\n",
    "            source_list.outnames_ifua[isource].append(outname_ifua)\n",
    "            if bg_observation and not bkg_method=='ddither' and not bkg_method=='sdither':\n",
    "                bg_files=sorted(glob.glob(spec2_bgdir+'jw*_bpc2.fits'))\n",
    "                writel3asn(calfiles, asnfile, outname, bg_files=bg_files)\n",
    "                writel3asn(calfiles, asnfile_ifua, outname_ifua, bg_files=bg_files)\n",
    "            else:\n",
    "                writel3asn(calfiles, asnfile, outname)\n",
    "                writel3asn(calfiles, asnfile_ifua, outname_ifua)\n",
    "            if do_rfc1d:\n",
    "                outname_ifua_rfc1d = 'Level3_ifua_rfc1d{}'.format(suffix[s])\n",
    "                asnfile_ifua_rfc1d=os.path.join(l3asn_dir, 'l3asn_ifua_rfc1d{}.json'.format(suffix[s])) # with rfc1d\n",
    "                source_list.asnfiles_ifua_rfc1d[isource].append(asnfile_ifua_rfc1d)\n",
    "                source_list.outnames_ifua_rfc1d[isource].append(outname_ifua_rfc1d)\n",
    "                if bg_observation and not bkg_method=='ddither' and not bkg_method=='sdither':\n",
    "                    writel3asn(calfiles, asnfile_ifua_rfc1d, outname_ifua_rfc1d, bg_files=bg_files)\n",
    "                else:\n",
    "                    writel3asn(calfiles, asnfile_ifua_rfc1d, outname_ifua_rfc1d)        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6906db60",
   "metadata": {},
   "source": [
    "## 5.1. Spec3 Pipeline\n",
    "------------------\n",
    "    \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "Here we'll run the Spec3 pipeline to produce a composite data cube from all dithered exposures.\n",
    "We will need to create an association file from all science data in order for the pipeline to use them appropriately.\n",
    "\n",
    "A word of caution: the data cubes created by the JWST pipeline are in SURFACE BRIGHTNESS units (MJy/steradian), not flux units.  What that means is that if you intend to sum spectra within an aperture you need to be sure to multiply by the pixel area in steradians first in order to get a spectrum in flux units (the PIXAR_SR keyword can be found in the SCI extension header).  This correction is already built into the pipeline Extract1D algorithm.\n",
    "\n",
    "See https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_spec3.html\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c035f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will call the spec3 pipeline with our desired set of parameters\n",
    "# This is designed to run on an association file\n",
    "def runspec3(filename, outname, output_dir, coord_sys='skyalign', overwrite=True, do_rfc1d_e1d=False):\n",
    "    # first delete existing files\n",
    "#     filelist =sorted(glob.glob(spec3_dir+'{}*_s3d.fits'.format(outname)))\n",
    "#     filelist_x1d =sorted(glob.glob(spec3_dir+'{}*_x1d.fits'.format(outname)))\n",
    "#     if overwrite and (len(filelist)>0 or len(filelist_x1d)>0):\n",
    "#         for f in filelist:\n",
    "#             os.system(\"rm \"+f)\n",
    "#         for f in filelist_x1d:\n",
    "#             os.system(\"rm \"+f)\n",
    "#     elif (len(filelist)>0 or len(filelist_x1d)>0):\n",
    "#         print(\"Level 3 products already exist (or partially exist). Set overwrite_51 to True to force re-calculating files.\")\n",
    "#         return None\n",
    "    # check whether outputs exist:\n",
    "    outlist = glob.glob(join(output_dir,'{}*_s3d.fits'.format(outname)))\n",
    "    if (len(outlist)>=12) and not overwrite:\n",
    "        return None\n",
    "    # This initial setup is just to make sure that we get the latest parameter reference files\n",
    "    # pulled in for our files.  This is a temporary workaround to get around an issue with\n",
    "    # how this pipeline calling method works.\n",
    "    crds_config = Spec3Pipeline.get_config_from_reference(filename)\n",
    "    spec3 = Spec3Pipeline.from_config_section(crds_config)\n",
    "    \n",
    "    spec3.output_dir = output_dir\n",
    "    #spec3.save_results = True\n",
    "    \n",
    "    # Cube building configuration options\n",
    "    # spec3.cube_build.output_file = 'bandcube' # Custom output name\n",
    "    if spec_mode == 'per_band':\n",
    "        spec3.cube_build.output_type = 'band' # 'band' better for fringe correction BUT 'channel' better for SDI (I think)\n",
    "    else:\n",
    "        spec3.cube_build.output_type = 'channel'\n",
    "    # spec3.cube_build.channel = '1' # Build everything from just channel 1 into a single cube (we could also choose '2','3','4', or 'ALL')\n",
    "    # spec3.cube_build.weighting = 'drizzle' # 'emsm' or 'drizzle'\n",
    "    # spec3.cube_build.coord_system = 'ifualign' # 'ifualign', 'skyalign', or 'internal_cal'\n",
    "    spec3.cube_build.weighting = dith_combi_method\n",
    "    spec3.cube_build.coord_system = coord_sys # 'ifualign', 'skyalign', or 'internal_cal'\n",
    "\n",
    "    # spec3.cube_build.scale1 = 0.5 # Output cube spaxel scale (arcsec) in dimension 1 if setting it by hand\n",
    "    # spec3.cube_build.scale2 = 0.5 # Output cube spaxel scale (arcsec) in dimension 2 if setting it by hand\n",
    "    # spec3.cube_build.scalew = 0.002 # Output cube spaxel size (microns) in dimension 3 if setting it by hand\n",
    "    \n",
    "    # Overrides for whether or not certain steps should be skipped\n",
    "    #spec3.assign_mtwcs.skip = False\n",
    "    spec3.master_background.skip = True # skip background subtraction: already done!\n",
    "    spec3.outlier_detection.skip = skip_outlier_det_s3\n",
    "\n",
    "    #spec3.mrs_imatch.skip = True\n",
    "    spec3.pixel_replace.skip = skip_pixel_replace_spec3\n",
    "    \n",
    "    spec3.spectral_leak.skip = (not do_leak_correction or psff)\n",
    "    \n",
    "    # Cubepar overrides\n",
    "    #spec3.cube_build.override_cubepar = 'myfile.fits'\n",
    "\n",
    "    # Extract1D overrides\n",
    "    #spec3.extract1d.override_extract1d = 'myfile.asdf'\n",
    "    #spec3.extract1d.override_apcorr = 'myfile.asdf'\n",
    "    spec3.extract_1d.skip = False\n",
    "    spec3.extract_1d.use_source_posn = True # try it - to double check what default extract_1d does.\n",
    "    spec3.extract_1d.subtract_background=(bkg_method=='annulus') #\n",
    "    spec3.extract_1d.ifu_autocen=True\n",
    "    spec3.extract_1d.ifu_rfcorr = do_rfc1d_e1d\n",
    "    if overwrite_target_classification:\n",
    "        spec3.extract_1d.ifu_set_srctype = new_sourcetype\n",
    "    spec3.extract_1d.ifu_rscale = apsize\n",
    "    spec3(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0573eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_sec5:\n",
    "    for isource in range(source_list.number_of_sources):\n",
    "        source = source_list.source_names[isource]\n",
    "        print(f'Source {isource+1}: {source}:')\n",
    "        input_dir = source_list.input_dirs[isource]\n",
    "        input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "        det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "        det1_dir = source_list.det1_dirs[isource]\n",
    "        det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "        spec2_dir = source_list.spec2_dirs[isource]\n",
    "        spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "        spec3_dir = source_list.spec3_dirs[isource]\n",
    "        \n",
    "# now let's run spec3\n",
    "        for aa, asnfile in enumerate(source_list.asnfiles[isource]):\n",
    "            runspec3(asnfile, outname=source_list.outnames[isource][aa], output_dir=spec3_dir, overwrite=overwrite_51) # skyalign\n",
    "            runspec3(source_list.asnfiles_ifua[isource][aa], outname=source_list.outnames_ifua[isource][aa], output_dir=spec3_dir, coord_sys='ifualign', \n",
    "                     overwrite=overwrite_51) # ifualign\n",
    "            if do_rfc1d:\n",
    "                runspec3(source_list.asnfiles_ifua_rfc1d[isource][aa], outname=source_list.outnames_ifua_rfc1d[isource][aa], output_dir=spec3_dir, \n",
    "                         coord_sys='ifualign', overwrite=overwrite_51, do_rfc1d_e1d=True) # ifualign & rfc1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e22779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the time benchmark\n",
    "time1 = time.perf_counter()\n",
    "print(f\"Runtime so far: {time1 - time0:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ed0576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "# from cascade_jitter.utilities import create_mask_from_dq\n",
    "# from gwcs.wcstools import grid_from_bounding_box\n",
    "# from jwst.extract_1d import Extract1dStep\n",
    "# from photutils.background import Background2D, MedianBackground\n",
    "# from astropy.stats import SigmaClip\n",
    "# for data_file in cubefiles:\n",
    "#     cdm = datamodels.open(data_file)\n",
    "#     grid = grid_from_bounding_box(cdm.meta.wcs.bounding_box)\n",
    "#     ra, dec, lam = cdm.meta.wcs(*grid)\n",
    "#     data_cube = cdm.data\n",
    "#     data_cube = np.ma.masked_invalid(data_cube)\n",
    "#     background = np.ma.zeros_like(data_cube)\n",
    "#     for i, data in enumerate(data_cube):\n",
    "#         #data = np.ma.median(data_cube, axis=0)\n",
    "#         coverage_mask = data.mask\n",
    "#         sigma_clip = SigmaClip(sigma=3.0)\n",
    "#         bkg_estimator = MedianBackground()\n",
    "#         bkg = Background2D(data.filled(0), (7, 7), filter_size=(3, 3), coverage_mask=coverage_mask, fill_value=np.nan,\n",
    "#                            sigma_clip=sigma_clip, bkg_estimator=bkg_estimator,\n",
    "#                            exclude_percentile=50.0)\n",
    "#         background[i, ...] = np.ma.array(bkg.background, mask = bkg.coverage_mask)\n",
    "#     bla1 = np.ma.median(np.ma.array(lam, mask=background.mask), axis=(1,2))\n",
    "#     bla2 = np.ma.median(np.ma.array(background), axis=(1,2))\n",
    "#     new_file = pathlib.Path(data_file.replace('_s3d.fits', '_median_smoothed_s3d.fits'))\n",
    "#     cdm.data = background.filled(np.nan)\n",
    "#     cdm.meta.filename = new_file.name\n",
    "#     cdm.save(new_file)\n",
    "#     new_file_name_x1d = str(pathlib.Path(str(new_file).replace('s3d', 'x1d')))\n",
    "#     save_dir = str(pathlib.Path(new_file).parent)\n",
    "#     _ = Extract1dStep.call(cdm, save_results=True, output_file=new_file_name_x1d, output_dir=save_dir,\n",
    "#                           apply_apcorr=False, subtract_background=False, bkg_fit='median', bkg_sigma_clip=3.0, use_source_posn=True, suffix='x1d')#,\n",
    "#     plt.plot(bla1, bla2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53edbb4",
   "metadata": {},
   "source": [
    "## 5.2. Residual bad pixel correction\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    " \n",
    "Even after the outlier detection step of the Spec3 pipeline a couple of cosmic rays appear to pollute a few channels. For a clean spectrum extraction, it is recommended to run an iterative sigma clipping algorithm, defined in `utils_bpc_VC.py` and based on routines from VIP. This step should remove most of the significant cosmic ray contribution from extracted spectra (exception being if the cosmic ray signal is right on top of the PSF).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3ea12e",
   "metadata": {},
   "source": [
    "**For skyalign cubes:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca7867b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_sec5:\n",
    "    for isource in range(source_list.number_of_sources):\n",
    "        source = source_list.source_names[isource]\n",
    "        print(f'Source {isource+1}: {source}:')\n",
    "        input_dir = source_list.input_dirs[isource]\n",
    "        input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "        det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "        det1_dir = source_list.det1_dirs[isource]\n",
    "        det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "        spec2_dir = source_list.spec2_dirs[isource]\n",
    "        spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "        spec3_dir = source_list.spec3_dirs[isource]\n",
    "        \n",
    "        # List relevant files\n",
    "        calfiles=sorted(glob.glob(join(spec3_dir,'Level3{}*_s3d.fits'.format(suffix[0]))))\n",
    "        print(len(calfiles))\n",
    "        \n",
    "        if do_bpc3:\n",
    "            if maxp > 1:\n",
    "        #         runbpc3d_partial = partial(bpix_corr3d, sig=sig_bpc, protect_mask=5, max_nit=5, \n",
    "        #                                    suffix='_bpc3', verbose=True, overwrite=overwrite_52, debug=False)\n",
    "        #         runmany(maxp, runbpc3d_partial, calfiles)\n",
    "                runmany(maxp, bpix_corr3d, calfiles, sig=sig_bpc, protect_mask=5, max_nit=5, suffix='_bpc3', \n",
    "                        verbose=True, overwrite=overwrite_52, debug=False)\n",
    "            else:\n",
    "                for file in calfiles:\n",
    "                    bpix_corr3d(file, sig=sig_bpc, protect_mask=5, max_nit=5, suffix='_bpc3', \n",
    "                                verbose=True, overwrite=overwrite_52, debug=False)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619c418e",
   "metadata": {},
   "source": [
    "**For ifualign cubes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef27ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_sec5:\n",
    "    for isource in range(source_list.number_of_sources):\n",
    "        source = source_list.source_names[isource]\n",
    "        print(f'Source {isource+1}: {source}:')\n",
    "        input_dir = source_list.input_dirs[isource]\n",
    "        input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "        det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "        det1_dir = source_list.det1_dirs[isource]\n",
    "        det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "        spec2_dir = source_list.spec2_dirs[isource]\n",
    "        spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "        spec3_dir = source_list.spec3_dirs[isource]\n",
    "\n",
    "        # List relevant files\n",
    "        calfiles_ifua=sorted(glob.glob(join(spec3_dir,'Level3_ifua{}*_s3d.fits'.format(suffix[0]))))\n",
    "        print(len(calfiles_ifua))\n",
    "        \n",
    "        if do_bpc3:\n",
    "            if maxp > 1:\n",
    "                #runbpc3d_partial = partial(bpix_corr3d, sig=sig_bpc, protect_mask=5, max_nit=5, \n",
    "                #                           suffix='_bpc3', verbose=True, overwrite=overwrite_52, debug=False)\n",
    "                #runmany(maxp, runbpc3d_partial, calfiles_ifua)\n",
    "                runmany(maxp, bpix_corr3d, calfiles_ifua, sig=sig_bpc, protect_mask=5, max_nit=5, \n",
    "                        suffix='_bpc3', verbose=True, overwrite=overwrite_52, debug=False)\n",
    "            else:\n",
    "                for file in calfiles_ifua:\n",
    "                    bpix_corr3d(file, sig=sig_bpc, protect_mask=5, max_nit=5, suffix='_bpc3', \n",
    "                                verbose=True, overwrite=overwrite_52, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f652741",
   "metadata": {},
   "source": [
    "## 5.3. Spectrum extraction\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    " \n",
    "In extract_1d, the assumed center for aperture photometry is based on RA/DEC information in the header. This can be up to ~8px off with respect to the actual centroid location in each channel (measured in the PDS 70 MRS dataset).\n",
    "Since the center location for the aperture is different in each channels, one cannot straightforwardly use the `center_xy` argument of the `extract_1d` function -- a recentering step is necessary first. \n",
    "    \n",
    "In this section, we offer two possibilities for spectrum extraction after recentering:\n",
    "    \n",
    "    - through the extract_1d function of the jwst pipeline, using default aperture correction factors;\n",
    "    - through a manual extraction of the spectrum using the photutils package, using either a fixed aperture size (in px) or a wavelength-dependent aperture (in FWHM).\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9074988f",
   "metadata": {},
   "source": [
    "### 5.3.1. Centering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33488b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_bpc3:\n",
    "    suf='bpc3'\n",
    "else:\n",
    "    suf='s3d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e720de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_sec5:\n",
    "    for isource in range(source_list.number_of_sources):\n",
    "        source = source_list.source_names[isource]\n",
    "        print(f'Source {isource+1}: {source}:')\n",
    "        input_dir = source_list.input_dirs[isource]\n",
    "        input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "        det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "        det1_dir = source_list.det1_dirs[isource]\n",
    "        det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "        spec2_dir = source_list.spec2_dirs[isource]\n",
    "        spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "        spec3_dir = source_list.spec3_dirs[isource]\n",
    "        fin3_dir = source_list.fin3_dirs[isource]\n",
    "        \n",
    "# remove any existing cube from final output directory\n",
    "        exist_cubes = sorted(glob.glob(join(fin3_dir,'Level3*{}_cen*fits'.format(suf))))\n",
    "        for file in exist_cubes:\n",
    "            os.system(\"rm {}\".format(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca5b39e",
   "metadata": {},
   "source": [
    "**For skyalign cubes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6150a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "discard_idx_band = []\n",
    "if discard_band_cen is not None:\n",
    "    for dd in discard_band_cen:\n",
    "        discard_idx_band.append(convert_band_to_idx(dd)) \n",
    "        \n",
    "if do_sec5:\n",
    "    for isource in range(source_list.number_of_sources):\n",
    "        source = source_list.source_names[isource]\n",
    "        print(f'Source {isource+1}: {source}:')\n",
    "        input_dir = source_list.input_dirs[isource]\n",
    "        input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "        det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "        det1_dir = source_list.det1_dirs[isource]\n",
    "        det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "        spec2_dir = source_list.spec2_dirs[isource]\n",
    "        spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "        spec3_dir = source_list.spec3_dirs[isource]\n",
    "        fin3_dir = source_list.fin3_dirs[isource]\n",
    "        \n",
    "        # List relevant files\n",
    "        calfiles=sorted(glob.glob(join(spec3_dir,'Level3{}*{}.fits'.format(suffix[0],suf))))\n",
    "        calfiles = sort_files_wl(calfiles)\n",
    "        print(len(calfiles))\n",
    "        print('Level3{}*{}.fits'.format(suffix[0],suf))\n",
    "        print(calfiles)        \n",
    "\n",
    "        cxy_med = []\n",
    "        for ff, file in enumerate(calfiles):\n",
    "            bname = splitext(file)[0]\n",
    "            print(\"*** Recentering {} ***\".format(bname))\n",
    "            if bname[-5:] == '_bpc3':\n",
    "                bname = bname[:-5]\n",
    "            outname = bname+'_cen.fits'\n",
    "            if ff in discard_idx_band:\n",
    "                os.system(\"cp {} {}\".format(file, outname))\n",
    "                cube = fits.getdata(file, 'SCI')\n",
    "                cy, cx = frame_center(cube)\n",
    "                cxy_med.append((cx,cy))\n",
    "            else:\n",
    "                cxy_med.append(recenter_cubes(file, sig=5, method=cen_method, imlib=imlib, crop_sz=crop_sz, debug=show_plots,\n",
    "                                              suffix='_cen', verbose=True, overwrite=True))\n",
    "                if show_plots:\n",
    "                    plt.show()\n",
    "            ## if no centering just copy input file\n",
    "        #         if cen_method is None:\n",
    "        #             os.system(\"cp {} {}\".format(file, outname))\n",
    "        #    os.system(\"cp {} {}/.\".format(outname, fin3_dir)) #    os.system(\"cp {} {}/.\".format(outname, fin3_dir)) --> NOW done after potential residual BKG subtraction\n",
    "        if discard_band_cen is not None:\n",
    "            # for bands with very low signal from the star, replace centroid coordinates with median of good coordinates in the same channel\n",
    "            for ff in discard_idx_band:\n",
    "                msg = \"WARNING: centroid in band#{} will be replaced by \".format(ff)\n",
    "                if ff%3 == 0:\n",
    "                    good_ff = [ff_tmp+ff for ff_tmp in [1,2] if ff_tmp+ff not in discard_idx_band]\n",
    "                elif ff%3 == 1:\n",
    "                    good_ff = [ff_tmp+ff for ff_tmp in [-1,1] if ff_tmp+ff not in discard_idx_band]\n",
    "                else:\n",
    "                    good_ff = [ff_tmp+ff for ff_tmp in [-2,-1] if ff_tmp+ff not in discard_idx_band]\n",
    "                if len(good_ff)==2:\n",
    "                    cxy_med[ff] = (np.median([cxy_med[good_ff[0]][0],cxy_med[good_ff[1]][0]]),\n",
    "                                   np.median([cxy_med[good_ff[0]][1],cxy_med[good_ff[1]][1]]))\n",
    "                    msg += \"median of centroid inferred in bands# {} and {}: {}\".format(good_ff[0], good_ff[1], cxy_med[ff])\n",
    "                elif len(good_ff)==1:\n",
    "                    cxy_med[ff] = (cxy_med[good_ff[0]][0], cxy_med[good_ff[0]][1])\n",
    "                    msg += \"centroid inferred in band# {}: {}\".format(good_ff[0], cxy_med[ff])\n",
    "                else:\n",
    "                    msg += \"center of field of view: {}\".format(cxy_med[ff])\n",
    "                print(msg)\n",
    "                \n",
    "        write_fits(join(spec3_dir,\"cxy_med.fits\"), np.array(cxy_med)) # write a first time to auto-generate some header keywords\n",
    "        _, head = open_fits(join(spec3_dir,\"cxy_med.fits\"), header=True)\n",
    "        chs = [1, 2, 3, 4]\n",
    "        bands = ['SHORT','MEDIUM','LONG']\n",
    "        print(\"XY coordinates of the star inferred in the sky-aligned cubes (not used for spectrum extraction - only the ifu-aligned cubes are used): \")\n",
    "        c=0\n",
    "        for i in chs:\n",
    "            for j in bands:\n",
    "                print(\"CH.{} {}: {}\".format(i, j, cxy_med[c]))\n",
    "                c+=1\n",
    "                head['C_PAIR{}'.format(c)] = 'CH.{} {}'.format(i,j)\n",
    "        write_fits(join(spec3_dir,\"cxy_med.fits\"), np.array(cxy_med), header=head)\n",
    "        #print(cxy_med)\n",
    "        os.system(\"cp {} {}/.\".format(join(spec3_dir,\"cxy_med.fits\"), fin3_dir))\n",
    "        \n",
    "        source_list.cxy_med[isource] = cxy_med"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab76543",
   "metadata": {},
   "source": [
    "**For ifualign cubes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eee9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_sec5:\n",
    "    for isource in range(source_list.number_of_sources):\n",
    "        source = source_list.source_names[isource]\n",
    "        print(f'Source {isource+1}: {source}:')\n",
    "        input_dir = source_list.input_dirs[isource]\n",
    "        input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "        det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "        det1_dir = source_list.det1_dirs[isource]\n",
    "        det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "        spec2_dir = source_list.spec2_dirs[isource]\n",
    "        spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "        spec3_dir = source_list.spec3_dirs[isource]\n",
    "        fin3_dir = source_list.fin3_dirs[isource]\n",
    "\n",
    "        # List relevant files\n",
    "        calfiles_ifua=sorted(glob.glob(join(spec3_dir,'Level3_ifua{}*{}.fits'.format(suffix[0],suf))))\n",
    "        calfiles_ifua = sort_files_wl(calfiles_ifua)\n",
    "        print(len(calfiles_ifua))\n",
    "        print(calfiles_ifua)\n",
    "\n",
    "        discard_idx_band = []\n",
    "        if discard_band_cen is not None:\n",
    "            for dd in discard_band_cen:\n",
    "                discard_idx_band.append(convert_band_to_idx(dd)) \n",
    "        cxy_med_ifua = []\n",
    "        for ff, file in enumerate(calfiles_ifua):\n",
    "            bname = splitext(file)[0]\n",
    "            print(\"*** Recentering {} ***\".format(bname))\n",
    "            if bname[-5:] == '_bpc3':\n",
    "                bname = bname[:-5]\n",
    "            outname = bname+'_cen.fits'\n",
    "            if ff in discard_idx_band:\n",
    "                os.system(\"cp {} {}\".format(file, outname))\n",
    "                cube = fits.getdata(file, 'SCI')\n",
    "                cy, cx = frame_center(cube)\n",
    "                cxy_med_ifua.append((cx,cy))\n",
    "            else:\n",
    "                cxy_med_ifua.append(recenter_cubes(file, sig=5, method=cen_method, imlib=imlib, debug=show_plots, \n",
    "                                                   crop_sz=crop_sz, suffix='_cen', verbose=True, overwrite=True))\n",
    "                if show_plots:\n",
    "                    plt.show()\n",
    "            ## if no centering just copy input file\n",
    "        #         if cen_method is None:\n",
    "        #             os.system(\"cp {} {}\".format(file, outname))\n",
    "        #    os.system(\"cp {} {}/.\".format(outname, fin3_dir)) --> NOW done after potential residual BKG subtraction\n",
    "        if discard_band_cen is not None:\n",
    "            # for bands with very low signal from the star, replace centroid coordinates with median of good coordinates in the same channel\n",
    "            for ff in discard_idx_band:\n",
    "                msg = \"WARNING: centroid in band#{} will be replaced by \".format(ff)\n",
    "                if ff%3 == 0:\n",
    "                    good_ff = [ff_tmp+ff for ff_tmp in [1,2] if ff_tmp+ff not in discard_idx_band]\n",
    "                elif ff%3 == 1:\n",
    "                    good_ff = [ff_tmp+ff for ff_tmp in [-1,1] if ff_tmp+ff not in discard_idx_band]\n",
    "                else:\n",
    "                    good_ff = [ff_tmp+ff for ff_tmp in [-2,-1] if ff_tmp+ff not in discard_idx_band]\n",
    "                if len(good_ff)==2:\n",
    "                    cxy_med_ifua[ff] = (np.median([cxy_med_ifua[good_ff[0]][0],cxy_med_ifua[good_ff[1]][0]]),\n",
    "                                        np.median([cxy_med_ifua[good_ff[0]][1],cxy_med_ifua[good_ff[1]][1]]))\n",
    "                    msg += \"median of centroid inferred in bands# {} and {}: {}\".format(good_ff[0], good_ff[1], cxy_med_ifua[ff])\n",
    "                elif len(good_ff)==1:\n",
    "                    cxy_med_ifua[ff] = (cxy_med_ifua[good_ff[0]][0], cxy_med_ifua[good_ff[0]][1])\n",
    "                    msg += \"centroid inferred in band# {}: {}\".format(good_ff[0], cxy_med_ifua[ff])\n",
    "                else:\n",
    "                    msg += \"center of field of view: {}\".format(cxy_med_ifua[ff])\n",
    "                print(msg)\n",
    "        write_fits(join(spec3_dir,\"cxy_med_ifua.fits\"), np.array(cxy_med_ifua)) # write a first time to auto-generate some header keywords\n",
    "        _, head = open_fits(join(spec3_dir,\"cxy_med_ifua.fits\"), header=True)\n",
    "        chs = [1, 2, 3, 4]\n",
    "        bands = ['SHORT', 'MEDIUM','LONG']\n",
    "        c = 0    \n",
    "        print(\"Assumed XY coordinates for source spectrum extraction in the IFU-aligned cubes: \")\n",
    "        for i in chs:\n",
    "            for j in bands:\n",
    "                print(\"CH.{} {}: {}\".format(i, j, cxy_med_ifua[c]))\n",
    "                c+=1\n",
    "                head['C_PAIR{}'.format(c)] = 'CH.{} {}'.format(i,j)\n",
    "        write_fits(join(spec3_dir,\"cxy_med_ifua.fits\"), np.array(cxy_med_ifua), header=head)\n",
    "        os.system(\"cp {} {}/.\".format(join(spec3_dir,\"cxy_med_ifua.fits\"), fin3_dir))\n",
    "        source_list.cxy_med_ifua[isource] = cxy_med_ifua"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203e564a",
   "metadata": {},
   "source": [
    "### 5.3.2. Residual background subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87db35b5-f4ad-4582-aea5-3d706957986b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if subtract_residual_bkg:\n",
    "    suf_bkg = '_rbgsub'\n",
    "else:\n",
    "    suf_bkg = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5342f215-d7a4-4c68-b1b1-df746eadbe6e",
   "metadata": {},
   "source": [
    "**For skyalign cubes:** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f5bd0d-7922-42bc-9b63-142e7bc8c5ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if do_sec5:\n",
    "    for isource in range(source_list.number_of_sources):\n",
    "        source = source_list.source_names[isource]\n",
    "        print(f'Source {isource+1}: {source}:')\n",
    "        input_dir = source_list.input_dirs[isource]\n",
    "        input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "        det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "        det1_dir = source_list.det1_dirs[isource]\n",
    "        det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "        spec2_dir = source_list.spec2_dirs[isource]\n",
    "        spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "        spec3_dir = source_list.spec3_dirs[isource]\n",
    "        fin3_dir = source_list.fin3_dirs[isource]\n",
    "        cxy_med = source_list.cxy_med[isource]\n",
    "        cxy_med_ifua = source_list.cxy_med_ifua[isource]\n",
    "        \n",
    "        # List relevant files\n",
    "        calfiles=sorted(glob.glob(join(spec3_dir,'Level3{}*{}_cen.fits'.format(suffix[0], suf))))\n",
    "        calfiles = sort_files_wl(calfiles)\n",
    "        print(len(calfiles))\n",
    "        \n",
    "        if subtract_residual_bkg:\n",
    "            for f, file in enumerate(calfiles):\n",
    "                file_x1d = file.replace('{}_cen'.format(suf),'x1d')\n",
    "                res_bkg_sub(file, file_x1d, cxy_med[f], r_min=r_min_res_bg_sub, suffix='_rbgsub', \n",
    "                            verbose=True, overwrite=overwrite_53)\n",
    "\n",
    "# Write s3d cubes after potential subtraction of residual background level in final output directory:\n",
    "        for ff, file in enumerate(calfiles):\n",
    "            bname = splitext(file)[0]\n",
    "            outname = bname+'{}.fits'.format(suf_bkg)\n",
    "            print(\"Copying to final directory: {}\".format(outname))\n",
    "            os.system(\"cp {} {}/.\".format(outname, fin3_dir)) #    os.system(\"cp {} {}/.\".format(outname, fin3_dir)) --> NOW done after potential residual BKG subtraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23435cc1-bbce-4de7-a0f2-5cac50b5b3b8",
   "metadata": {},
   "source": [
    "**For ifualign cubes:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6273dd-1495-4a8e-8bef-c7fc416d9d5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if do_sec5:\n",
    "    for isource in range(source_list.number_of_sources):\n",
    "        source = source_list.source_names[isource]\n",
    "        print(f'Source {isource+1}: {source}:')\n",
    "        input_dir = source_list.input_dirs[isource]\n",
    "        input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "        det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "        det1_dir = source_list.det1_dirs[isource]\n",
    "        det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "        spec2_dir = source_list.spec2_dirs[isource]\n",
    "        spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "        spec3_dir = source_list.spec3_dirs[isource]\n",
    "        fin3_dir = source_list.fin3_dirs[isource]\n",
    "        cxy_med = source_list.cxy_med[isource]\n",
    "        cxy_med_ifua = source_list.cxy_med_ifua[isource]\n",
    "        \n",
    "        # List relevant files\n",
    "        calfiles_ifua=sorted(glob.glob(join(spec3_dir,'Level3_ifua{}*{}_cen.fits'.format(suffix[0], suf))))\n",
    "        calfiles_ifua = sort_files_wl(calfiles_ifua)\n",
    "        print(len(calfiles_ifua))\n",
    "        print(calfiles_ifua)\n",
    "        \n",
    "        if subtract_residual_bkg:\n",
    "            for f, file in enumerate(calfiles_ifua):\n",
    "                file_x1d = file.replace('{}_cen'.format(suf),'x1d')\n",
    "                res_bkg_sub(file, file_x1d, cxy_med_ifua[f], r_min=r_min_res_bg_sub, suffix='_rbgsub', \n",
    "                            verbose=True, overwrite=overwrite_53)        \n",
    "\n",
    "# Write s3d cubes after potential subtraction of residual background level in final output directory:\n",
    "        for ff, file in enumerate(calfiles_ifua):\n",
    "            bname = splitext(file)[0]\n",
    "            outname = bname+'{}.fits'.format(suf_bkg)\n",
    "            print(\"Copying to final directory: {}\".format(outname))\n",
    "            os.system(\"cp {} {}/.\".format(outname, fin3_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff58aa4-9b2e-4b5d-8a76-5206eb7558af",
   "metadata": {},
   "source": [
    "### 5.3.3. Aperture photometry + spike filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af69c8e",
   "metadata": {},
   "source": [
    "**Using `extract_1d`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f8014e",
   "metadata": {},
   "source": [
    "Note 1: the aperture correction factors are not used in `extract_1d` if src_type is defined as EXTENDED.\n",
    "\n",
    "Note 2: we systematically run `extract_1d` whether it is the desired output or not. In the latter case, it is used to save the manually extracted spectrum into the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abf35c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractspec(filename, outdir, overwrite, cxy=None, autocen=False,\n",
    "                do_rfc1d_e1d=False, subtract_background=False):\n",
    "    #print(filename)\n",
    "    #suffix = make_spec1d_suffix(autocen=autocen, do_rfc1d_e1d=do_rfc1d_e1d, \n",
    "    #                            subtract_background=subtract_background)\n",
    "    suffix = 'x1d_minds'\n",
    "    if do_rfc1d_e1d:\n",
    "        suffix += '_rfc1d'\n",
    "    outfile = splitext(filename)[0]+'_'+suffix+'.fits'\n",
    "\n",
    "    print(outfile)\n",
    "    if not isfile(outfile) or overwrite:\n",
    "        crds_config = Extract1dStep.get_config_from_reference(filename)\n",
    "        e1d = Extract1dStep.from_config_section(crds_config) # Instantiate the pipeline\n",
    "\n",
    "        if autocen:\n",
    "            e1d.ifu_autocen = True\n",
    "        else:\n",
    "            e1d.center_xy = list(cxy)\n",
    "            e1d.ifu_autocen = False\n",
    "        \n",
    "        if do_rfc1d_e1d:\n",
    "            e1d.ifu_rfcorr = True\n",
    "            \n",
    "        if overwrite_target_classification:\n",
    "            e1d.ifu_set_srctype = new_sourcetype\n",
    "        e1d.ifu_rscale = apsize\n",
    "        \n",
    "        e1d.subtract_background = subtract_background\n",
    "        \n",
    "        # force the center of the aperture to be at the center of the images (since they've been recentered)\n",
    "        #if extended:\n",
    "        #    e1d.apply_apcorr=False\n",
    "        e1d.suffix = suffix\n",
    "        e1d.save_results = True \n",
    "        e1d.output_dir = outdir\n",
    "        e1d(filename) # Run the pipeline on the science calibrated files\n",
    "        \n",
    "    return outfile, suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c242b535",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_sec5:\n",
    "    for isource in range(source_list.number_of_sources):\n",
    "        source = source_list.source_names[isource]\n",
    "        print(f'Source {isource+1}: {source}:')\n",
    "        input_dir = source_list.input_dirs[isource]\n",
    "        input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "        det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "        det1_dir = source_list.det1_dirs[isource]\n",
    "        det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "        spec2_dir = source_list.spec2_dirs[isource]\n",
    "        spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "        spec3_dir = source_list.spec3_dirs[isource]\n",
    "        fin3_dir = source_list.fin3_dirs[isource]\n",
    "        cxy_med = source_list.cxy_med[isource]\n",
    "        cxy_med_ifua = source_list.cxy_med_ifua[isource]\n",
    "        \n",
    "# Run extractspec a first time without 1D residual fringe correction:\n",
    "        # List relevant files\n",
    "        if bkg_method=='annulus':\n",
    "            calfiles=sorted(glob.glob(join(spec3_dir,'Level3_ifua{}*{}_cen.fits'.format(suffix[0], suf)))) #don't take manually sub residual bkg - done better internally\n",
    "        else:\n",
    "            calfiles=sorted(glob.glob(join(spec3_dir,'Level3_ifua{}*{}_cen{}.fits'.format(suffix[0], suf, suf_bkg))))\n",
    "        calfiles = sort_files_wl(calfiles)\n",
    "        bkgfiles=[]\n",
    "        for c in calfiles:\n",
    "            if bkg_method=='sdither':\n",
    "                basecal = basename(c).replace('Level3_ifua_psf','Level3_ifua_bkg')\n",
    "                basecal = basecal.replace('_cen.fits','.fits')\n",
    "                bkgfiles.append(join(spec3_dir,basecal))\n",
    "            else:\n",
    "                bkgfiles.append(None)\n",
    "        print(len(calfiles))\n",
    "        \n",
    "        if not psff:\n",
    "            calfiles_x1d = []\n",
    "            for f, file in enumerate(calfiles):\n",
    "                cf, fin_suff = extractspec(file, spec3_dir,\n",
    "                                           cxy=(cxy_med_ifua[f][0],cxy_med_ifua[f][1]), overwrite=overwrite_53,\n",
    "                                           autocen=autocen, do_rfc1d_e1d=False, \n",
    "                                           subtract_background=(bkg_method == \"annulus\"))\n",
    "                calfiles_x1d.append(cf)\n",
    "            source_list.calfiles_x1d[isource] = calfiles_x1d\n",
    "\n",
    "# If requested, run extractspec a second time with 1D residual fringe correction:\n",
    "        if not psff and do_rfc1d:\n",
    "            calfiles_x1d_rfc1d = []\n",
    "            for f, file in enumerate(calfiles):\n",
    "                cf, fin_suff_rfc1d = extractspec(file, spec3_dir,\n",
    "                                                 cxy=(cxy_med_ifua[f][0],cxy_med_ifua[f][1]), overwrite=overwrite_53,\n",
    "                                                 autocen=autocen, do_rfc1d_e1d=do_rfc1d_bef_leak, \n",
    "                                                 subtract_background=(bkg_method == \"annulus\"))\n",
    "                calfiles_x1d_rfc1d.append(cf)        \n",
    "            source_list.calfiles_x1d_rfc1d[isource] = calfiles_x1d_rfc1d\n",
    "        \n",
    "# Manual extraction method based on photutils                \n",
    "        # Extract spectra from the science residual_fringes files (if no specific bg observation)\n",
    "        #calfiles_x1d=sorted(glob.glob(join(spec3_dir,'*Level3{}*_x1d.fits'.format(suffix[0]))))  # used to create outputs with the same format\n",
    "\n",
    "        nfile=len(calfiles)\n",
    "\n",
    "        if isinstance(spike_filtering, bool):\n",
    "            spike_filtering = [spike_filtering]*nfile\n",
    "        if np.isscalar(sp_sig):\n",
    "            sp_sig = [sp_sig]*nfile\n",
    "\n",
    "        base_suffix = '_s3d_cen'\n",
    "        if bkg_method != 'annulus':\n",
    "            base_suffix += suf_bkg\n",
    "\n",
    "        if psff:\n",
    "            calfiles_x1d=[calfiles[i].replace('s3d_cen{}'.format(suf_bkg),'x1d') for i in range(len(calfiles))]\n",
    "            suffix_ex1d = base_suffix+'_x1d_ap{:.1f}FWHM'.format(apsize)\n",
    "            if suf_bkg == '':\n",
    "                nchar = 13 # number of characters to be removed from suffix_ex1d when printing labels in plots\n",
    "            else:\n",
    "                nchar = 20\n",
    "            if do_sec5:\n",
    "                for ii in range(nfile):\n",
    "                    extract_ap(calfiles[ii], calfiles_x1d[ii], suffix=suffix_ex1d, fname_bkg=bkgfiles[ii], cxy=cxy_med_ifua[ii], \n",
    "                               apcorr=apcorr, apcorr_fn=apcorr_fn, list_aps=list_aps, apsize=apsize, \n",
    "                               spike_filtering=spike_filtering[ii], sp_corr_win=sp_corr_win, sp_sig=sp_sig[ii], \n",
    "                               include_bkg_err=(bkg_method=='sdither'), ann_removal=(bkg_method=='annulus'), \n",
    "                               max_nspax=max_nspax, neg_only=neg_only, verbose=True, overwrite=overwrite_53, debug=False)\n",
    "        elif do_rfc1d:# and do_rfc1d_bef_leak:\n",
    "            suffix_ex1d = base_suffix+'_'+fin_suff_rfc1d #'_x1d_minds_rfc1d'\n",
    "            nchar = len(suffix_ex1d) # number of characters to be removed from suffix_ex1d when printing labels in plots\n",
    "        else:\n",
    "            suffix_ex1d = base_suffix+'_'+fin_suff #'_x1d_minds'\n",
    "            nchar = len(suffix_ex1d) # number of characters to be removed from suffix_ex1d when printing labels in plots\n",
    "\n",
    "        # rename existing x1d files from vanilla pipeline run\n",
    "        if do_rfc1d:\n",
    "            sstring = os.path.join(spec3_dir, f'Level3_ifua_rfc1d_psf*x1d.fits')\n",
    "        else:\n",
    "            sstring = os.path.join(spec3_dir, f'Level3_ifua_psf*x1d.fits')\n",
    "        x1d_files = sorted(glob.glob(sstring))\n",
    "        x1d_files = [x1d_files[i] for i in range(len(x1d_files)) if not '_{}_cen'.format(suf) in x1d_files[i]]\n",
    "        if len(x1d_files) != 12:\n",
    "            print(\"{} 'vanilla' files found, which is different than 12. Check this is the intended behaviour.\".format(len(x1d_files)))\n",
    "            pdb.set_trace()\n",
    "        for xff, xfile in enumerate(x1d_files):\n",
    "            nxfile = xfile.replace('_x1d.fits', '_x1d_vanilla.fits')\n",
    "            # CORRECT RFC1D HERE and include in x1d_vanilla\n",
    "            os.system(\"cp {} {}\".format(os.path.join(spec3_dir, xfile),\n",
    "                                        os.path.join(spec3_dir, nxfile)))                \n",
    "else:\n",
    "    base_suffix = '_s3d_cen'\n",
    "    if bkg_method != 'annulus':\n",
    "        base_suffix += suf_bkg\n",
    "    fin_suff = 'x1d_minds'\n",
    "    fin_suff_rfc1d = fin_suff+'_rfc1d'    \n",
    "    if psff:\n",
    "        suffix_ex1d = base_suffix+'_x1d_ap{:.1f}FWHM'.format(apsize)\n",
    "        if suf_bkg == '':\n",
    "            nchar = 13 # number of characters to be removed from suffix_ex1d when printing labels in plots\n",
    "        else:\n",
    "            nchar = 20\n",
    "    elif do_rfc1d:# and do_rfc1d_bef_leak:\n",
    "        suffix_ex1d = base_suffix+'_'+fin_suff_rfc1d #'_x1d_minds_rfc1d'\n",
    "        nchar = len(suffix_ex1d) # number of characters to be removed from suffix_ex1d when printing labels in plots\n",
    "    else:\n",
    "        suffix_ex1d = base_suffix+'_'+fin_suff #'_x1d_minds'  \n",
    "        nchar = len(suffix_ex1d) # number of characters to be removed from suffix_ex1d when printing labels in plots\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddec366",
   "metadata": {},
   "source": [
    "### 5.3.4. Spectral leak fix in band 3A\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "This step runs the spectral leak fix to remove the flux at 12.2 micron leaking through the dichroics - on the spectra extracted at the centroid locations inferred in the MINDS notebook (otherwise it is also done in stage 3 for the default pipeline-extracted spectrum).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e34b1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writel3asn(scifiles, asnfile, prodname, bg_files=None):\n",
    "    # Define the basic association of science files\n",
    "    asn = afl.asn_from_list(scifiles, rule=DMS_Level3_Base, product_name=prodname)\n",
    "        \n",
    "    if bg_files is not None:   # Add background files to the association\n",
    "        nbg=len(bg_files)\n",
    "        for ii in range(0, nbg):\n",
    "            asn['products'][0]['members'].append({'expname': bg_files[ii], 'exptype': 'background'})\n",
    "        \n",
    "    # Write the association to a json file\n",
    "    _, serialized = asn.dump()\n",
    "    with open(asnfile, 'w') as outfile:\n",
    "        outfile.write(serialized)\n",
    "        \n",
    "        outname_ifua = 'Level3_ifua{}'.format(suffix[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27b2ecf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if do_sec5:\n",
    "    for isource in range(source_list.number_of_sources):\n",
    "        source = source_list.source_names[isource]\n",
    "        print(f'Source {isource+1}: {source}:')\n",
    "        input_dir = source_list.input_dirs[isource]\n",
    "        input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "        det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "        det1_dir = source_list.det1_dirs[isource]\n",
    "        det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "        spec2_dir = source_list.spec2_dirs[isource]\n",
    "        spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "        spec3_dir = source_list.spec3_dirs[isource]\n",
    "        fin3_dir = source_list.fin3_dirs[isource]\n",
    "        cxy_med = source_list.cxy_med[isource]\n",
    "        cxy_med_ifua = source_list.cxy_med_ifua[isource]\n",
    "        calfiles_x1d = source_list.calfiles_x1d[isource]\n",
    "        calfiles_x1d_rfc1d = source_list.calfiles_x1d_rfc1d[isource]\n",
    "        l3asn_dir = source_list.l3asn_dirs[isource]\n",
    "\n",
    "        if do_leak_correction:\n",
    "            # open files as datamodel\n",
    "            file_list = calfiles_x1d\n",
    "            print(calfiles_x1d)\n",
    "            for i in range(len(calfiles_x1d)):\n",
    "                if 'ch1-medium' in calfiles_x1d[i]:\n",
    "                    ind1b = i\n",
    "                if 'ch3-short' in calfiles_x1d[i]:\n",
    "                    ind3a = i\n",
    "\n",
    "            dm1b = fits.open(calfiles_x1d[ind1b])\n",
    "            dm3a = fits.open(calfiles_x1d[ind3a])\n",
    "\n",
    "            if psff:\n",
    "                ## get columns\n",
    "                dm1b_tab = dm1b[1].data\n",
    "                wavelength1b = dm1b_tab['WAVELENGTH']\n",
    "                flux1b = dm1b_tab['FLUX']\n",
    "\n",
    "                dm3a_tab = dm3a[1].data\n",
    "                wavelength3a = dm3a_tab['WAVELENGTH']\n",
    "                flux3a = dm3a_tab['FLUX']\n",
    "                dm1b.close()\n",
    "\n",
    "                # Read the measured transmission curves from the csv files\n",
    "                # zeroth colum is wavelength [micrometer]\n",
    "                # first column is room temperature transmission\n",
    "                # second column is 7K transmission\n",
    "                col = 2\n",
    "                filterWave= np.genfromtxt(join(psff_dir,'MRSFilterTransmissions/',\"fm_dichroics_1a.csv\"), delimiter=\";\")[:,0]\n",
    "                D1A = np.genfromtxt(join(psff_dir,'MRSFilterTransmissions/', \"fm_dichroics_1a.csv\"), delimiter=\";\")[:,col]/100.\n",
    "                D2A = np.genfromtxt(join(psff_dir,'MRSFilterTransmissions/', \"fm_dichroics_2a.csv\"), delimiter=\";\")[:,col]/100.\n",
    "                D3A = np.genfromtxt(join(psff_dir,'MRSFilterTransmissions/', \"fm_dichroics_3a.csv\"), delimiter=\";\")[:,col]/100.\n",
    "                system_transmission = D1A*D2A*(1-D3A)\n",
    "                system_transmission_int = interp1d(filterWave+0.02,system_transmission)\n",
    "\n",
    "                # correct\n",
    "                corr = flux1b*system_transmission_int(wavelength1b)\n",
    "                corr_int = interp1d(wavelength1b,corr)\n",
    "                ind = np.where(np.array(wavelength3a)/2>wavelength1b[-1])[0][0]\n",
    "                flux3a[:ind] = flux3a[:ind] - corr_int(np.array(wavelength3a[:ind])/2)\n",
    "\n",
    "                dm3a[1].data['FLUX'] = flux3a\n",
    "                dm3a.writeto(calfiles_x1d[ind3a], overwrite=True)\n",
    "                dm3a.close()\n",
    "            else:\n",
    "                sls = SpectralLeakStep()\n",
    "                mc_dm = ModelContainer(os.path.join(l3asn_dir, 'l3asn_ifua{}.json'.format(suffix[0])))#[idm1b, idm3a])\n",
    "                crds_config = sls.get_reference_file(mc_dm, 'mrsptcorr')\n",
    "                if do_rfc1d:\n",
    "                    files1b_x1d = calfiles_x1d_rfc1d[ind1b]\n",
    "                    files3a_x1d = calfiles_x1d_rfc1d[ind3a]\n",
    "                else:\n",
    "                    files1b_x1d = calfiles_x1d[ind1b]\n",
    "                    files3a_x1d = calfiles_x1d[ind3a]        \n",
    "                with datamodels.open(files1b_x1d) as idm1b:\n",
    "                    with datamodels.open(files3a_x1d) as idm3a:\n",
    "                        flux3a_corr = spectral_leak.do_correction(crds_config, idm1b, idm3a)\n",
    "                dm1b.close()\n",
    "                dm3a[1].data['FLUX'] = flux3a_corr\n",
    "                if do_rfc1d:\n",
    "                    dm3a.writeto(calfiles_x1d_rfc1d[ind3a], overwrite=True)\n",
    "                else:\n",
    "                    dm3a.writeto(calfiles_x1d[ind3a], overwrite=True)\n",
    "                dm3a.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3331c8a4",
   "metadata": {},
   "source": [
    "If rfc1d is requested **after** spectral leak correction, run it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dab844",
   "metadata": {},
   "outputs": [],
   "source": [
    "if do_sec5:\n",
    "    for isource in range(source_list.number_of_sources):\n",
    "        source = source_list.source_names[isource]\n",
    "        print(f'Source {isource+1}: {source}:')\n",
    "        input_dir = source_list.input_dirs[isource]\n",
    "        input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "        det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "        det1_dir = source_list.det1_dirs[isource]\n",
    "        det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "        spec2_dir = source_list.spec2_dirs[isource]\n",
    "        spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "        spec3_dir = source_list.spec3_dirs[isource]\n",
    "        fin3_dir = source_list.fin3_dirs[isource]\n",
    "        cxy_med = source_list.cxy_med[isource]\n",
    "        cxy_med_ifua = source_list.cxy_med_ifua[isource]\n",
    "        calfiles_x1d = source_list.calfiles_x1d[isource]\n",
    "        calfiles_x1d_rfc1d = source_list.calfiles_x1d_rfc1d[isource]\n",
    "        l3asn_dir = source_list.l3asn_dirs[isource]\n",
    "        \n",
    "        if do_rfc1d and not do_rfc1d_bef_leak:\n",
    "            for ff, file in enumerate(calfiles_x1d_rfc1d):\n",
    "                print('processing {}'.format(basename(file)))\n",
    "\n",
    "                hdul = fits.open(file)\n",
    "                hdul.verify('ignore')\n",
    "                channel = int(hdul[0].header['CHANNEL'])\n",
    "                # open file as datamodel\n",
    "                dm = fits.open(file)\n",
    "                # get columns\n",
    "                wavelength = dm.spec[0].spec_table['WAVELENGTH']\n",
    "                temp_flux = dm.spec[0].spec_table['FLUX']\n",
    "                surf_bright = dm.spec[0].spec_table['SURF_BRIGHT']\n",
    "                background = dm.spec[0].spec_table['BACKGROUND']\n",
    "\n",
    "                # Apply residual fringe to the flux array\n",
    "                try:\n",
    "                    temp_flux = rfutils.fit_residual_fringes_1d(temp_flux, wavelength, channel=channel,\n",
    "                                                                dichroic_only=False, max_amp=None)\n",
    "                except Exception:\n",
    "                    log.info(\"Flux residual fringe correction failed- skipping.\")\n",
    "\n",
    "                # Apply residual fringe to the surf_bright array\n",
    "                try:\n",
    "                    surf_bright = rfutils.fit_residual_fringes_1d(surf_bright, wavelength, channel=channel,\n",
    "                                                                  dichroic_only=False, max_amp=None)\n",
    "                except Exception:\n",
    "                    log.info(\"Surf bright residual fringe correction failed- skipping.\")\n",
    "\n",
    "                # Apply residual fringe to the background array\n",
    "                try:\n",
    "                    background = rfutils.fit_residual_fringes_1d(background, wavelength, channel=channel,\n",
    "                                                                 dichroic_only=False, max_amp=None)\n",
    "                except Exception:\n",
    "                    log.info(\"Background residual fringe correction failed- skipping.\")\n",
    "\n",
    "                dm[1].data['FLUX'] = temp_flux\n",
    "                dm[1].data['SURF_BRIGHT'] = surf_bright\n",
    "                dm[1].data['BACKGROUND'] = background\n",
    "                dm.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac581c8e",
   "metadata": {},
   "source": [
    "### 5.4. Stitch and write results in final directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dddcac1",
   "metadata": {},
   "source": [
    "(+ if relevant: gathering of spectra obtained from past reductions, application of scaling factors to stitch bands)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e2c2d5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "This step runs the additional Residual Fringe step to correct fringes originating in the MRS dichroics at the spectrum level (particularly for Channels 3 and 4). \n",
    "After running the Spec3pipeline, this step would be run on the x1d.fits files, producing _x1d_rfc1d.txt files and plotting them, together with the extracted spectra on which this final fringe correction is not performed for comparison.\n",
    "    \n",
    "**Note: if Danny's reference files that are optimised for point sources are used (i.e. `psff=True`), it is recommended to skip the residual fringe correction**\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb87e8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for isource in range(source_list.number_of_sources):\n",
    "    source = source_list.source_names[isource]\n",
    "    print(f'Source {isource+1}: {source}:')\n",
    "    input_dir = source_list.input_dirs[isource]\n",
    "    input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "    det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "    det1_dir = source_list.det1_dirs[isource]\n",
    "    det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "    spec2_dir = source_list.spec2_dirs[isource]\n",
    "    spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "    spec3_dir = source_list.spec3_dirs[isource]\n",
    "    fin3_dir = source_list.fin3_dirs[isource]\n",
    "    cxy_med = source_list.cxy_med[isource]\n",
    "    cxy_med_ifua = source_list.cxy_med_ifua[isource]\n",
    "    calfiles_x1d = source_list.calfiles_x1d[isource]\n",
    "    calfiles_x1d_rfc1d = source_list.calfiles_x1d_rfc1d[isource]\n",
    "    l3asn_dir = source_list.l3asn_dirs[isource]\n",
    "\n",
    "    # gather the spectra\n",
    "    files = glob.glob(join(spec3_dir, 'Level3_ifua{}*{}.fits'.format(suffix[0], suffix_ex1d)))\n",
    "    # sort them by WL\n",
    "    files = sort_files_wl(files)\n",
    "    files_bn = [splitext(files[i])[0] for i in range(len(files))]\n",
    "    print(\"Found spectra:\")\n",
    "    for f in files:\n",
    "        print(f)\n",
    "\n",
    "# define suffix for output filename, as a function of selected options\n",
    "    lab = ''\n",
    "    if use_agg_det1_params:\n",
    "        lab += '_aggdet1'\n",
    "    if do_bpc1:\n",
    "        lab += '_bpc1'\n",
    "    if not do_bpc2:\n",
    "        lab += '_nobpc2'\n",
    "    if do_bpc3:\n",
    "        lab += '_bpc3'\n",
    "    if bg_observation:\n",
    "        lab += '_bg_observation'\n",
    "    else:\n",
    "        lab += '_'+bkg_method\n",
    "    if cen_method is not None:\n",
    "        lab += '_'+cen_method\n",
    "    if not psff:\n",
    "        lab += '_nopsff'\n",
    "    if dith_combi_method != 'drizzle':\n",
    "        lab += '_{}'.format(dith_combi_method) # {'drizzle', 'emsm', 'msm'}\n",
    "    if do_rfc1d and '_rfc1d' not in files[0]: # second condition added - to be tested.\n",
    "        lab += '_rfc1d'\n",
    "    if correct_jumps is not None:\n",
    "        lab += '_'+correct_jumps\n",
    "    if psff:\n",
    "        if np.sum(spike_filtering) == 0: # spike_filtering is necessarily a list of 12 elements at this stage\n",
    "            lab += '_nospikefilt'\n",
    "        else:\n",
    "            if neg_only:\n",
    "                lab += '_neg-spikefilt'\n",
    "            if np.sum(spike_filtering) < 12:\n",
    "                spfilt_int = [str(int(spike_filtering[i])) for i in range(len(spike_filtering))]\n",
    "                if neg_only:\n",
    "                    lab += ''.join(spfilt_int)\n",
    "                else:\n",
    "                    lab += '_spikefilt'+''.join(spfilt_int)\n",
    "    if overwrite_target_classification and not psff:\n",
    "        lab += '_'+new_sourcetype\n",
    "    lab\n",
    "        \n",
    "# find other existing spectra (e.g. from previous reduction with different parameters)\n",
    "    all_txt_files = glob.glob(join(spec3_dir, '*.txt'))\n",
    "    other_txt_files = [txtf for txtf in all_txt_files if basename(txtf).startswith('Level3_ifua{}'.format(suffix[0]))]\n",
    "    for file_tmp in files_bn:\n",
    "        for ot_files in other_txt_files:\n",
    "            if file_tmp in ot_files and basename(ot_files).endswith(lab+'.txt'):\n",
    "                other_txt_files.remove(ot_files)\n",
    "                break\n",
    "\n",
    "    nsuff = len(other_txt_files)//len(files)\n",
    "    suff_tmp = []\n",
    "    o_nchars = [] # number of characters to remove for labels in plots, for other files\n",
    "    for f, ftmp in enumerate(other_txt_files):\n",
    "        if ftmp.startswith(splitext(files[0])[0][:-len(suffix_ex1d)]):# or ftmp.startswith(splitext(other_txt_files[0])[0][:-len(suffix_ex1d)]):\n",
    "            flen = len(files[0][:-len(suffix_ex1d+'.fits')])\n",
    "            if \"_x1d_ap\" in ftmp:\n",
    "                add_len = len('_s3d_cen_x1d_')\n",
    "                suff_tmp.append(splitext(ftmp[flen+add_len:])[0])\n",
    "            else:\n",
    "                add_len = len('_s3d_cen_x1d_minds_')\n",
    "                suff_tmp.append(splitext(ftmp[flen+add_len:])[0])\n",
    "\n",
    "    if len(suff_tmp) != nsuff:\n",
    "        pdb.set_trace() # double-check why there is a problem.\n",
    "\n",
    "    if nsuff>0:\n",
    "        other_files = [None]*nsuff    \n",
    "        for s in range(nsuff):\n",
    "            sorted_ofiles = glob.glob(join(spec3_dir, '*{}.txt'.format(suff_tmp[s])))\n",
    "            if spec_mode == 'per_band':\n",
    "                sorted_ofiles.sort()\n",
    "                # sort by wavelength (i.e. not alphabetically for bands spectra)\n",
    "                sorted_otfiles = []\n",
    "                for ff, fi in enumerate(sorted_ofiles):\n",
    "                    if ff%3 == 1 or ff%3 == 2:\n",
    "                        sorted_otfiles.insert(len(sorted_otfiles)-ff%3, fi)\n",
    "                    else:\n",
    "                        sorted_otfiles.append(fi)\n",
    "            other_files[s] = sorted_otfiles\n",
    "    else:\n",
    "        other_files = []\n",
    "\n",
    "    nfiles = len(files)\n",
    "    all_wavelengths = []\n",
    "    all_fluxes = []\n",
    "    all_scal_facs = []\n",
    "    # Find scaling factors to correct for jumps between bands. Use Occam Razor argument to either scale or correct for slope.\n",
    "    nbad_ch_for_scaling = 5 # number of edge channels removed when considering stitching based on overlapping wavelengths\n",
    "\n",
    "    for ff, file in enumerate(files):\n",
    "        print('processing {}'.format(basename(file)))\n",
    "\n",
    "        # open file as datamodel\n",
    "        dm = datamodels.open(file)\n",
    "        # get columns\n",
    "        ori_wavelength = dm.spec[0].spec_table['WAVELENGTH']\n",
    "        flux = dm.spec[0].spec_table['FLUX']\n",
    "\n",
    "\n",
    "    #     if type(nch_cut_per_band) == str:\n",
    "    #         if \"red\" in nch_cut_per_band:\n",
    "    #             if ff < len(files)-1:\n",
    "    #                 dm_next = datamodels.open(files[ff+1])\n",
    "    #                 next_wavelengths = dm_next.spec[0].spec_table['WAVELENGTH']\n",
    "    #                 idx_overlap = find_nearest(ori_wavelength, next_wavelengths[0], constraint='floor')\n",
    "    #                 if ff == 0:\n",
    "    #                     nch_bad_per_band = [(0,len(ori_wavelength)-idx_overlap-1)]\n",
    "    #                 else: \n",
    "    #                     nch_bad_per_band.append((0,len(ori_wavelength)-idx_overlap-1))\n",
    "    #             else:\n",
    "    #                 idx_overlap = len(ori_wavelength)-1\n",
    "    #                 nch_bad_per_band.append((0,0))\n",
    "    #             wavelength = ori_wavelength[:idx_overlap+1]\n",
    "    #             flux = flux[:idx_overlap+1]\n",
    "    #         else:\n",
    "    #             if ff == 0:\n",
    "    #                 idx_overlap = 0\n",
    "    #                 nch_bad_per_band = [(0,0)]\n",
    "    #             else:\n",
    "    #                 dm_prev = datamodels.open(files[ff-1])\n",
    "    #                 prev_wavelengths = dm_prev.spec[0].spec_table['WAVELENGTH']\n",
    "    #                 idx_overlap = find_nearest(ori_wavelength, prev_wavelengths[-1], constraint='ceil')\n",
    "    #                 nch_bad_per_band.append((idx_overlap,0))\n",
    "    #             wavelength = ori_wavelength[idx_overlap:]\n",
    "    #             flux = flux[idx_overlap:]           \n",
    "    #    elif nch_bad_per_band[ff][0]>0 or nch_bad_per_band[ff][1]>0:\n",
    "    #         wavelength = ori_wavelength[nch_bad_per_band[ff][0]:len(ori_wavelength)-nch_bad_per_band[ff][1]]\n",
    "    #         flux = flux[nch_bad_per_band[ff][0]:len(flux)-nch_bad_per_band[ff][1]]\n",
    "    #     else:\n",
    "    #         wavelength=ori_wavelength\n",
    "\n",
    "        wavelength = ori_wavelength[nbad_ch_for_scaling:-nbad_ch_for_scaling]\n",
    "        flux = flux[nbad_ch_for_scaling:-nbad_ch_for_scaling]\n",
    "\n",
    "        all_wavelengths.append(wavelength)\n",
    "        all_fluxes.append(flux)\n",
    "\n",
    "        # calculate first guess rescaling factors between bands, based on overlapping wavelengths\n",
    "        if ff and correct_jumps is not None:\n",
    "            try:\n",
    "                scal_fac = calc_scal_fac(all_wavelengths[ff-1], all_wavelengths[ff], all_fluxes[ff-1], all_fluxes[ff])\n",
    "                print(\"First guess scale factor for Ch. {} band {}: {:.2f}\".format(((ff//3)+1), (ff%3)+1, scal_fac))\n",
    "            except:\n",
    "                scal_fac = 1 # e.g. for lack of overlapping channels (as between CH4B and CH4C, after removing bad channels of CH4B)\n",
    "        else:\n",
    "            scal_fac = 1\n",
    "        all_scal_facs.append(scal_fac)            \n",
    "\n",
    "# infer when to scale and when to bend spectrum\n",
    "    if correct_jumps == 'occam':\n",
    "        final_scal_facs, do_bend = identify_scalefac_vs_bending(all_scal_facs, thr=thr_occam)\n",
    "        for i in range(len(do_bend)):\n",
    "            print(\"CH.{} B{}: scaling factor = {:.2f}, bend spectrum slope? {}\".format((i//3)+1, (i%3)+1, final_scal_facs[i], do_bend[i]))\n",
    "    elif correct_jumps == 'scale_only':\n",
    "        final_scal_facs = [all_scal_facs[0]]\n",
    "        for i in range(1,len(all_scal_facs)):\n",
    "            final_scal_facs.append(all_scal_facs[i]*final_scal_facs[i-1])\n",
    "        do_bend = [None]*len(all_scal_facs)\n",
    "    else:\n",
    "        final_scal_facs = [1]*len(all_scal_facs)\n",
    "        do_bend = [None]*len(all_scal_facs)            \n",
    "    print(final_scal_facs)    \n",
    "\n",
    "# remove existing spectra from final output directory, before writing new ones\n",
    "    exist_specfiles = glob.glob(join(fin3_dir, 'Level3_ifua_psf*.txt'))\n",
    "    for e, efile in enumerate(exist_specfiles):\n",
    "        os.system(\"rm {}\".format(efile))\n",
    "\n",
    "    exist_specfiles = glob.glob(join(fin3_dir, 'Level3_ifua_psf*x1d*fits'))\n",
    "    for e, efile in enumerate(exist_specfiles):\n",
    "        os.system(\"rm {}\".format(efile))\n",
    "\n",
    "    for ff, file in enumerate(files):\n",
    "        print('processing {}'.format(basename(file)))\n",
    "\n",
    "        hdul = fits.open(file)\n",
    "        hdul.verify('ignore')\n",
    "        channel = hdul[0].header['CHANNEL']\n",
    "        # open file as datamodel\n",
    "        dm = datamodels.open(file)\n",
    "        # get columns\n",
    "        ori_wavelength = dm.spec[0].spec_table['WAVELENGTH']\n",
    "        flux = dm.spec[0].spec_table['FLUX']\n",
    "\n",
    "        if type(nch_cut_per_band) == str:\n",
    "            if \"red\" in nch_cut_per_band:\n",
    "                if ff < len(files)-1:\n",
    "                    dm_next = datamodels.open(files[ff+1])\n",
    "                    next_wavelengths = dm_next.spec[0].spec_table['WAVELENGTH']\n",
    "                    idx_overlap = find_nearest(ori_wavelength, next_wavelengths[0], constraint='floor')\n",
    "                    if ff == 0:\n",
    "                        nch_bad_per_band = [(0,len(ori_wavelength)-idx_overlap-1)]\n",
    "                    else: \n",
    "                        nch_bad_per_band.append((0,len(ori_wavelength)-idx_overlap-1))\n",
    "                else:\n",
    "                    idx_overlap = len(ori_wavelength)-1\n",
    "                    nch_bad_per_band.append((0,0))\n",
    "                wavelength = ori_wavelength[:idx_overlap+1]\n",
    "                flux = flux[:idx_overlap+1]\n",
    "            else:\n",
    "                if ff == 0:\n",
    "                    idx_overlap = 0\n",
    "                    nch_bad_per_band = [(0,0)]\n",
    "                else:\n",
    "                    dm_prev = datamodels.open(files[ff-1])\n",
    "                    prev_wavelengths = dm_prev.spec[0].spec_table['WAVELENGTH']\n",
    "                    idx_overlap = find_nearest(ori_wavelength, prev_wavelengths[-1], constraint='ceil')\n",
    "                    nch_bad_per_band.append((idx_overlap,0))\n",
    "                wavelength = ori_wavelength[idx_overlap:]\n",
    "                flux = flux[idx_overlap:]\n",
    "        elif nch_bad_per_band[ff][0]>0 or nch_bad_per_band[ff][1]>0:\n",
    "            wavelength = ori_wavelength[nch_bad_per_band[ff][0]:len(ori_wavelength)-nch_bad_per_band[ff][1]]\n",
    "            flux = flux[nch_bad_per_band[ff][0]:len(flux)-nch_bad_per_band[ff][1]]\n",
    "        else:\n",
    "            wavelength=ori_wavelength\n",
    "\n",
    "        # identify first wavelength of CH4A\n",
    "        if (ff//3)+1 == 4 and (ff%3)+1 == 1: #file == files[-3]:\n",
    "            print(\"First wavelength of CH4A recorded\")\n",
    "            wl0_ch4a = wavelength[0]\n",
    "\n",
    "        # apply either unique scaling factor or bending factors:\n",
    "        if not ff:\n",
    "            scal_fac = 1\n",
    "        elif do_bend[ff]:\n",
    "            scal_fac = bend_spectrum(final_scal_facs[ff], final_scal_facs[ff+1], wavelength)\n",
    "            print(\"Bend spectrum for Ch. {} band {} (linear interp. from {:.2f} to {:.2f})\".format(((ff//3)+1), (ff%3)+1, final_scal_facs[ff], final_scal_facs[ff+1]))\n",
    "        else:\n",
    "            scal_fac = final_scal_facs[ff]\n",
    "            print(\"Final scale factor for Ch. {} band {}: {:.2f}\".format(((ff//3)+1), (ff%3)+1, scal_fac))\n",
    "        flux*= scal_fac\n",
    "\n",
    "        # build final spectrum\n",
    "        if ff==0:\n",
    "            all_wavelength = list(wavelength)\n",
    "            all_corrected_flux = list(flux)\n",
    "        else:\n",
    "            all_wavelength.extend(wavelength)\n",
    "            all_corrected_flux.extend(flux)\n",
    "\n",
    "        # save the spectrum to a text file\n",
    "        out_name = splitext(file)[0] + '{}.txt'.format(lab)\n",
    "        out_array = np.savetxt(out_name, np.array([wavelength, flux]).T)\n",
    "        ## copy final spectrum to final output directory\n",
    "        bname = basename(out_name)\n",
    "\n",
    "        ## different suffixes for clarity\n",
    "        out_name_tmp = join(fin3_dir,basename(splitext(file)[0]))\n",
    "        if psff:\n",
    "            out_name_tmp += '_psff'\n",
    "        if do_rfc1d:\n",
    "            out_name_tmp += '_rfc1d'\n",
    "\n",
    "        out_name_txt = out_name_tmp + '.txt'\n",
    "        out_name_x1d = out_name_tmp + '.fits'\n",
    "\n",
    "        os.system(\"cp {} {}\".format(out_name, out_name_txt))\n",
    "        \n",
    "        source_list.all_wavelength[isource] = all_wavelength\n",
    "        source_list.all_corrected_flux[isource] = all_corrected_flux\n",
    "        \n",
    "        write_x1d(out_name_x1d, file, wavelength, flux, overwrite=True)            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae3c65",
   "metadata": {},
   "source": [
    "### 5.5. Continuum subtraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495c3e6a",
   "metadata": {},
   "source": [
    "Let's first make a csv file compiling the spectrum of all bands together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dd9eef",
   "metadata": {},
   "source": [
    "For this, let's define a routine to turn all individual 1d spectrum files into a DataFrame table. This puts all channels into one table and adds columns with fringe corrected spectrum, band etc (e.g. for easier plotting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54761444",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x1d_files_to_table(x1d_fnames, x1d_fnames_rfc1d=None, nch_bad_per_band=None, correct_jumps=None, \n",
    "                       final_scal_facs=None, do_bend=None, contsub_spectrum=None):\n",
    "    def _correct_spectrum(ff, x1d_fname):\n",
    "        try:\n",
    "            dm = datamodels.open(x1d_fname)\n",
    "            wl = np.array(dm.spec[0].spec_table['WAVELENGTH'])\n",
    "        except:\n",
    "            hdu = fits.open(x1d_fname)\n",
    "            wl = hdu[1].data['WAVELENGTH']\n",
    "            \n",
    "        hdul = fits.open(x1d_fname)\n",
    "        hdul.verify('ignore')\n",
    "        channel = hdul[0].header['CHANNEL']\n",
    "        table = hdul[1].data\n",
    "        cols = hdul[1].columns\n",
    "        ncols = cols.names\n",
    "        wl = table['WAVELENGTH']\n",
    "        flux = table['FLUX']\n",
    "        flux_error = table['FLUX_ERROR']\n",
    "        \n",
    "        hdul.close()        \n",
    "        \n",
    "        # remove bad chs at edge of bands\n",
    "        if nch_bad_per_band is None:\n",
    "            raise ValueError(\"Run Sec. 5.4 to define nch_bad_per_band\")\n",
    "        elif nch_bad_per_band[ff][0]>0 or nch_bad_per_band[ff][1]>0:\n",
    "            wl2 = wl[nch_bad_per_band[ff][0]:len(wl)-nch_bad_per_band[ff][1]]\n",
    "            flux2 = flux[nch_bad_per_band[ff][0]:len(flux)-nch_bad_per_band[ff][1]]\n",
    "            flux_error2 = flux_error[nch_bad_per_band[ff][0]:len(flux_error)-nch_bad_per_band[ff][1]]\n",
    "        else:\n",
    "            wl2 = np.empty(len(wl))\n",
    "            wl2[:] = wl\n",
    "            flux2 = np.empty(len(flux))\n",
    "            flux2[:] = flux\n",
    "            flux_error2 = np.empty(len(flux_error))\n",
    "            flux_error2[:] = flux_error\n",
    "        # remove channels with no information\n",
    "        wl2 = wl2[np.where(flux2!=0)]\n",
    "        flux2 = flux2[np.where(flux2!=0)]\n",
    "        flux_error2 = flux_error2[np.where(flux2!=0)]\n",
    "            \n",
    "        basename, _ = os.path.splitext(os.path.basename(fname_x1d))\n",
    "        print(basename)\n",
    "        if 'long' in basename:\n",
    "            band = 'long'\n",
    "            band_num = 'C'\n",
    "        elif 'medium' in basename:\n",
    "            band = 'medium'\n",
    "            band_num = 'B'\n",
    "        elif 'short' in basename:\n",
    "            band ='short'\n",
    "            band_num = 'A'\n",
    "\n",
    "        hdul = fits.open(fname_x1d)\n",
    "        hdul.verify('ignore')\n",
    "        channel = hdul[0].header['CHANNEL']\n",
    "        table = hdul[1].data\n",
    "        cols = hdul[1].columns\n",
    "        ncols = cols.names\n",
    "        wl = table['WAVELENGTH']\n",
    "        flux = table['FLUX']\n",
    "        flux_error = table['FLUX_ERROR']\n",
    "        \n",
    "        hdul.close()\n",
    "            \n",
    "        return wl2, flux2, flux_error2, band, band_num, channel \n",
    "    \n",
    "    def _stitch(flux2, correct_jumps, do_bend):\n",
    "        # correct jumps\n",
    "        if correct_jumps is not None:\n",
    "            # apply either unique scaling factor or bending factors:\n",
    "            if not ff:\n",
    "                scal_fac = 1\n",
    "            elif do_bend[ff]:\n",
    "                scal_fac = bend_spectrum(final_scal_facs[ff], final_scal_facs[ff+1], wavelength)\n",
    "                print(\"Bend spectrum for Ch. {} band {} (linear interp. from {:.2f} to {:.2f})\".format(((ff//3)+1), (ff%3)+1, final_scal_facs[ff], final_scal_facs[ff+1]))\n",
    "            else:\n",
    "                scal_fac = final_scal_facs[ff]\n",
    "                print(\"Final scale factor for Ch. {} band {}: {:.2f}\".format(((ff//3)+1), (ff%3)+1, scal_fac))\n",
    "            flux2*=scal_fac\n",
    "        return flux2\n",
    "\n",
    "    spectra = []\n",
    "    flux2_st = None\n",
    "    flux2_rfc1d = None\n",
    "    for ff, fname_x1d in enumerate(x1d_fnames):\n",
    "        wl2, flux2, flux_error2, band, band_num, channel = _correct_spectrum(ff, fname_x1d)\n",
    "        spectrum = pd.DataFrame({'wavelength': wl2, 'flux': flux2, 'flux_error': flux_error2})\n",
    "        \n",
    "        if correct_jumps is not None: \n",
    "            flux2_st = _stitch(flux2, correct_jumps, do_bend)\n",
    "            spectrum['flux_stitch_{}'.format(correct_jumps)] = flux2\n",
    "                \n",
    "        if x1d_fnames_rfc1d is not None:\n",
    "            wl2, flux2_rfc1d, flux_error2_rfc1d, _, _, _ = _correct_spectrum(ff, x1d_fnames_rfc1d[ff])   \n",
    "            spectrum['flux_fcorr'] = flux2_rfc1d\n",
    "            spectrum['flux_fcorr_error'] = flux_error2_rfc1d\n",
    "            if correct_jumps is not None: \n",
    "                flux2_st = _stitch(flux2_rfc1d, correct_jumps, do_bend)\n",
    "                spectrum['flux_stitch_{}_fcorr'.format(correct_jumps)] = flux2_st\n",
    "                    \n",
    "        if contsub_spectrum is not None:\n",
    "            wl_contsub = np.array(contsub_spectrum['Wavelength'])\n",
    "            spec_contsub = np.array(contsub_spectrum['CSFlux'])\n",
    "            idx0 = find_nearest(wl_contsub, wl2[0])\n",
    "            idxN = find_nearest(wl_contsub, wl2[-1])\n",
    "            if flux2_rfc1d is not None:\n",
    "                if flux2_st is not None:\n",
    "                    spectrum['flux_continuum_fcorr'] = flux2_st-spec_contsub[idx0:idxN+1]\n",
    "                else:\n",
    "                    spectrum['flux_continuum_fcorr'] = flux2_rfc1d-spec_contsub[idx0:idxN+1]\n",
    "            spectrum['flux_continuum'] = flux2-spec_contsub[idx0:idxN+1]\n",
    "            spectrum['flux_contsub'] = spec_contsub[idx0:idxN+1]\n",
    "\n",
    "        spectrum['CHANNEL'] = channel\n",
    "        spectrum['BAND'] = band\n",
    "        spectrum['BAND_NUM'] = band_num\n",
    "        band_id = f'{channel}'+f'{band_num}'\n",
    "        spectrum['BAND_ID'] = band_id\n",
    "        spectrum['wavelength_index'] = spectrum.index\n",
    "        spectra.append(spectrum)\n",
    "\n",
    "    spectrum_df = pd.concat(spectra, ignore_index=True)\n",
    "    \n",
    "    return spectrum_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca97d1f",
   "metadata": {},
   "source": [
    "Now let's save the data frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf1a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for isource in range(source_list.number_of_sources):\n",
    "    source = source_list.source_names[isource]\n",
    "    print(f'Source {isource+1}: {source}:')\n",
    "    input_dir = source_list.input_dirs[isource]\n",
    "    input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "    det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "    det1_dir = source_list.det1_dirs[isource]\n",
    "    det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "    spec2_dir = source_list.spec2_dirs[isource]\n",
    "    spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "    spec3_dir = source_list.spec3_dirs[isource]\n",
    "    fin3_dir = source_list.fin3_dirs[isource]\n",
    "    cxy_med = source_list.cxy_med[isource]\n",
    "    cxy_med_ifua = source_list.cxy_med_ifua[isource]\n",
    "    calfiles_x1d = source_list.calfiles_x1d[isource]\n",
    "    calfiles_x1d_rfc1d = source_list.calfiles_x1d_rfc1d[isource]\n",
    "    l3asn_dir = source_list.l3asn_dirs[isource]\n",
    "\n",
    "# let's first save the vanilla jwst pipeline spectrum (no rescaling involved)\n",
    "    sstring = os.path.join(spec3_dir, f'Level3_ifua_psf*x1d.fits')\n",
    "    x1d_files = sorted(glob.glob(sstring))\n",
    "    x1d_files = sort_files_wl(x1d_files)\n",
    "    x1d_files = [x1d_files[i] for i in range(len(x1d_files)) if '_{}_cen'.format(suf) not in x1d_files[i]] # just to be sure, do not consider \"s3d_cen\" suffix\n",
    "    for ff, fin_file in enumerate(x1d_files):\n",
    "        bname, _ = os.path.splitext(os.path.basename(fin_file))\n",
    "        os.system(\"cp {} {}\".format(fin_file, os.path.join(fin3_dir, bname+'_vanilla.fits')))\n",
    "    spectrum_df_vanilla_pip = x1d_files_to_table(x1d_files, nch_bad_per_band=nch_bad_per_band)\n",
    "    spectrum_df_vanilla_pip.to_csv(os.path.join(fin3_dir, f\"spectrum_vanilla.csv\"))\n",
    "    source_list.spectrum_df_vanilla_pip[isource] = spectrum_df_vanilla_pip\n",
    "    \n",
    "# now let's save the MINDS-processed spectrum\n",
    "    if psff:\n",
    "        sstring = os.path.join(spec3_dir, f'Level3_ifua_psf*'+'{}.fits'.format(suffix_ex1d))\n",
    "        x1d_files = sorted(glob.glob(sstring))\n",
    "        x1d_files = sort_files_wl(x1d_files)\n",
    "        spectrum_df = x1d_files_to_table(x1d_files, nch_bad_per_band=nch_bad_per_band, correct_jumps=correct_jumps, \n",
    "                                         final_scal_facs=final_scal_facs, do_bend=do_bend)\n",
    "        spectrum_df.to_csv(os.path.join(fin3_dir, f\"spectrum_minds_ap{apsize}FWHM_psff.csv\"))\n",
    "        source_list.spectrum_df[isource] = spectrum_df\n",
    "    else:\n",
    "        # note: this will save the MINDS-enhanced pipeline-extracted spectrum\n",
    "        sstring = os.path.join(spec3_dir, f'Level3_ifua_psf*_minds.fits')\n",
    "        x1d_files = sorted(glob.glob(sstring))\n",
    "        x1d_files = sort_files_wl(x1d_files)\n",
    "        if bkg_method == 'annulus':\n",
    "            x1d_suf = '_{}_cen'.format(suf)\n",
    "        else:\n",
    "            x1d_suf = '_{}_cen{}'.format(suf, suf_bkg)\n",
    "        x1d_files = [x1d_files[i] for i in range(len(x1d_files)) if x1d_suf in x1d_files[i]]\n",
    "        ## also include x1d_minds_rfc1d if they exist\n",
    "        if do_rfc1d:\n",
    "            sstring = os.path.join(spec3_dir, f'Level3_ifua_psf*_minds_rfc1d.fits')\n",
    "            x1d_files_rfc1d = sorted(glob.glob(sstring))\n",
    "            x1d_files_rfc1d = sort_files_wl(x1d_files_rfc1d)\n",
    "            x1d_files_rfc1d = [x1d_files_rfc1d[i] for i in range(len(x1d_files_rfc1d)) if x1d_suf in x1d_files_rfc1d[i]]\n",
    "            spectrum_df = x1d_files_to_table(x1d_files, x1d_files_rfc1d, nch_bad_per_band=nch_bad_per_band,\n",
    "                                             correct_jumps=correct_jumps, final_scal_facs=final_scal_facs,\n",
    "                                             do_bend=do_bend)\n",
    "        else:\n",
    "            x1d_files = [x1d_files[i] for i in range(len(x1d_files)) if '_minds_rfc1d.fits' not in x1d_files[i]] ## do not include any potential \"rfc1d\" files that may exist from a previous run\n",
    "            spectrum_df = x1d_files_to_table(x1d_files, nch_bad_per_band=nch_bad_per_band, correct_jumps=correct_jumps, \n",
    "                                             final_scal_facs=final_scal_facs, do_bend=do_bend)\n",
    "        spectrum_df.to_csv(os.path.join(fin3_dir, f\"spectrum_minds.csv\"))        \n",
    "        source_list.spectrum_df[isource] = spectrum_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f935dabc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if contsub:\n",
    "    for isource in range(source_list.number_of_sources):\n",
    "        source = source_list.source_names[isource]\n",
    "        print(f'Source {isource+1}: {source}:')\n",
    "        input_dir = source_list.input_dirs[isource]\n",
    "        input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "        det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "        det1_dir = source_list.det1_dirs[isource]\n",
    "        det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "        spec2_dir = source_list.spec2_dirs[isource]\n",
    "        spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "        spec3_dir = source_list.spec3_dirs[isource]\n",
    "        fin3_dir = source_list.fin3_dirs[isource]\n",
    "        cxy_med = source_list.cxy_med[isource]\n",
    "        cxy_med_ifua = source_list.cxy_med_ifua[isource]\n",
    "        calfiles_x1d = source_list.calfiles_x1d[isource]\n",
    "        calfiles_x1d_rfc1d = source_list.calfiles_x1d_rfc1d[isource]\n",
    "        l3asn_dir = source_list.l3asn_dirs[isource]\n",
    "        \n",
    "        sstring = os.path.join(spec3_dir, f'Level3_ifua_psf*x1d.fits')\n",
    "        x1d_files = sorted(glob.glob(sstring))\n",
    "        x1d_files = sort_files_wl(x1d_files)\n",
    "        x1d_files = [x1d_files[i] for i in range(len(x1d_files)) if '_{}_cen'.format(suf) not in x1d_files[i]] # just to be sure, do not consider \"s3d_cen\" suffix\n",
    "\n",
    "        if psff:\n",
    "            Spectrum = pd.read_csv(os.path.join(fin3_dir, f\"spectrum_minds_ap{apsize}FWHM_psff.csv\"))#, \n",
    "                                   #skiprows=1, usecols=[1,2], names=['Wavelength', 'Flux'])\n",
    "        else:\n",
    "            Spectrum = pd.read_csv(os.path.join(fin3_dir, f\"spectrum_minds.csv\"))#, skiprows=1, \n",
    "                                   #usecols=[1,2], names=['Wavelength', 'Flux'])\n",
    "        print(Spectrum)\n",
    "\n",
    "        StitchedWav  = Spectrum['wavelength'].to_numpy()\n",
    "        if do_rfc1d:\n",
    "            if correct_jumps is not None:\n",
    "                key = 'flux_stitch_{}_fcorr'.format(correct_jumps) \n",
    "            else:\n",
    "                key = 'flux_fcorr'\n",
    "        else:\n",
    "            if correct_jumps is not None:\n",
    "                key = 'flux_stitch_{}'.format(correct_jumps) \n",
    "            else:\n",
    "                key = 'flux'   \n",
    "        StitchedFlux = Spectrum[key].to_numpy()\n",
    "\n",
    "        # Mask required for outlier detection in the continuum fitting algorithm - only done internally of Baseline() now\n",
    "    #     Mask         = (StitchedWav <= 27.8)\n",
    "    #     StitchedWav  = StitchedWav[Mask]\n",
    "    #     StitchedFlux = StitchedFlux[Mask]\n",
    "\n",
    "\n",
    "        MaskFeature   = mask_12mu_feature\n",
    "        OStitchedFlux = StitchedFlux.copy()\n",
    "        if type(nch_cut_per_band) != str: # this means there may still be overlaps to be solved. Double-check:\n",
    "            dlambda = StitchedWLs[1:]-StitchedWLs[:-1]\n",
    "            if np.amin(dlambda) < 0:\n",
    "                raise ValueError(\"Set nch_cut_per_band to either one of the 'no overlap' options, or to a list of int/tuples such that there is no overlap between bands.\")\n",
    "        ## Do not use the interpolated version for your analysis; it is unclear what the underlying spectrum actually is here!\n",
    "        if MaskFeature:\n",
    "            FMask = (StitchedWav >= 12.118) & (StitchedWav <= 12.132)\n",
    "\n",
    "            Interpolation = I1D(x=StitchedWav[~FMask], y=StitchedFlux[~FMask], kind='cubic')\n",
    "            StitchedFlux  = Interpolation(StitchedWav)\n",
    "            pass\n",
    "\n",
    "        BL, Mask = Baseline(Wavelength = StitchedWav,\n",
    "                            Flux       = StitchedFlux,  ## The interpolated flux is being used for obtaining the continuum; if MaskFeature = True\n",
    "                            WL         = bl_params['WL'],\n",
    "                            NK         = int(np.ceil(len(StitchedWav)/bl_params['NKf'])),\n",
    "                            Quant      = bl_params['Quant'],\n",
    "                            SD         = bl_params['SD'],\n",
    "                            DO         = bl_params['DO'],\n",
    "                            MI         = bl_params['MI'],\n",
    "                            Lam        = bl_params['Lam'],\n",
    "                            Tol        = bl_params['Tol'],\n",
    "                            sigma_clip = bl_params['sigma_clip'],\n",
    "                            W_max_out  = bl_params['W_max_out'])\n",
    "\n",
    "        StitchedBL     = BL\n",
    "        StitchedMask   = Mask\n",
    "        StitchedFluxCS = StitchedFlux - StitchedBL\n",
    "        \n",
    "        if bl_params['NKf_ch4'] != bl_params['NKf']:\n",
    "            ## === Channel 4: smaller knot spacing\n",
    "            WavMask1 = (StitchedWav >= wl0_ch4a)\n",
    "            WavMask2 = (StitchedWav[~StitchedMask] >= wl0_ch4a)\n",
    "\n",
    "            BL4ABC   = PureBaseline(Wavelength = StitchedWav[WavMask1],\n",
    "                                    MaskedWav  = StitchedWav[~StitchedMask][WavMask2],\n",
    "                                    MaskedFlux = StitchedFlux[~StitchedMask][WavMask2],\n",
    "                                    NK         = int(np.ceil(len(StitchedWav[WavMask1])/bl_params['NKf_ch4'])),\n",
    "                                    Quant      = bl_params['Quant_ch4'],\n",
    "                                    SD         = bl_params['SD'],\n",
    "                                    DO         = bl_params['DO'],\n",
    "                                    MI         = bl_params['MI'],\n",
    "                                    Lam        = bl_params['Lam'],\n",
    "                                    Tol        = bl_params['Tol'])\n",
    "\n",
    "            BL[WavMask1]   = BL4ABC\n",
    "\n",
    "            if bl_params['IP_ch4']:\n",
    "                IPMask = (StitchedWav >= wl0_ch4a-0.05) & (StitchedWav <= wl0_ch4a+0.05)\n",
    "                Interpolation = I1D(x=StitchedWav[~IPMask], y=BL[~IPMask], kind='cubic')\n",
    "                BL            = Interpolation(StitchedWav)\n",
    "                pass\n",
    "\n",
    "            StitchedBL = BL\n",
    "            StitchedFluxCS = StitchedFlux - StitchedBL\n",
    "        \n",
    "        ## === Save the results:\n",
    "        ContSubData = {'Wavelength': StitchedWav,\n",
    "                       'Flux': OStitchedFlux,\n",
    "                       'CSFlux': StitchedFluxCS,\n",
    "                       'Mask': StitchedMask,\n",
    "                       'Baseline': StitchedBL}\n",
    "        pickle.dump(ContSubData, open(os.path.join(fin3_dir, f'FullSpectrum_CS.p'), 'wb'))        \n",
    "\n",
    "# Let's write again the final csv file including a column for the continuum-subtracted spectrum:        \n",
    "        # this saves the MINDS pipeline-extracted spectrum, including columns for continuum estimation and continuum-subtracted spectra\n",
    "        if psff:\n",
    "            sstring = os.path.join(spec3_dir, f'Level3_ifua_psf*'+'{}.fits'.format(suffix_ex1d))\n",
    "            x1d_files = sorted(glob.glob(sstring))\n",
    "            x1d_files = sort_files_wl(x1d_files)\n",
    "            spectrum_df = x1d_files_to_table(x1d_files, nch_bad_per_band=nch_bad_per_band, correct_jumps=correct_jumps, \n",
    "                                             final_scal_facs=final_scal_facs, do_bend=do_bend, contsub_spectrum=ContSubData)\n",
    "            spectrum_df.to_csv(os.path.join(fin3_dir, f\"spectrum_minds_ap{apsize}FWHM_psff.csv\"))\n",
    "            source_list.spectrum_df[isource] = spectrum_df\n",
    "        else:\n",
    "            if do_rfc1d:\n",
    "                spectrum_df = x1d_files_to_table(x1d_files, x1d_files_rfc1d, nch_bad_per_band, correct_jumps=correct_jumps, \n",
    "                                                 final_scal_facs=final_scal_facs, do_bend=do_bend, contsub_spectrum=ContSubData)\n",
    "            else:\n",
    "                spectrum_df = x1d_files_to_table(x1d_files, nch_bad_per_band=nch_bad_per_band, correct_jumps=correct_jumps, \n",
    "                                                 final_scal_facs=final_scal_facs, do_bend=do_bend, contsub_spectrum=ContSubData)\n",
    "            spectrum_df.to_csv(os.path.join(fin3_dir, f\"spectrum_minds.csv\"))        \n",
    "            source_list.spectrum_df[isource] = spectrum_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcf2455",
   "metadata": {},
   "source": [
    "## 6. Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ffc1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot options\n",
    "cols = ['b','g','orange','m','y','r','c']*2 # color list for: current reduction, Spitzer spectrum, and other reductions found in the same folder (if compare=True)\n",
    "ls = ['-', ':', 'dashdot']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c59fb8f",
   "metadata": {},
   "source": [
    "### 6.1. Plots with DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c123f2e6",
   "metadata": {},
   "source": [
    "#### Example for simple plot (Matthias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9706dc-01ea-4683-b006-b0c0789bb24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for isource in range(source_list.number_of_sources):\n",
    "    source = source_list.source_names[isource]\n",
    "    print(f'Source {isource+1}: {source}:')\n",
    "    input_dir = source_list.input_dirs[isource]\n",
    "    input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "    det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "    det1_dir = source_list.det1_dirs[isource]\n",
    "    det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "    spec2_dir = source_list.spec2_dirs[isource]\n",
    "    spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "    spec3_dir = source_list.spec3_dirs[isource]\n",
    "    fin3_dir = source_list.fin3_dirs[isource]\n",
    "    cxy_med = source_list.cxy_med[isource]\n",
    "    cxy_med_ifua = source_list.cxy_med_ifua[isource]\n",
    "    calfiles_x1d = source_list.calfiles_x1d[isource]\n",
    "    calfiles_x1d_rfc1d = source_list.calfiles_x1d_rfc1d[isource]\n",
    "    l3asn_dir = source_list.l3asn_dirs[isource]\n",
    "    spectrum_df = source_list.spectrum_df[isource]\n",
    "    spectrum_df_vanilla_pip = source_list.spectrum_df_vanilla_pip[isource]\n",
    "    figs_dir = source_list.figs_dirs[isource]\n",
    "    \n",
    "    try:\n",
    "        import seaborn as sns\n",
    "        sns.set_context(context='talk', font_scale=1, rc=None)\n",
    "        sns.set_style(\"ticks\")\n",
    "        plt.rcParams.update({\"xtick.bottom\" : True, \"ytick.left\" : True, \"xtick.minor.visible\" : True, \"ytick.minor.visible\": True})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if show_plots:\n",
    "        if interactive_plots:\n",
    "            %matplotlib widget\n",
    "        else:\n",
    "            %matplotlib inline\n",
    "\n",
    "    # vanilla\n",
    "    flux_col = 'flux'\n",
    "    plt.close('all')\n",
    "    band_ids = np.unique(spectrum_df_vanilla_pip['BAND_ID'])\n",
    "    for band in band_ids:\n",
    "        mask_band = spectrum_df_vanilla_pip['BAND_ID'] == band\n",
    "        spectrum_seg = spectrum_df_vanilla_pip[mask_band]\n",
    "        mask_wavelength = spectrum_seg['wavelength'] < wl_max\n",
    "        plt.plot(spectrum_seg['wavelength'][mask_wavelength].values,\n",
    "                 spectrum_seg[flux_col][mask_wavelength].values,\n",
    "                 label=band)\n",
    "    plt.xlabel('Wavelength (micron)')\n",
    "    plt.ylabel('Flux (Jy)')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(figs_dir, f\"spectrum_vanilla_pipeline.png\"))\n",
    "\n",
    "    # psff    \n",
    "    if psff:\n",
    "        flux_col = 'flux'\n",
    "        if 'flux_fcorr' in spectrum_df.columns:\n",
    "            flux_col = 'flux_fcorr'\n",
    "        plt.close('all')\n",
    "        band_ids = np.unique(spectrum_df['BAND_ID'])\n",
    "        for band in band_ids:\n",
    "            mask_band = spectrum_df['BAND_ID'] == band\n",
    "            spectrum_seg = spectrum_df[mask_band]\n",
    "            mask_wavelength = spectrum_seg['wavelength'] < wl_max\n",
    "            plt.plot(spectrum_seg['wavelength'][mask_wavelength].values,\n",
    "                     spectrum_seg[flux_col][mask_wavelength].values,\n",
    "                     label=band)\n",
    "        plt.xlabel('Wavelength (micron)')\n",
    "        plt.ylabel('Flux (Jy)')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(figs_dir, f\"spectrum_minds_psff.png\"))\n",
    "    else:       \n",
    "        # minds + jwst\n",
    "        flux_col = 'flux'\n",
    "        if 'flux_fcorr' in spectrum_df.columns:\n",
    "            flux_col = 'flux_fcorr'\n",
    "        plt.close('all')\n",
    "        band_ids = np.unique(spectrum_df['BAND_ID'])\n",
    "        for band in band_ids:\n",
    "            mask_band = spectrum_df['BAND_ID'] == band\n",
    "            spectrum_seg = spectrum_df[mask_band]\n",
    "            mask_wavelength = spectrum_seg['wavelength'] < wl_max\n",
    "            plt.plot(spectrum_seg['wavelength'][mask_wavelength].values,\n",
    "                     spectrum_seg[flux_col][mask_wavelength].values,\n",
    "                     label=band)\n",
    "        plt.xlabel('Wavelength (micron)')\n",
    "        plt.ylabel('Flux (Jy)')\n",
    "        plt.legend()\n",
    "        plt.savefig(os.path.join(figs_dir, f\"spectrum_minds.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af81b925",
   "metadata": {},
   "source": [
    "**Seaborn package works great for quickly plotting data frames, if you have seaborn installed try:**\n",
    "\n",
    "**Does not work with newest version of Pandas and Seaborn. Waiting for fix....**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c541d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import seaborn as sns\n",
    "    plot_fringes = True\n",
    "    plot_legend = True\n",
    "    if psff:\n",
    "        g = sns.FacetGrid(spectrum_df, row=\"BAND_ID\", sharex=False, sharey=False, aspect=5)\n",
    "        g.map_dataframe(\n",
    "            sns.lineplot, x=\"wavelength\", y=\"flux_fcorr\",\n",
    "                        color='black', drawstyle='steps-mid',\n",
    "                        label='Fringe correction')\n",
    "        if plot_fringes:\n",
    "            g.map_dataframe(\n",
    "                sns.lineplot, x=\"wavelength\", y=\"flux\",\n",
    "                alpha=0.5, color='orange', drawstyle='steps-mid',\n",
    "                label='No fringe correction')\n",
    "        if plot_legend:\n",
    "            g.add_legend()\n",
    "    else:\n",
    "        g = sns.FacetGrid(spectrum_df_std_phot, row=\"BAND_ID\", sharex=False, sharey=False, aspect=5)\n",
    "        if rfc1d:\n",
    "            label='Fringe correction'\n",
    "        else:\n",
    "            label='No fringe correction'\n",
    "        label += \"(std pipeline)\"\n",
    "        g.map_dataframe(\n",
    "            sns.lineplot, x=\"wavelength\", y=\"flux_fcorr\",\n",
    "                        color='black', drawstyle='steps-mid',\n",
    "                        label=label)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3501903e",
   "metadata": {},
   "source": [
    "### 6.2. Comparison plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3728d268",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for isource in range(source_list.number_of_sources):\n",
    "    source = source_list.source_names[isource]\n",
    "    print(f'Source {isource+1}: {source}:')\n",
    "    input_dir = source_list.input_dirs[isource]\n",
    "    input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "    det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "    det1_dir = source_list.det1_dirs[isource]\n",
    "    det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "    spec2_dir = source_list.spec2_dirs[isource]\n",
    "    spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "    spec3_dir = source_list.spec3_dirs[isource]\n",
    "    fin3_dir = source_list.fin3_dirs[isource]\n",
    "    cxy_med = source_list.cxy_med[isource]\n",
    "    cxy_med_ifua = source_list.cxy_med_ifua[isource]\n",
    "    calfiles_x1d = source_list.calfiles_x1d[isource]\n",
    "    calfiles_x1d_rfc1d = source_list.calfiles_x1d_rfc1d[isource]\n",
    "    l3asn_dir = source_list.l3asn_dirs[isource]\n",
    "    spectrum_df = source_list.spectrum_df[isource]\n",
    "    spectrum_df_vanilla_pip = source_list.spectrum_df_vanilla_pip[isource]\n",
    "    figs_dir = source_list.figs_dirs[isource]\n",
    "    all_wavelength = source_list.all_wavelength[isource]\n",
    "    all_corrected_flux = source_list.all_corrected_flux[isource]\n",
    "        \n",
    "    # gather the spectra\n",
    "    files = glob.glob(join(spec3_dir, 'Level3_ifua{}*{}.fits'.format(suffix[0], suffix_ex1d)))\n",
    "    # sort them by WL\n",
    "    files = sort_files_wl(files)\n",
    "    \n",
    "# plot the spectrum (last panel shows the full spectrum)\n",
    "    if spec_mode == 'per_band':\n",
    "        npanels = 13\n",
    "        fig, axs = plt.subplots(npanels, 1, figsize=(10, 40))#, sharex=True)\n",
    "    else: # per channel\n",
    "        npanels = 5 \n",
    "        fig, axs = plt.subplots(npanels, 1, figsize=(10, 20))#, sharex=True)\n",
    "    axs[0].set_title(\"{} spectrum\".format(source), fontsize=14)\n",
    "\n",
    "    # PLOT SPITZER SPECTRUM AS 'SPACE TRUTH'\n",
    "    if spitzer_file is not None:\n",
    "        #spitzer_spec = pd.read_csv(join(input_dir,\"PDS70_Spitzer-CASSIS-LR.1.csv\"), header=1, skiprows=0) ## old spectrum\n",
    "        spitzer_spec = pd.read_csv(spitzer_file, header=0, skiprows=0, sep='       ', engine='python') # new Spitzer data reduction\n",
    "        #swav = spitzer_spec['#lam:mic']\n",
    "        swav = np.array(spitzer_spec['lambda'])\n",
    "        sf = np.array(spitzer_spec['Flux'])\n",
    "        sf_er = np.array(spitzer_spec['F_Er'])\n",
    "\n",
    "        idx0 = find_nearest(swav, wl_min)\n",
    "        idx1 = find_nearest(swav, wl_max)\n",
    "        if npanels>4:\n",
    "            axs[npanels-1].errorbar(swav[idx0:idx1], sf[idx0:idx1], sf_er[idx0:idx1], c=cols[1], markersize=0, \\\n",
    "                                    linestyle='--', linewidth=1, \n",
    "                                    label='Spitzer IRS LRS', alpha=0.8)\n",
    "    # plot photometry\n",
    "    if photom_file is not None:\n",
    "        photom_data = pd.read_csv(photom_file, header=0, skiprows=0, sep='\\t')\n",
    "        wl_p = photom_data['wavelength']\n",
    "        f_p = photom_data['flux']\n",
    "        ferr_p = photom_data['err_flux']\n",
    "        ins_p = photom_data['instrument']\n",
    "\n",
    "        if npanels>4:\n",
    "            for pp in range(len(wl_p)):\n",
    "                if pp == 0:\n",
    "                    lab_tmp = 'Photometry'\n",
    "                else:\n",
    "                    lab_tmp = None\n",
    "                axs[npanels-1].errorbar(wl_p[pp], f_p[pp], ferr_p[pp], c='k', marker='s', markersize=3,\n",
    "                                        label=lab_tmp, alpha=0.8)\n",
    "\n",
    "    # PLOT MRS spectra\n",
    "    for ff, file in enumerate(files):\n",
    "\n",
    "        # Load the spectrum\n",
    "        out_name = splitext(file)[0] + '{}.txt'.format(lab)\n",
    "        wavelength, corrected_flux = np.loadtxt(out_name).T\n",
    "\n",
    "        # plot MRS spectrum\n",
    "        good_idx = np.where(wavelength<wl_max)\n",
    "        axs[ff].plot(wavelength[good_idx], corrected_flux[good_idx], c=cols[0], markersize=0, linestyle='-', linewidth=1, \n",
    "                    label='MIRI/MRS - {} ({})'.format(suffix_ex1d[nchar:], lab[1:]), alpha=0.8)\n",
    "        if npanels>4:\n",
    "            if ff == 0:\n",
    "                label = 'MIRI/MRS - {}{}'.format(suffix_ex1d[nchar:], lab)\n",
    "            else:\n",
    "                label = None\n",
    "            axs[npanels-1].plot(wavelength[good_idx], corrected_flux[good_idx], c=cols[0], markersize=0, linestyle='-', linewidth=1, \n",
    "                                label=label, alpha=0.8)\n",
    "\n",
    "        # plot Spitzer spectrum\n",
    "        if spitzer_file is not None:\n",
    "            idx0s = find_nearest(swav, max(np.amin(wavelength), wl_min))\n",
    "            idx1s = find_nearest(swav, min(np.amax(wavelength), wl_max))\n",
    "            axs[ff].errorbar(swav[idx0s:idx1s], sf[idx0s:idx1s], sf_er[idx0s:idx1s], c=cols[1], markersize=0, linestyle='--', linewidth=1, \n",
    "                        label='Spitzer IRS LRS', alpha=0.8)\n",
    "\n",
    "        # also plot other spectra if requested and if they exist\n",
    "        if compare_spec and nsuff>0:\n",
    "            other_plot_files = [other_files[i] for i in range(min(5,nsuff))]\n",
    "            suff_tmp_plot = [suff_tmp[i] for i in range(min(5,nsuff))]\n",
    "            lab_c = '_comparison'\n",
    "            f_i = 2 # set to 0 if you wish reductions starting from the first one to be plotted\n",
    "            f_n = 3 # set to -1 if you wish reductions up to the last one to be plotted\n",
    "            for f, flist in enumerate(other_plot_files[f_i:f_n]): #adapt \"5\" to change max number of comparisons\n",
    "                #if f!=1 and f!=2:\n",
    "                arr = np.loadtxt(flist[ff])\n",
    "                wavelength = arr[:,0]\n",
    "                corrected_flux_autobg = arr[:,1]\n",
    "                good_idx = np.where(wavelength<wl_max)\n",
    "                axs[ff].plot(wavelength[good_idx], corrected_flux_autobg[good_idx], c=cols[2+f], markersize=0, linestyle='-', linewidth=1, \n",
    "                             label='MIRI/MRS - {}'.format(suff_tmp_plot[f_i+f]), alpha=0.8)\n",
    "                if npanels>4:\n",
    "                    if ff == 0:\n",
    "                        label = 'MIRI/MRS - {}'.format(suff_tmp_plot[f_i+f])\n",
    "                    else:\n",
    "                        label = None\n",
    "                    axs[npanels-1].plot(wavelength[good_idx], corrected_flux_autobg[good_idx], c=cols[2+f], markersize=0, linestyle='-', linewidth=1, \n",
    "                                        label=label, alpha=0.8)\n",
    "        else:\n",
    "            lab_c = ''\n",
    "\n",
    "        axs[ff].set_ylabel('Flux (Jy)', fontsize=12)\n",
    "        axs[ff].legend(prop={'size': 10}, loc=0)\n",
    "        axs[npanels-1].set_xlabel(r'Wavelength ($\\mu$m)', fontsize=12)\n",
    "        axs[npanels-1].set_ylabel('Flux (Jy)', fontsize=12)\n",
    "        axs[npanels-1].legend(prop={'size': 10}, loc=0)\n",
    "\n",
    "        plt.tight_layout(h_pad=0)\n",
    "        if spec_mode == 'per_band' and npanels == 13:\n",
    "            plt.savefig(join(figs_dir,\"{}_spectra_band{}.pdf\".format(source, lab_c)), bbox_inches='tight')\n",
    "        elif spec_mode == 'per_channel' and npanels == 5:\n",
    "            plt.savefig(join(figs_dir,\"{}_spectra_channel{}.pdf\".format(source, lab_c)), bbox_inches='tight')\n",
    "        else:\n",
    "            plt.savefig(join(figs_dir,\"{}_spectra{}.pdf\".format(source, lab_c)), bbox_inches='tight')\n",
    "        if show_plots:\n",
    "            plt.show()\n",
    "\n",
    "    final_out_name = 'spectrum_minds.txt'\n",
    "    final_out_array = np.savetxt(join(fin3_dir,final_out_name), np.array([all_wavelength, all_corrected_flux]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61982f12",
   "metadata": {},
   "source": [
    "Make a single plot of the spectrum at all wavelengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62f8c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting options\n",
    "cols_b = ['b','y','r','k']\n",
    "ls = ['-', '--',':', 'dashdot']\n",
    "band_labs = ['SHORT', 'MEDIUM', 'LONG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7154119b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for isource in range(source_list.number_of_sources):\n",
    "    source = source_list.source_names[isource]\n",
    "    print(f'Source {isource+1}: {source}:')\n",
    "    input_dir = source_list.input_dirs[isource]\n",
    "    input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "    det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "    det1_dir = source_list.det1_dirs[isource]\n",
    "    det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "    spec2_dir = source_list.spec2_dirs[isource]\n",
    "    spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "    spec3_dir = source_list.spec3_dirs[isource]\n",
    "    fin3_dir = source_list.fin3_dirs[isource]\n",
    "    cxy_med = source_list.cxy_med[isource]\n",
    "    cxy_med_ifua = source_list.cxy_med_ifua[isource]\n",
    "    calfiles_x1d = source_list.calfiles_x1d[isource]\n",
    "    calfiles_x1d_rfc1d = source_list.calfiles_x1d_rfc1d[isource]\n",
    "    l3asn_dir = source_list.l3asn_dirs[isource]\n",
    "    spectrum_df = source_list.spectrum_df[isource]\n",
    "    spectrum_df_vanilla_pip = source_list.spectrum_df_vanilla_pip[isource]\n",
    "    figs_dir = source_list.figs_dirs[isource]\n",
    "\n",
    "    # gather the spectra\n",
    "    files = glob.glob(join(spec3_dir, 'Level3_ifua{}*{}.fits'.format(suffix[0], suffix_ex1d)))\n",
    "    # sort them by WL\n",
    "    files = sort_files_wl(files)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(10, 5))#, sharex=True)\n",
    "    axs.set_title(\"{} - Full {:.1f}-{:.1f}$\\mu$m spectrum\".format(source,wl_min,wl_max), fontsize=14)\n",
    "\n",
    "    # PLOT SPITZER SPECTRUM AS 'SPACE TRUTH'\n",
    "    if spitzer_file is not None:\n",
    "        #spitzer_spec = pd.read_csv(join(input_dir,\"PDS70_Spitzer-CASSIS-LR.1.csv\"), header=1, skiprows=0) ## old spectrum\n",
    "        spitzer_spec = pd.read_csv(spitzer_file, header=0, skiprows=0, sep='       ', engine='python') # new Spitzer data reduction\n",
    "        #swav = spitzer_spec['#lam:mic']\n",
    "        swav = np.array(spitzer_spec['lambda'])\n",
    "        sf = np.array(spitzer_spec['Flux'])\n",
    "        sf_er = np.array(spitzer_spec['F_Er'])\n",
    "\n",
    "    all_wavelengths = []\n",
    "    all_fluxes = []\n",
    "    for ff, file in enumerate(files):\n",
    "        if ff>0:\n",
    "            label=None\n",
    "        print('processing {}'.format(file))\n",
    "\n",
    "        # open file as datamodel\n",
    "        dm = datamodels.open(file)\n",
    "        ## get columns\n",
    "        ori_wavelength = dm.spec[0].spec_table['WAVELENGTH']\n",
    "        flux = dm.spec[0].spec_table['FLUX']\n",
    "        if type(nch_bad_per_band) == str:\n",
    "            if \"red\" in nch_bad_per_band:\n",
    "                if ff < len(files)-1:\n",
    "                    dm_next = datamodels.open(files[ff+1])\n",
    "                    next_wavelengths = dm_next.spec[0].spec_table['WAVELENGTH']\n",
    "                    idx_overlap = find_nearest(next_wavelengths[0], ori_wavelength, constraint='floor')\n",
    "                    if ff == 0:\n",
    "                        nch_bad_per_band = [(0,len(ori_wavelength)-idx_overlap-1)]\n",
    "                    else: \n",
    "                        nch_bad_per_band.append((0,len(ori_wavelength)-idx_overlap-1))\n",
    "                else:\n",
    "                    idx_overlap = len(ori_wavelength)-1\n",
    "                    nch_bad_per_band.append((0,0))\n",
    "                wavelength = ori_wavelength[:idx_overlap+1]\n",
    "                flux = flux[:idx_overlap+1]\n",
    "            else:\n",
    "                if ff == 0:\n",
    "                    idx_overlap = 0\n",
    "                    nch_bad_per_band = [(0,0)]\n",
    "                else:\n",
    "                    dm_prev = datamodels.open(files[ff-1])\n",
    "                    prev_wavelengths = dm_prev.spec[0].spec_table['WAVELENGTH']\n",
    "                    idx_overlap = find_nearest(prev_wavelengths[-1], ori_wavelength, constraint='ceil')\n",
    "                    nch_bad_per_band.append((idx_overlap,0))\n",
    "                wavelength = ori_wavelength[idx_overlap:]\n",
    "                flux = flux[idx_overlap:]\n",
    "        elif nch_bad_per_band[ff][0]>0 or nch_bad_per_band[ff][1]>0:\n",
    "            wavelength = ori_wavelength[nch_bad_per_band[ff][0]:len(ori_wavelength)-nch_bad_per_band[ff][1]]\n",
    "            flux = flux[nch_bad_per_band[ff][0]:len(flux)-nch_bad_per_band[ff][1]]\n",
    "        else:\n",
    "            wavelength = ori_wavelength\n",
    "\n",
    "        # load the spectrum from a text file\n",
    "        out_name = splitext(file)[0] + '{}.txt'.format(lab)\n",
    "        out_array = np.loadtxt(out_name)\n",
    "        wavelength = out_array[:,0]\n",
    "        corrected_flux = out_array[:,1]\n",
    "\n",
    "        # plot MRS spectrum\n",
    "        good_idx = np.where(wavelength<wl_max)\n",
    "        if ff < 3:\n",
    "            label_tmp = 'MIRI/MRS ({} bands)'.format(band_labs[ff%3])\n",
    "        else:\n",
    "            label_tmp = None\n",
    "        axs.plot(wavelength[good_idx], corrected_flux[good_idx], c=cols_b[ff%3], markersize=0, linestyle=ls[0], linewidth=1, \n",
    "                 label=label_tmp, alpha=0.8)\n",
    "\n",
    "        # plot Spitzer spectrum\n",
    "        if spitzer_file is not None:\n",
    "            if ff == 0:\n",
    "                label_tmp = 'Spitzer IRS LRS'\n",
    "            else:\n",
    "                label_tmp = None\n",
    "            idx0s = find_nearest(swav, max(np.amin(ori_wavelength), wl_min))\n",
    "            idx1s = find_nearest(swav, min(np.amax(ori_wavelength), wl_max))\n",
    "            axs.errorbar(swav[idx0s:idx1s], sf[idx0s:idx1s], sf_er[idx0s:idx1s], c=cols[1], markersize=0, linestyle=ls[1], linewidth=1, \n",
    "                         label=label_tmp, alpha=0.8)\n",
    "\n",
    "        # also plot other spectra if requested and if they exist\n",
    "        if compare_spec:\n",
    "            for f, flist in enumerate(other_files[2:3]):\n",
    "                if f == 0 and ff==0:\n",
    "                    lab_tmp = 'MIRI/MRS - {}'.format(suff_tmp[2+f])\n",
    "                else:\n",
    "                    lab_tmp = None\n",
    "                arr = np.loadtxt(flist[ff])\n",
    "                wavelength = arr[:,0]\n",
    "                corrected_flux_autobg = arr[:,1]\n",
    "                good_idx = np.where(wavelength<wl_max)\n",
    "                axs.plot(wavelength[good_idx], corrected_flux_autobg[good_idx], c=cols_b[ff%3], markersize=0, linestyle=ls[2+f], linewidth=1, \n",
    "                        label=lab_tmp, alpha=0.8)\n",
    "\n",
    "    # plot photometry\n",
    "    if photom_file is not None:\n",
    "        photom_data = pd.read_csv(photom_file, header=0, skiprows=0, sep='\\t')\n",
    "        wl_p = photom_data['wavelength']\n",
    "        f_p = photom_data['flux']\n",
    "        ferr_p = photom_data['err_flux']\n",
    "        ins_p = photom_data['instrument']\n",
    "        for pp in range(len(wl_p)):\n",
    "            if pp == 0:\n",
    "                lab_tmp = 'Photometry'\n",
    "            else:\n",
    "                lab_tmp = None\n",
    "            axs.errorbar(wl_p[pp], f_p[pp], ferr_p[pp], c='k', marker='s',markersize=3,\n",
    "                         label=lab_tmp, alpha=0.8)\n",
    "\n",
    "    axs.set_ylabel('Flux (Jy)', fontsize=12)\n",
    "    axs.set_xlabel(r'Wavelength ($\\mu$m)', fontsize=12)\n",
    "    axs.legend(prop={'size': 10}, loc=0)\n",
    "\n",
    "    plt.tight_layout(h_pad=0)\n",
    "    plt.savefig(join(figs_dir,\"{}_spectra_full{}.pdf\".format(source,ver)), bbox_inches='tight')\n",
    "    if show_plots:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fba35e4",
   "metadata": {},
   "source": [
    "### 6.3. Final plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5543a42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fwl_min=4.5      # min wavelength for the plot\n",
    "fwl_max=26.8  # max wavelength for the plot\n",
    "cols_f = ['k','c','b','g','orange','r','m','brown']\n",
    "#compare_spec = True # whether to compare with other spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8bec22",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for isource in range(source_list.number_of_sources):\n",
    "    source = source_list.source_names[isource]\n",
    "    print(f'Source {isource+1}: {source}:')\n",
    "    input_dir = source_list.input_dirs[isource]\n",
    "    input_bgdir = source_list.input_bkg_dirs[isource]\n",
    "    det1_dir_ori = source_list.det1_dirs_ori[isource]\n",
    "    det1_dir = source_list.det1_dirs[isource]\n",
    "    det1_bgdir = source_list.det1_bkg_dirs[isource]\n",
    "    spec2_dir = source_list.spec2_dirs[isource]\n",
    "    spec2_bgdir = source_list.spec2_bkg_dirs[isource]\n",
    "    spec3_dir = source_list.spec3_dirs[isource]\n",
    "    fin3_dir = source_list.fin3_dirs[isource]\n",
    "    cxy_med = source_list.cxy_med[isource]\n",
    "    cxy_med_ifua = source_list.cxy_med_ifua[isource]\n",
    "    calfiles_x1d = source_list.calfiles_x1d[isource]\n",
    "    calfiles_x1d_rfc1d = source_list.calfiles_x1d_rfc1d[isource]\n",
    "    l3asn_dir = source_list.l3asn_dirs[isource]\n",
    "    spectrum_df = source_list.spectrum_df[isource]\n",
    "    spectrum_df_vanilla_pip = source_list.spectrum_df_vanilla_pip[isource]\n",
    "    figs_dir = source_list.figs_dirs[isource]\n",
    "\n",
    "    # gather the spectra\n",
    "    files = glob.glob(join(spec3_dir, 'Level3_ifua{}*{}.fits'.format(suffix[0], suffix_ex1d)))\n",
    "    # sort them by WL\n",
    "    files = sort_files_wl(files)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 1, figsize=(10, 5))#, sharex=True)\n",
    "    axs.set_title(\"{}\".format(source), #\" - Full {:.1f}-{:.1f}$\\mu$m spectrum\".format(source,wl_min,wl_max), \n",
    "                  fontsize=14)\n",
    "\n",
    "    # PLOT SPITZER SPECTRUM AS 'SPACE TRUTH'\n",
    "    if spitzer_file is not None and compare_spec:\n",
    "        #spitzer_spec = pd.read_csv(join(input_dir,\"PDS70_Spitzer-CASSIS-LR.1.csv\"), header=1, skiprows=0) ## old spectrum\n",
    "        spitzer_spec = pd.read_csv(spitzer_file, header=0, skiprows=0, sep='       ', engine='python') # new Spitzer data reduction\n",
    "        #swav = spitzer_spec['#lam:mic']\n",
    "        swav = np.array(spitzer_spec['lambda'])\n",
    "        sf = np.array(spitzer_spec['Flux'])\n",
    "        sf_er = np.array(spitzer_spec['F_Er'])\n",
    "\n",
    "    all_wavelengths = []\n",
    "    all_fluxes = []\n",
    "    for ff, file in enumerate(files):\n",
    "        if ff>0:\n",
    "            label=None\n",
    "        print('processing {}'.format(file))\n",
    "\n",
    "        # open file as datamodel\n",
    "        dm = datamodels.open(file)\n",
    "        ## get columns\n",
    "        ori_wavelength = dm.spec[0].spec_table['WAVELENGTH']\n",
    "        flux = dm.spec[0].spec_table['FLUX']\n",
    "        if type(nch_bad_per_band) == str:\n",
    "            if \"red\" in nch_bad_per_band:\n",
    "                if ff < len(files)-1:\n",
    "                    dm_next = datamodels.open(files[ff+1])\n",
    "                    next_wavelengths = dm_next.spec[0].spec_table['WAVELENGTH']\n",
    "                    idx_overlap = find_nearest(next_wavelengths[0], ori_wavelength, constraint='floor')\n",
    "                    if ff == 0:\n",
    "                        nch_bad_per_band = [(0,len(ori_wavelength)-idx_overlap-1)]\n",
    "                    else: \n",
    "                        nch_bad_per_band.append((0,len(ori_wavelength)-idx_overlap-1))\n",
    "                else:\n",
    "                    idx_overlap = len(ori_wavelength)-1\n",
    "                    nch_bad_per_band.append((0,0))\n",
    "                wavelength = ori_wavelength[:idx_overlap+1]\n",
    "                flux = flux[:idx_overlap+1]\n",
    "            else:\n",
    "                if ff == 0:\n",
    "                    idx_overlap = 0\n",
    "                    nch_bad_per_band = [(0,0)]\n",
    "                else:\n",
    "                    dm_prev = datamodels.open(files[ff-1])\n",
    "                    prev_wavelengths = dm_prev.spec[0].spec_table['WAVELENGTH']\n",
    "                    idx_overlap = find_nearest(prev_wavelengths[-1], ori_wavelength, constraint='ceil')\n",
    "                    nch_bad_per_band.append((idx_overlap,0))\n",
    "                wavelength = ori_wavelength[idx_overlap:]\n",
    "                flux = flux[idx_overlap:]\n",
    "        elif nch_bad_per_band[ff][0]>0 or nch_bad_per_band[ff][1]>0:\n",
    "            wavelength = ori_wavelength[nch_bad_per_band[ff][0]:-nch_bad_per_band[ff][1]]\n",
    "            flux = flux[nch_bad_per_band[ff][0]:-nch_bad_per_band[ff][1]]\n",
    "        else:\n",
    "            wavelength = ori_wavelength\n",
    "\n",
    "        # load the spectrum from a text file\n",
    "        out_name = splitext(file)[0] + '{}.txt'.format(lab)\n",
    "        out_array = np.loadtxt(out_name)\n",
    "        wavelength = out_array[:,0]\n",
    "        corrected_flux = out_array[:,1]\n",
    "\n",
    "        # plot MRS spectrum\n",
    "        good_idx = np.where(wavelength<fwl_max)\n",
    "        if ff == 0:\n",
    "            label_tmp = 'MIRI/MRS'\n",
    "        else:\n",
    "            label_tmp = None\n",
    "        axs.plot(wavelength[good_idx], corrected_flux[good_idx], c=cols_f[0], markersize=0, linestyle=ls[0], linewidth=1, \n",
    "                 label=label_tmp, alpha=0.8)\n",
    "\n",
    "    # plot photometry\n",
    "    if photom_file is not None:\n",
    "        photom_data = pd.read_csv(photom_file, header=0, skiprows=0, sep='\\t')\n",
    "        wl_p = photom_data['wavelength']\n",
    "        f_p = photom_data['flux']\n",
    "        ferr_p = photom_data['err_flux']\n",
    "        ins_p = photom_data['instrument']\n",
    "        for pp in range(len(wl_p)):\n",
    "            axs.errorbar(wl_p[pp], f_p[pp], ferr_p[pp], c=cols_f[1+pp], marker='s', markersize=3,\n",
    "                         label=ins_p[pp], alpha=0.8)\n",
    "\n",
    "    # plot Spitzer spectrum\n",
    "    if spitzer_file is not None:# and compare_spec:\n",
    "        label_tmp = 'Spitzer IRS LRS'\n",
    "        idx0s = find_nearest(swav, fwl_min)\n",
    "        idx1s = find_nearest(swav, fwl_max)\n",
    "        axs.errorbar(swav[idx0s:idx1s], sf[idx0s:idx1s], sf_er[idx0s:idx1s], c=cols[1], markersize=0, linestyle=ls[1], \n",
    "                     linewidth=1, label=label_tmp, alpha=0.8)\n",
    "\n",
    "    axs.set_ylabel('Flux (Jy)', fontsize=12)\n",
    "    axs.set_xlabel(r'Wavelength ($\\mu$m)', fontsize=12)\n",
    "    axs.legend(prop={'size': 10}, loc=0)\n",
    "\n",
    "    plt.tight_layout(h_pad=0)\n",
    "    plt.savefig(join(figs_dir,\"{}_spectra_final{}.pdf\".format(source,ver)), bbox_inches='tight')\n",
    "    if show_plots:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26e4fd2",
   "metadata": {},
   "source": [
    "**Plot the continuum-subtracted spectrum** (if requested)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f7b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if contsub:\n",
    "#     ## === Plot results\n",
    "#     PlotSpectrum(Wav      = StitchedWav,\n",
    "#                  Flux     = OStitchedFlux,  ## Using the original stitched spectrum in the plotting.\n",
    "#                  Baseline = StitchedBL,\n",
    "#                  Mask     = StitchedMask,\n",
    "#                  dY       = 0.025,\n",
    "#                  SavePlot = os.path.join(figs_dir, f'FullSpectrum_CS'),\n",
    "#                  MaskFeature=MaskFeature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab8174e",
   "metadata": {},
   "source": [
    "### 6.4. Aperture image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2f2a40",
   "metadata": {},
   "source": [
    "Adapt cell below to include the wavelengths for which you would like to show a spectral image, along with the corresponding aperture size used for extracting the spectrum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1558503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_wls = [7.0, 14.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a6997",
   "metadata": {},
   "source": [
    "Let's define the number of pixels to crop from each side of the frames for the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb11d19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop_px = [19, 12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c75e75",
   "metadata": {},
   "source": [
    "Most of the other cells should not be modified (only the cell with ellipse parameters if you wish to include a sketch of the disk):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f3920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if make_aperture_image:\n",
    "#     from hciplot import plot_frames\n",
    "#     cxy_med = open_fits(join(fin3_dir,\"cxy_med.fits\")) # Note: order is CH1_LONG, CH1_MED, CH1_SHORT, CH2_LONG, etc.\n",
    "#     cxy_med"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cad0ef",
   "metadata": {},
   "source": [
    "Find the indices of the bands in which requested wavelengths are located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2423bdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if make_aperture_image:\n",
    "#     # gather the spectra\n",
    "#     afiles = glob.glob(join(spec3_dir, '*{}*{}*.fits'.format(suffix[0],suffix_ex1d)))\n",
    "#     afiles.sort()\n",
    "#     afiles = sort_files_wl(afiles)\n",
    "#     afiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dae67ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if make_aperture_image:\n",
    "#     calfiles=sorted(glob.glob(join(spec3_dir,'Level3{}*{}_cen{}.fits'.format(suffix[0],suf, suf_bkg))))\n",
    "#     calfiles = sort_files_wl(calfiles)\n",
    "#     print(calfiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d56edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if make_aperture_image:\n",
    "#     good_ffs = []\n",
    "#     good_wavelengths = []\n",
    "#     for ff, file in enumerate(afiles):\n",
    "#         # open file as datamodel\n",
    "#         dm = datamodels.open(file)\n",
    "#         # get wavelengths\n",
    "#         wavelength = dm.spec[0].spec_table['WAVELENGTH']\n",
    "#         for wl in img_wls:\n",
    "#             if wl > wavelength[0] and wl < wavelength[-1]:\n",
    "#                 good_ffs.append(ff)\n",
    "#                 good_wavelengths.append(wavelength)\n",
    "#     good_ffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75644e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# if make_aperture_image:\n",
    "#     good_frames = []\n",
    "#     good_plscs = []\n",
    "#     c = 0\n",
    "#     for ff, file in enumerate(calfiles):\n",
    "#         if ff in good_ffs:\n",
    "#             bname = basename(file)\n",
    "#             foutname = join(fin3_dir, bname)\n",
    "#             cube_cen, head = open_fits(foutname, header=True, n=1)\n",
    "#             dq_cube = fits.open(foutname, memmap=False)[3].data\n",
    "#             gp_cube = np.ones(dq_cube.shape)\n",
    "#             gp_cube[:] = np.nan\n",
    "#             gp_cube[np.where(dq_cube==0)]=1\n",
    "#             pxscale1 = float(head['CDELT1'])\n",
    "#             pxscale2 = float(head['CDELT2'])\n",
    "#             good_plscs.append(float(np.mean([pxscale1, pxscale2])*3600))\n",
    "#             # now find closest channel to requested wavelength\n",
    "#             close_idx = find_nearest(good_wavelengths[c], img_wls[c])\n",
    "#             print(len(good_wavelengths[c]))\n",
    "#             good_frames.append(cube_cen[close_idx, crop_px[c]:-crop_px[c], crop_px[c]:-crop_px[c]]*gp_cube[close_idx,crop_px[c]:-crop_px[c],crop_px[c]:-crop_px[c]])\n",
    "#             # previously: using band-edge cropped spectra\n",
    "#             #good_frames.append(cube_cen[nch_bad_per_band[ff][0]+close_idx,crop_px[c]:-crop_px[c], crop_px[c]:-crop_px[c]]*gp_cube[nch_bad_per_band[ff][0]+close_idx,crop_px[c]:-crop_px[c],crop_px[c]:-crop_px[c]])\n",
    "#             c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a41bd3",
   "metadata": {},
   "source": [
    "Show image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02604503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if make_aperture_image and show_plots:\n",
    "#     circles = [(float(cxy_med[ff, 0])-crop_px[fc], float(cxy_med[ff, 1])-crop_px[fc]) for fc, ff in enumerate(good_ffs)]\n",
    "#     labels = [r\"{:.1f} $\\mu$m\".format(img_wls[i]) for i in range(len(img_wls))]\n",
    "#     for ll, wl in enumerate(img_wls):\n",
    "#         vmin = abs(float(np.nanpercentile(good_frames[ll],0.01)))\n",
    "#         ticksep = 1./good_plscs[ll]\n",
    "#         fwhm = wl/good_plscs[ll]*0.31/8\n",
    "#         circ_rad = float(apsize*fwhm)\n",
    "#         plot_frames(good_frames[ll], label=labels[ll], dpi=300, colorbar=True, circle=circles[ll], cmap='inferno',\n",
    "#                     circle_radius=circ_rad, vmin=vmin, log=True, ang_scale=True, label_color='w',\n",
    "#                     ang_ticksep=ticksep, pxscale=good_plscs[ll], colorbar_label='MJy/sr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479a9d6e",
   "metadata": {},
   "source": [
    "Let's now save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24600e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if make_aperture_image:\n",
    "#     for ll, wl in enumerate(img_wls):\n",
    "#         vmin = abs(float(np.nanpercentile(good_frames[ll],0.01)))\n",
    "#         ticksep = 1./good_plscs[ll]\n",
    "#         fwhm = wl/good_plscs[ll]*0.31/8\n",
    "#         circ_rad = float(apsize*fwhm)\n",
    "#         plot_frames(good_frames[ll], label=labels[ll], dpi=300, colorbar=True, circle=circles[ll], cmap='inferno',\n",
    "#                     circle_radius=circ_rad, vmin=vmin, log=True, ang_scale=True, label_color='w',\n",
    "#                     save=join(figs_dir,'Aperture_{:.1f}mu.pdf'.format(wl)),\n",
    "#                     ang_ticksep=ticksep, pxscale=good_plscs[ll], colorbar_label='MJy/sr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524deac",
   "metadata": {},
   "source": [
    "Let's now add a sketch of the system. Here for PDS 70, we add 2 planets, and ellipses corresponding to inner and outer disks - feel free to adapt accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1cae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if make_aperture_image:\n",
    "#     # planets\n",
    "#     b_dxy_as = (-0.112781, -0.111194) # expected position on 01/08/2022\n",
    "#     c_dxy_as = (0.210882, 0.005433)  # expected position on 01/08/2022\n",
    "#     # disk\n",
    "#     din_au = 18.*2 #diameter\n",
    "#     dout_au = 54.*2 #diameter\n",
    "#     dist = 113.43 #pc\n",
    "#     incl = 49.7 #deg\n",
    "#     PA_a = 158.6-90 # trigonometric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f48c8a",
   "metadata": {},
   "source": [
    "Final plot + save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd25a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if make_aperture_image:\n",
    "#     for ll, wl in enumerate(img_wls):\n",
    "#         vmin = abs(float(np.nanpercentile(good_frames[ll],0.01)))\n",
    "#         ticksep = 1./good_plscs[ll]\n",
    "#         fwhm = wl/good_plscs[ll]*0.31/8\n",
    "#         ap_rad = float(apsize*fwhm)\n",
    "#         # planet positions in frame\n",
    "#         b_xy = (circles[ll][0]+b_dxy_as[0]/good_plscs[ll], circles[ll][1]+b_dxy_as[1]/good_plscs[ll])\n",
    "#         c_xy = (circles[ll][0]+c_dxy_as[0]/good_plscs[ll], circles[ll][1]+c_dxy_as[1]/good_plscs[ll])\n",
    "#         sbc_circ = (circles[ll], b_xy, c_xy)\n",
    "#         circ_rads = (ap_rad, 0.1, 0.1)\n",
    "#         circle_colors = ('y', 'w', 'w')\n",
    "#         # ellipses in frame\n",
    "#         in_a = (din_au/dist)/good_plscs[ll]\n",
    "#         out_a = (dout_au/dist)/good_plscs[ll]\n",
    "#         ells = (circles[ll],circles[ll])\n",
    "#         ells_a = (in_a, out_a)\n",
    "#         ells_b = (in_a*np.cos(np.deg2rad(incl)), out_a*np.cos(np.deg2rad(incl)))\n",
    "#         ells_angs = (PA_a, PA_a)\n",
    "#         if show_plots:\n",
    "#             plot_frames(good_frames[ll], label=labels[ll], dpi=300, colorbar=True, cmap='inferno', \n",
    "#                         circle=sbc_circ, circle_radius=circ_rads, circle_color=circle_colors, # circles\n",
    "#                         ellipse=ells, ellipse_a=ells_a, ellipse_b=ells_b, ellipse_angle=ells_angs, ellipse_label_color='g', # ellipses\n",
    "#                         vmin=vmin, log=True, ang_scale=True, label_color='w',\n",
    "#                         ang_ticksep=ticksep, pxscale=good_plscs[ll], colorbar_label='MJy/sr')\n",
    "#         # same but to save figure\n",
    "#         plot_frames(good_frames[ll], label=labels[ll], dpi=300, colorbar=True, cmap='inferno', \n",
    "#                     circle=sbc_circ, circle_radius=circ_rads, circle_color=circle_colors, # circles\n",
    "#                     ellipse=ells, ellipse_a=ells_a, ellipse_b=ells_b, ellipse_angle=ells_angs, ellipse_label_color='g', # ellipses\n",
    "#                     vmin=vmin, log=True, ang_scale=True, label_color='w',\n",
    "#                     save=join(figs_dir,'Aperture_{:.1f}mu_annotated.pdf'.format(wl)),\n",
    "#                     ang_ticksep=ticksep, pxscale=good_plscs[ll], colorbar_label='MJy/sr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55e2553",
   "metadata": {},
   "source": [
    "<hr style=\"border:1px solid gray\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970ce8dc",
   "metadata": {},
   "source": [
    "# <img style=\"float: center;\" src=\"https://www.stsci.edu/~dlaw/stsci_logo.png\" alt=\"stsci_logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
